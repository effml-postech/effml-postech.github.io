[{"id":0,"href":"/docs/spring24/00_taco_example/","title":"00 Taco Example","section":"Spring24","content":" Example : Content # This paper propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss. By doing so, they avoid decoding based on text-guided generative models\u0026mdash;known for high generative diversity\u0026mdash;and effectively utilize the semantic information of text at a global level.\nExample : Using KaTeX for math equation # KaTeX shortcode let you render math typesetting in markdown document. See KaTeX\nHere is some inline example: \\(\\pi(x)\\) , rendered in the same line. And below is display example, having display: block \\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi\\] Text continues here!!!\n"},{"id":1,"href":"/docs/spring24/01_/","title":"01","section":"Spring24","content":" Is Bigger Edit Batch Size Always Better? - An Empirical Study on Model Editing with Llama-3 # Authors: Junsang Yoon, Akshat Gupta, Gopala Anumanchipalli\nPosted by Jin Hyun, Gyuhyun Jung\nBackground # What is model editing? # Fig 1. Concept of model editing. The rapidly evolving field of artificial intelligence faces the challenge of keeping large language models (LLMs) up-to-date with new information, as traditional retraining methods are time-consuming and resource-intensive. As shown in figure, an alternative is model editing proposed in (Sinitsin et al., 2020). It enables data-efficient alterations to the behavior of models.\nFig 2. Example of model editing in case of MEMIT. Model editing modifies stored facts within a model and corrects inaccuracies without retraining. Techniques such as ROME (Rank-One Model Editing) (Meng et al., 2022a), MEMIT (Mass Editing Memory in Transformer) (Meng et al., 2022b), and EMMET (Equality-constrained Mass Model Editing algorithm for Transformers) (Gupta et al., 2024), known as \u0026ldquo;locate-and-edit\u0026rdquo; algorithms, have emerged to optimize the preservation-memorization (PM) objective. These methods directly modify specific areas of the model and are applicable to any transformer-based LLMs, offering a more efficient way to update models without retraining.\nHow model editing works? # For a relation \\((s,r,o)\\) expressed as a tuple in the form of (subject, relation, object). In model editing, we aim to update the memory of the existing model with new facts by learning about a new object \\((s,r,o^*)\\) . Model editing directly reform the weight by objective function, called the preservation-memorization objective. This objective consists of two parts, a preservation term and a memorization term. Below equation shows how ROME works with preservation term and memorization term.\n\\( \\argmin_{\\hat{W}} \\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\quad \\text{s.t.} \\quad \\hat{W} k_e = v_e \\\\Preservation\\_term=\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\\\ Memorization\\_term=\\hat{W} k_e = v_e \\) Where W represents the weights of the feedforward layer we want to edit, k is a key-vector representative of a fact, \\(v_e\\) is the desired output, and \\(K_0 =[k_1^0 |k_2^0 |\\cdots| k_0^N]\\) is a matrix consisting of facts we want to preserve. Above equation is optimized by follwing gradient.\n\\(\\hat{W} = W_0 \u0026#43; \\Delta \\quad \\text{where} \\\\ \\Delta = (v_e - W_0 k_e) \\frac{k_e^T C_0^{-1}}{k_e^T C_0^{-1} k_e} \\) For MEMIT model editing. it optimizes same objectives with ROME, but performance memorization using a least-square constraint, which allows for a closed-form solution. It has similar form with ROME method, but it multiplies \\(\\lambda\\) term, which is hyperparameter, to preservation term. Also, it combines memorization term for minimize target\n\\(\\argmin_{\\hat{W}} \\lambda\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \u0026#43; \\left\\| \\hat{W} K_E - V_E \\right\\|\\\\Preservation\\_term=\\lambda\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\\\ Memorization\\_term=\\hat{W} K_E - V_E \\) \\(V_E\\) is stacked matrix of \\(v_e\\) vectors, and fact is represented by a pair of vectors denoted as key ( \\(k_e\\) ) and value ( \\(v_e\\) ). This objective has similar solution of ROME, followed by below equations.\n\\(\\hat{W} = W_0 \u0026#43; \\Delta \\quad \\text{where} \\\\ \\Delta = (V_E - W_0 K_R)K^T_E (\\lambda C_0 \u0026#43; K_E^T K_E^T)^{-1} \\) In EMMET, it shows model editing is possible with batched facts. It is possible by allowing memorization happens using an equality-constraint. EMMET objective and gradient solution is followed by below equations.\n\\(\\argmin_{\\hat{W}} \\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\|\\quad \\text{s.t.} \\hat{W} k_i^e = v_i^e \\quad \\forall i \\in [1, 2, \\cdots, E] \\\\Preservation\\_term=\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\\\ Memorization\\_term=\\hat{W} k_i^e = v_i^e \\quad \\forall i \\in [1, 2, \\cdots, E] \\\\ \\hat{W} = W_0 \u0026#43; \\Delta \\quad \\text{where} \\\\ \\Delta = (V_E - W_0 K_R)(K_E^T C_0^{-1}K_E)^{-1}K_E^TC_0^{-1} \\) How model editing performance is estimated? # Model performance is estimated with 4 main scores, and these scores are bsed on how model editing works with expressions of correct facts in \\((s,r,o^{c})\\) and false facts in \\((s,r,o^{*})\\) .\nEfficacy Score (ES) # ES measures if the new fact, which we want to edit, is successfully edited to model. It is measured by percentage where \\(\\mathbb{P}[o^*] \u0026gt; \\mathbb{P}[o^{c}]\\) , which means the portion of correct edition result from predictions.\nParaphrase Score (PS) # PS measures model\u0026rsquo;s ability to generalize following an edit. It is measured by where P(new fact) \u0026gt; P(old fact) under paraphrases of the query prompt.\nNeighborhood Score (NS) # NS represents the specificity of model editing. To measure NS, we collect a set of nearby subjects \\(s_n\\) for which \\((s_n,r,o^{c})\\) holds true. Then we test \\(\\mathbb{P}[o^*] \u0026gt; \\mathbb{P}[o^{c}]\\) , reporting the success fraction asn NS.\nComposite Score (S) # S represents the overall performance. It combines aspect of edit success, generalization, and specificity. It is calculated as the harmonic mean of Edit Success (ES), Paraphrase Score (PS), and Neighborhood Score (NS). It provies overall efficacy of model edits.\nExperiments \u0026amp; Results # What is the Optimal Layer for Model Editing? # Investigating the effectiveness of hidden states in LLMS for recalling facts using causal tracing showed thjat subject’s last token within the feed-forward networks at intermediate layer plays a significant role. (Meng et al., 2022b)\nMotivation : Later work showed that layers deemed important during causal tracing did not always translate to model editing performance. Therefore, this work focused on finding the optimal layer for model editing layer empirically.\nSteps for finding optimal layer\nMake 1000 non-sequential edits from the CounterFact (Meng et al., 2022a) dataset at each layer of the Llama-3 model. Calculate various model metrics(ES, PS, NS, S) to evaluate their impact. The layer that achieves the highest score is selected as the most suitable for targeted interventions. Fig 3. Post-edit performance of various metrics for Llama3-8b model using MEMIT and ROME on various layers. Eqch layer is edited with 1000 facts, one at a time and non-sequentially. Fig 4. Post-edit performance of various metrics on Llama2-7b for MEMIT on various layers. Evaluation results showed that layer 1 for Llama-3 outperformed on numerous metrics. Furthermore this trend was also shown in previous version, Llama-2, as seen in Figure 6. Here, MEMIT and ROME have very similar performance for model editing across layer of a model.\n→ Why? : Both algorithms optimize for the same objective with difference in the memorization constraints. This shows that memorization constraints plays minor effect on editing performance.\nOptimal way of Scaling Up model editing? # After finding the optimal layer, scaling of model editing on the same model can happen in two ways : batch editing \u0026amp; sequential-batched editing.\n1. Batch Editing :\nA large number(batch size) of knowledge edits are performed on the model with the same update. This work stick to editing a single layer of the model.\nExperimental settings\nTargeting layer1 in Llama-3 with batch size 16, 64, 256, 1024, and 4096 for Batched editing. Evaluation Results of Batch Editing\nFig 5. Various metric results (PS, NS, ES, S) after a batch edit (16, 64, 256, 1024, 4096) on MEMIT and EMMET respectively. For both MEMIT \u0026amp; EMMET editing, metrics are seen to consistently fall with larger batches, with NS being the most pronounced to fall. ES is most resilient metric to edits. PS, only metric to do so, seen to increase dramatically between batch sizes of 16 and 64. The similar trend between two editing techniques reflect the similarity in their optimization objectives.\n2. Sequential-batched Editing :\nSequential Editing is an alternate way to scale up model editing where facts are added sequentially to a model.\nThis work proposes optimal way to scale model editing that strikes a balance between Batch Editing \u0026amp; Sequential Editing.\nSequential-batched editing sequentially edit many batch of facts at a time. And the experiment was conducted going from batch size of 1 up to 4096. (1, 64, 256, 1024, 4096)\nFig 6. Single layer sequential editing performance for various batch sizes on MEMIT and EMMET respectively. Experimental results according to figures above showed that larger batch sizes are actually worse for model performance than sequential edits with smaller batches. In contrast, larger batch sizes seem to be better for metrics in NS : while batch edits are less successful in general, it is better in preserving locality of edits. This results were concluded to optimal batch size of 1024 for both MEMIT and EMMET. Increasing batch-size beyond that lead to larger model degradation and better editing results can be achieved by sequential-batched editing with smaller batch sizes.\nConclusion # This work examines several model editing techniques in the context of the newly released Llama-3 model and there are some conclusion as follows:\nEarlier layers may be more optimal intervention points. Model editing techniques that share same optimization objectives shows similar trends in layer and editing. Smaller, frequent sequential batch size edits have a superior performance. Batch size of 1024 for MEMIT and EMMET is optimal batchsize with sequential-batched editing. The authors argue that the current trend of pushing towards bigger edit batch sizes for scaling model editing may have limitations. Instead, they propose that future research should focus on methods that combine both batched and sequential editing to optimize performance while minimizing model degradation. Also, future work was proposed for experiments on multi-layer intervention for edits, as well as experiments against other popular models and algorithms, including methods that are hyper-network based.\nDiscussions, and research direction proposal from post writers # The paper empirically analyzes the performance of model editing based on batch size. It would be more beneficial for model editing research if the theoretical reasons behind the overall metrics decreasing as batch size increases are elucidated, rather than just empirically.\nWhile the work presents a hybrid format combining sequential editing and batch editing, it lacks in-depth analysis of the strengths and weaknesses of both approaches. Additionally, it is important to ensure that the individual characteristics of techniques such as ROME, MEMIT, and EMMET are appropriately integrated into editing optimization.\nAnalyzing the reasons behind the improvement in performance when layers are edited later in the network (NS) and the improvement when batch size is increased (PS) could help in identifying the optimal point for multi-layer editing\nIt seems necessary to investigate how many layers should be edited in multi-layer editing to achieve effective results beyond single-layer editing.\nReferences # Implementation code: Link.\nYunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang. 2023. Editing large language models: Problems, methods, and opportunities. arXiv preprint arXiv:2305.13172.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, Artem Babenko. 2020. Editable neural networks. arXiv preprint arXiv:2004.00345.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022a. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359–17372.\nKevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. 2022b. Massediting memory in a transformer. arXiv preprint arXiv:2210.07229.\nAkshat Gupta, Dev Sajnani, and Gopala Anumanchipalli. 2024. A unified framework for model editin. arXiv preprint arXiv:2403.14236.\n"},{"id":2,"href":"/docs/spring24/02_/","title":"02","section":"Spring24","content":" Spectrally Pruned Gaussian Fields with Neural Compensation (SUNDAE) # *Authors: Yang, Runyi, et al\n*Team: Donggeon Lee, Chiho Yoon\nSummary # 3D representation, which is the basis for many VR/AR and robotics applications, has long been an area of interest in computer vision and graphics. With the advent of neural radiation fields (NeRFs) Link, several methods have emerged to improve the quality and efficiency of NeRFs.\nConventional 3D Gaussian Splatting (3DGS) # Fig. 1. Comparison of 3D Gaussian Splatting to previous NeRF technologies. One of the recent hot topics in the NeRF field, 3D Gaussian Splatting (3DGS), demonstrates high quality and particularly fast, near real-time rendering speeds (about 100FPS).\nPros: Superior rendering speed and quality Cons: High memory consumption Proposed SUNDAE # Fig. 2. Comparison of 3D gaussian splatting and proposed SUNDAE It constructs a memory-efficient Gaussian field using spectral pruning and neural compensation. It considers the relationship between primitives, reducing memory usage while maintaining rendering quality. It significantly reduces memory consumption while preserving high rendering quality. Code: https://runyiyang.github.io/projects/SUNDAE/. Introduction # Fig. 3. Conceptual illustration of vanilla 3DGS, SUNDAE spectral pruning technique, and neural compensation. 3D Gaussian Splatting (3DGS) Link # Recently, 3DGS has been proposed as a novel 3D scene representation, utilizing a set of 3D positions, opacity, anisotropic covariance, and spherical harmonic (SH) coefficients to represent a 3D scene (left panel of Fig. 2). 3DGS demonstrates notable advantages in rendering speed, rendering quality, and training time. But it requires a large storage.\nSpectral graph pruning # Gaussian fields utilize a collection of Gaussian primitives as the representation of the scene. As these primitives are irregularly distributed in 3D space, they propose a graph-based data structure, rather than regular structures like grids, to capture the relationship between these primitives (middle panel of Fig. 2).\nNeural compensation # To address an inevitable decrease in rendering quality, they employ a neural compensation head to compensate for this quality loss (right panel of Fig. 2).\nContributions # A newly proposed primitive pruning framework for Gaussian fields based upon the spectrum of primitive graphs. A novel feature splatting and mixing module to compensate for the performance drop caused by the pruning. State-of-the-art results, in terms of both quality and speed, on various benchmarks with low memory footprint. Methods # We have 4 steps for the method\n3D Gaussian Splatting Warm Up Spectral Graph Pruning Neural Compensation Continuous Pruning as a Strategy The overall framework is in Fig.\nFig. 4. The overall framework of SUNDAE with pipeline and graph-based pruning. Let’s see how each step works.\n1. 3D Gaussian Splatting Warm up # SUNDAE initialize Gaussian centers of the vanilla 3D Gaussian Splating as the first step for generating a dense representation using Gaussian primitives. Then, an effective densification strategy is used to increase the primitives.\n- Gaussian Primitive Initialization # The point cloud $P_c$ is the first input for representing the 3D scene using Gaussian primitives $P$. Then they turn 3D coordinates $x \\in P_c$ into Gaussian primitives $p \\in P$ by the following equation:\n$p(x)=exp(-1/2(x)^T\\Sigma^-1(x))$ where the $\\Sigma$ is defined as 3D covariance matrix. They also use an ellipsoid configuration. The decomposition of $\\Sigma$ is achieved with scaling matrix $S$ and a rotation matrix $R$, as expressed in the equation:\n$\\Sigma=RSS^TR^T$ - Gaussian Primitive Densification # They optimize all parameters of Gaussian primitives and integrate a densification strategy to improve representation power during the training process.\n2. Spectral Graph Pruning # After warm-up, a dense representation incurs significant storage consumption. To efficiently prune redundant primitives, they used the graph signal processing theory and construct a graph based on Gaussian primitives.\n- Graph Signal Processing Preliminaries # Graph shift: Graph shift represents the connections between nodes in a weighted graph, typically represented by a weighted adjacency matrix. It quantitatively describes the relationships between nodes using the weights of edges.\nGraph signal: Graph signal is a mapping assigning values to each node in a graph, utilized to model interactions between nodes.\nGraph Fourier Transform: Graph Fourier Transform is the process of expanding a graph signal using the eigenbasis of the graph shift, enabling analysis of the structure and interactions within the graph.\n- Graph Construction # Given a set of Gaussian Primitives %P% , they construct a nearest neighbor graph with the adjacent matrix $W$ of the graph:\n$$ W_{ij} = \\begin{cases} \\exp\\left( -\\frac{\\|x_i - x_j\\|^2}{2 \\sigma^2} \\right), \u0026 \\text{if } \\|x_i - x_j\\|^2 \u003c \\tau \\\\ 0, \u0026 \\text{otherwise} \\end{cases} $$ where $x_i$ and $x_j$ are central points in $P$, $\\tau$ is a hyperparameter, and $\\sigma$ is the variance of the distance matrix.\n- Graph Filtering and Sampling # SUNDAE propose a band-limited graph filter that combined with a high-frequency filter and low-frequency filter. By doing this, they can catch both the detailed information and general information. Design of filters are Haar-like.\nThey also prune the abundant primitives according to the response magnitude of the high-pass filter.\n3. Neural Compensation # There is a decrease in rendering quality for large pruning ratio. To address this, they employ a neural compresation network to model the relationship between primitives in the 2D domain.\nThey render the 3D Gaussian primitives into neural images in a differentiable manner, using the differentiable 3D Gaussian renderer from 3DGS with feature rendering instead of RGB rendering. The center of each Gaussian primitive is projected using a standard point rendering method, and the covariance in the neural image space is calculated.\nThe neural image is then computed using the feature vectors of Gaussian primitives. A lightweight neural network (U-Net with skip connections) is used to compensate for the quality drop after spectral pruning.\nThe overall optimization process is based on the difference between the rendered images and the ground truth images from the dataset. The compensation network and the 3D Gaussian primitives are optimized simultaneously during training, using a loss function that combines L1 loss and D-SSIM loss.\n4. Continuous Pruning as a Strategy # In addition to the training-then-pruning strategy, a continuous pruning strategy is explored. Continuous pruning periodically removes a specific number or percentage of primitives during the training process. This aims to lower peak memory usage and allow training on GPUs with lower memory. However, it can result in less predictable final memory usage, as the reduction may vary across different scenes. Therefore, continuous pruning is considered an alternative strategy when needed.\nResults # Quantitative Results # Table 1. Quatitative evaluation of SUNDAE. SUNDAE demonstrates strong performance across various metrics, including PSNR, SSIM, FPS, and memory usage.\nCompared to existing methods on the MipNeRF360 dataset, SUNDAE achieves a balance between rendering quality and efficiency, maintaining high FPS rates while significantly reducing memory consumption. Even at low sampling rates, SUNDAE remains competitive with established approaches, showcasing the effectiveness of its spectral pruning and neural compensation techniques in managing Gaussian primitive relationships and retaining scene information. Overall, SUNDAE represents scenes more compactly while maintaining high quality rendering. Qualitative Results # Fig5. Qualitative results of SUNDAE. The qualitative results demonstrate that SUNDAE achieves comparable novel view synthesis quality with significantly lower memory consumption (1% or 10%).\nThe graph effectively captures primitive relationships, while the neural compensation head preserves rendering quality. Spectral pruning notably removes outliers near the camera, enhancing scene coherence. Ablation Study # Fig6. Ablations experiment on the ratio 𝛾 of the bandlimited filter of graph based pruning. Band-limited ratio of Graph-based pruning: The band-limited filter\u0026rsquo;s ratio, represented by 𝛾, significantly impacts rendering quality, with a 𝛾 value of 50% yielding the most favorable outcomes, emphasizing the advantage of spectral pruning in preserving important high-frequency details and low-frequency background (Fig. 4). Table 2. Ablations of neural compensation module size. Fig. 7. Visualization with and without neural compensation. The compensation performance of the network: Employing the neural compensation module enhances performance across all sampling rates evide(Table 2, Fig. 5), highlighting its compensatory capability in mitigating performance drops caused by spectral pruning and effectively modeling the relationship between primitives. Neural Compensation Module Size: Increasing the size of the neural compensation module does not necessarily enhance rendering quality (Table 2), aligning with findings from ADOP and indicating a balance between quality and memory usage. Conclusion # They propose SUNDAE, a novel approach to spectrally prune Gaussian fields with neural compensation, efficiently capturing the relationship between Gaussian primitives using graph signal processing and blending information to offset pruning-induced information loss. By leveraging spatial information among Gaussian primitives to construct a graph and spectrally pruning less significant ones, they employ a lightweight neural network to compensate for quality degradation post-pruning. Experimental findings demonstrate SUNDAE\u0026rsquo;s ability to maintain the efficiency of 3DGS while significantly reducing its size across various scenarios. "},{"id":3,"href":"/docs/spring24/03_/","title":"03","section":"Spring24","content":" Introduction # The significant advances in deep learning over the past decade have largely relied on the development of algorithms that efficiently leverage available hardware. As the size of state-of-the-art models increases, hardware efficiency becomes crucial for reducing training costs, which have grown substantially in terms of money, time, and environmental impact. However, with the end of Moore\u0026rsquo;s Law and Dennard scaling, increased transistor density alone cannot provide a straightforward path to greater efficiency. The use of low-precision number formats is a promising alternative. These formats offer substantial gains in compute, memory, and bandwidth efficiency, making them valuable in the context of modern deep learning.\nBackground # Floating-Point Formats for Deep Learning # Traditionally, floating-point numbers are defined by the IEEE 754 standard, which specifies the number of exponent bits (E) and mantissa bits (M). Common floating-point formats used in machine learning include FP32, TF32, BFLOAT16, and FP16. Recently, two types of FP8 formats (E4 and E5) have been proposed.\nSuggested Image: \u0026ldquo;Table A.1. Common floating point formats for deep learning.\u0026rdquo;\nAdvantages and Disadvantages of Low-Precision Training # Disadvantages: FP16 and BFLOAT16 offer different trade-offs. FP16 has higher precision, but BFLOAT16 has a wider range. FP8 formats reduce both range and precision. The use of low-precision formats can introduce quantization noise and other issues. Advantages: Using low-precision formats can significantly improve efficiency in terms of memory usage, bandwidth usage, compute performance, and cross-device communication costs. Suggested Image: \u0026ldquo;Figure 2. The signal to noise ratio (SNR) of samples from a normal distribution, quantised in FP16 and FP8, as a function of the distribution’s scale.\u0026rdquo; 【3†source】\nTechniques for Low-Precision Training # Mixed Precision: This technique uses multiple number formats with different bit-widths, placing most activations, weights, and gradients in FP16 without loss of accuracy.\nLoss Scaling: To overcome the limited range of FP16 and FP8, the loss can be multiplied by a scalar to increase the scale of gradients. This method requires empirically finding a suitable loss scale:\n\\[ \\text{scaled\\_loss} = \\text{loss} \\times \\text{scale\\_factor} \\] \\[ \\text{scaled\\_gradients} = \\text{gradients} \\times \\text{scale\\_factor} \\] Automatic Loss Scaling: This dynamically adjusts the loss scale during training, removing the need to sweep for an initial loss scale.\nPer-Tensor Scaling: This system locally rescales based on runtime statistics to address scaling difficulties in FP8 training.\nSuggested Image: \u0026ldquo;Table 1. A comparison of techniques for low precision training.\u0026rdquo;\nAnalysis # Ideal Scaling # The ability to predict the scale of tensors at the start of training is crucial. We argue that unit variance ( \\(\\sigma = 1\\) ) is an optimal balance among various competing factors. This approach helps concentrate values within the representable range, reducing clipping errors during training.\nIn floating-point formats, values are represented as:\n\\[ \\text{value} = (-1)^{b_{\\text{sign}}} \\times 2^{\\text{exponent}} \\times \\left(1 \u0026#43; \\frac{b_{\\text{mantissa}}}{2^M}\\right) \\] where \\(b_{\\text{sign}}\\) , \\(b_{\\text{exponent}}\\) , and \\(b_{\\text{mantissa}}\\) represent the sign, exponent, and mantissa bits, respectively.\nSuggested Image: \u0026ldquo;Figure 1. Above: Unit scaling of an FFN layer. We multiply each tensor by a fixed scalar to achieve consistent scale, no longer requiring a loss scale to control the scale of gradients. Below: A histogram of exponent values at initialisation for the above FFN.\u0026rdquo;\nPredictable Scaling # If we can predict the scale of tensors in a deep learning model, we can effectively address clipping errors. At initialization, parameters are drawn from known distributions, allowing us to analytically or empirically derive the scale of each tensor.\nFor example, by considering the scaling factors for each operation in the neural network, we can perform scaled operations:\n\\[ y = \\alpha \\cdot f(x) \\] where \\(\\alpha\\) is the scaling factor and \\(f\\) represents the operation.\nUnit Scaling # Unit scaling is proposed to address the limitations of existing methods for managing scale in typical models. A model is considered unit-scaled if its activations, weights, and gradients have approximately unit variance at initialization. This is achieved by inserting scaling factors into the forward and backward passes. Unlike loss scaling, which requires an empirically determined hyperparameter or an adaptive algorithm, unit scaling determines these scales based on a set of rules for each operation, approximately preserving the variance of the inputs. This leads to global unit scaling throughout the model, ensuring tensor values are centered within the exponent range at initialization, providing headroom during training to avoid going out of range.\nA framework for scaling computational graphs # Computational Graphs\nRepresent model by the differentiable function \\(f_{model}(x_1,...,x_m)\\) Describe the structure of such a model using a directed acyclic graph (DAG) denoted \\(\\mathcal{G} =(\\mathcal{V}, \\mathcal{E}) \\) This kind of graph is commonly known as a computational graph, with vertices as nodes and their corresponding functions as ops. Forward and backward graphs\nWe refer to the computational graph corresponding to \\(f_{model}\\) as the forward graph In deep learning we typically apply reverse-mode automatic differentiation to the forward graph to create a second computational graph whose output nodes represent the partial derivatives of the model with respect to its inputs: \\( \\frac{\\partial f_{model}}{\\partial x_i}, \\forall i \\in[1 . . m] \\) . We call this the backward graph Scaled ops\nGiven an op \\(f\\left(x_1, \\ldots, x_k\\right)\\) , we define the scaled op \\( f^*\\left(x_1, \\ldots, x_k, \\alpha, \\beta_1, \\ldots, \\beta_k\\right) \\) with scaling factors \\( \\alpha, \\beta_1, \\ldots, \\beta_k \\in \\mathbb{R}^{\u0026#43;} \\) , such that \\(f^* \u0026amp; \\triangleq \\alpha \\cdot f\\left(x_1, \\ldots, x_k\\right)\\) \\( f_{\\text {grad }}^*\\left(x_1, \\ldots x_k, g\\right)_i \u0026amp; \\triangleq \\beta_i \\cdot f_{\\text {grad }}\\left(x_1, \\ldots x_k, g\\right)_i, \\forall i \\in[1 . . k] \\) Scaled computational graph\nA scaled computational graph is one where every op \\(f\\) in the forward graph is replaced by a scaled equivalent \\(f^{*}\\) , with the backward graph then generated to produce \\(f^{*}_{grad}\\) grad for each \\(f_{grad}\\) , using any choice of scaling factors. Constraint-scaled computational graphs\nA constraint-scaled computational graph is a scaled computational graph where we restrict the scaling factors of ops that consume non-cut-edge variables in the following way: for any edge \\(e \\notin \\mathcal{C}\\) , we require the op consuming the variable \\(x_e\\) to have scaling factors \\(\\alpha = \\beta_e f\\) . Proposition 5.1\nFor any scaled op, there is an equivalent unscaled op with the same training dynamics under a firstorder optimiser.\nTheorem 5.2\nA constraint-scaled computational graph itself represents a scaled op.\nA scaling strategy for unit variance # Unit scaled computational graphs\nInitially set aside any scale constraints, and calculate the scaling factors that give each op expected unit variance outputs (this process is covered below). Now resolve any scale constraints by taking each constrained group \\( {\\alpha, \\beta_1, \\ldots, \\beta_l } \\) and selecting the geometric mean \\( \\left(\\alpha, \\beta_1, \\ldots, \\beta_l \\right)^\\frac{1}{l\u0026#43;1} \\) Selecting scaling factors\nAssuming unit-scaled inputs to \\( y = f(x_i,\\ldots,x_k) \\) , derive the output scale \\( \\sigma_Y \\) and set the forward scaling factor \\( \\alpha = 1/\\sigma_Y \\) . Repeat this process for \\( x_i\u0026#39;=f_{grad}(\\ldots)_i, \\forall i \\in[1 . . k] \\) , to obtain the gradient scale \\( \\sigma_{x_i\u0026#39;} \\) i and set the backward scaling factor \\( \\beta_i = 1/\\sigma_{x_i\u0026#39;} \\) . Weighted addition # When tensors of different scales, such as those in residual layers, losses, and positional encodings, are added, simply adding them can adversely affect performance. To address this, we propose using weighted_add. In this approach, we can maintain unit scale while performing operations using a scaled identity function.\nRecipe # We now outline a high-level recipe for a unit-scaled model:\nInitialise non-bias parameters with unit variance. Calculate scaling factors for all scaled ops. Identify non-cut-edges, and constrain the ops consumingthem to have \\( \\alpha = \\beta \\) by taking the geometric mean. Replace adds with weighted adds. Example # Using the unit scaling recipe, we first build a scaled op, and then a full scaled layer. Consider a scaled projection op with learnable weights:\n\\( \\operatorname{matmul}^*(X,W) =\\alpha \\cdot X W \\) \\( \\operatorname{matmul}_{\\text {grad }}^*(X, W, G)_1 = \\beta_1 \\cdot G W^{\\top} \\) \\( \\operatorname{matmul}_{\\text {grad }}^*(X, W, G)_2 = \\beta_2 \\cdot X^{\\top} G \\) for input \\( X \\in \\mathbb{R}^{b \\times m} \\) , weight \\( W \\in \\mathbb{R}^{m \\times n} \\) , output \\( \\mathbb{R}^{b \\times n} \\) and incoming gradients \\( G \\in \\mathbb{R}^{b \\times n} \\) We show code for the above in Figure 3, which also gives a scaled layer for the Transformer FFN\nFig3. PyTorch examples Results # Character language modelling\nExperimental Setup: Train causal language models on WikiText-103 raw character language modeling, using cross-entropy loss during training and evaluating on bits per character (BPC). Below the product of these settings, we compare the performance of regular (baseline) and unit scaling in both FP32 and FP16.\nSequence layer type: Attention, RNN and Convolution Norm placement: PreNorm, PostNorm and NoNorm Residual scaling: default, fixed and running-mean Results\nFirst, these demonstrate the need for scaling when using FP16. This is due to gradient underflow, since loss scaling with a factor of 2048 resolves the issue. Second, they demonstrate that unit scaling, despite changing the training behaviour of the model beyond just numerics, matches or even slightly improves upon baseline performance in almost all cases. Finally, they show that no tuning is necessary when switching unit scaling to FP16. suggest that running-mean or fixed are reasonable choices when using unit scaling Fig4. Character language modelling, showing validation bits per character over a wide range of models Masked language modelling\nExperimental Setup\nTo evaluate the advantages of unit scaling, we assess BERTBASE and BERTLARGE models, which typically struggle with loss scaling. Results\nTable2. Downstream performance of regular and unit-scaled BERT models Related Work # Variance scaling analysis\nVariance scaling and residual networks, along with normalization variants, complement unit scaling, which considers both activation and gradient norms. The reparameterization implied by unit scaling, utilized in analyzing deep network training dynamics, applies scaling factors locally throughout the compute graph, akin to training hyperparameter scaling. FP8 inference\nFP8 training lacks hardware support, yet accelerated 8-bit inference is becoming more prevalent through integer quantization to INT8. While this process often leads to reduced accuracy, recent efforts aim to enhance efficient INT8 quantization. FP8 adoption allows accelerated inference in the same format as training, promising significant improvements in the simplicity and accuracy of 8-bit inference. Discussion # Compute overhead\nUnit scaling introduces minimal compute overhead by adding scaling operations that can be fused into preceding operations, resulting in negligible memory-access cost. While basic loss scaling operates similarly, automatic loss scaling may incur additional overhead due to occasional batch discards, particularly noticeable in FP8. Proposed automatic per-tensor scaling schemes may introduce overhead, depending on software and hardware characteristics, as they trade off accuracy for complexity. In contrast, unit scaling with fixed precomputed scaling factors offers a simpler alternative without such complexities. Broader impact\nWith the potential for unit scaling to effectively train larger models, concerns arise about issues such as toxicity, misinformation, privacy concerns, and environmental damage. To address these challenges, various methods have been proposed, including AI feedback, anti-experts, and baked-in safety models. Conclusion Unit scaling has demonstrated to address the complexities of low-precision training, providing a simpler and more granular solution, even enabling the training of BERTLARGE without loss scaling for the first time, even in FP8.\n"},{"id":4,"href":"/docs/spring24/04_/","title":"04","section":"Spring24","content":" Better \u0026amp; Faster Large Language Models via Multi-token Prediction # Posted by Jinoh Cho and Seonghyeon Park\nAuthors: Gloeckle et al. Institution : FAIR at Meta, CERMICS Ecole des Ponts ParisTech and LISN Universite Paris-Saclay Preliminaries # Language Modeling and Next-Token Prediction Task # Standard language modeling involves learning from a large text corpus $x_1, \\ldots, x_T$ by implementing a next-token prediction task. The goal is to minimize the cross-entropy loss, defined as:\n$$ L_1 = - \\sum_{t} \\log P_{\\theta}(x_{t+1} \\mid x_{t:1}), $$\nwhere $P_{\\theta}$ represents our large language model under training. The objective is to maximize the probability of the next token $x_{t+1}$, given the history of previous tokens $x_{t:1} = x_1, \\ldots, x_t$.\nCore Idea # Multi-Token Prediction Task # In this work, authors generalize this approach by implementing a multi-token prediction task. At each position of the training corpus, the model is instructed to predict $n$ future tokens at once. Thus, training objective can be expressed as follow:\n$$ L_n = - \\sum_{t} \\log P_{\\theta}(x_{t+n:t+1} \\mid x_{t:1}) = - \\sum_{t}\\sum_{i=1}^{n} \\log P_{\\theta}(x_{t+i} \\mid x_{t:1}). $$\nThis formulation allows the model to learn to predict multiple future tokens simultaneously, enhancing its predictive capabilities and efficiency.\nMemory-Efficient Implementation # Naive\n"},{"id":5,"href":"/docs/spring24/05_/","title":"05","section":"Spring24","content":" Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting # Author: Liu F, Tang Y, Liu Z, Ni Y, Han K, Wang Y Written by Nayoung Kwon and Jiwoong Im\n#Introduction\n"},{"id":6,"href":"/docs/spring24/06_/","title":"06","section":"Spring24","content":" # "},{"id":7,"href":"/docs/spring24/07_/","title":"07","section":"Spring24","content":" # "},{"id":8,"href":"/docs/spring24/08_/","title":"08","section":"Spring24","content":" MIXTURE OF LORA EXPERTS # Background # What is LoRA? # LoRA is a methodology for effective fine-tuning large-scale pretrained models.\nModels such as OPT, LLaMA, and CLIP demonstrate remarkable performance when fine-tuned for various downstream tasks. However, full fine-tuning of these massive models requires substantial computational resources. LoRA enables parameter-efficient fine-tuning by keeping the pretrained model\u0026rsquo;s weights frozen and adding trainable low-rank decomposition matrices.\nIn the above figure, only the matrices A and B are trained, with dimensions (d x r) and (r x d) respectively. By setting r \u0026laquo; d, the number of parameters to be trained can be reduced. These trained matrices are then added to the existing pretrained weights, allowing tuning without affecting the inference speed of the original model.\nLoRAs Composistion # The common solution to further improve the performance of LoRA is to compose multiple trained LoRAs. Research on LoRA composition can be broadly categorized into the following two methodologies.\nLinear arithmetic composition. It is a method of directly adding multiple LoRAs. This approach is simple and has been effective in the NLP and Vision-Language domain, but it can result in the loss of pre-trained model\u0026rsquo;s generative capabilities or the individual characteristics of each LoRA.\nReference tuning-based composition tackles the above limitations of linear arithmetic method by introducing gradient fusion and controllable sampling, but is requires retaining when incorporating different LoRAs or creating new masks, which results non-trivial computational costs.\nMixture-of-Experts # Mixture of LoRA experts # Motivations # Direct linear arithmetic composition reduced the generative power of the model, while normalized linear arithmetic composition retained the generative power of the model but lost its LORA character. In the V\u0026L domain, directly composing multiple trained LoRAs into the original embedding caused significant parameter variations and meaningless output, while normalization compromised their original characteristics. In the NLP domain, composing four or more LoRAs within the FLAN-T5 model resulted in disordered output, and weight normalization across five datasets decreased the performance, suggesting adverse effects on the intrinsic qualities of the trained LoRAs. 2. Each layer of the trained LoRA represented a unique characteristic, which cumulatively defined the overall properties of the LoRA. (Right: Observed that different layers of LoRA encode distinct features, such as dog coat color and facial features., left: When evaluated on a subset of datasets, there were significant differences in performance across the different layers of LoRA.) So, The conjecture is that adjusting the characteristics by varying the layer-specific weights according to the desired domain objective will result in a more effective composition of trained LORAs.\nMethod # See related formulas Symbols input $x \\in \\mathbb{R} ^ {L \\times d}$ L: sequence length d: dim of $x$ Multi attention layer : $$\\mathcal{f}_{Attn} (\\centerdot)$$ Feed forward neural network layer: $$\\mathcal{f}_{FFN} (\\centerdot)$$ LN: layer normalization Trained LORAs $$\\Omega = \\left\\{ \\Delta \\Theta \\right\\}^N_{i=0}$$ learnable gating function $$\\mathcal{G} (\\centerdot)$$ The weight of the $i^{th}$ trained LorA $$\\mathcal{G}_i (\\centerdot)$$ Concatenation operation: $$\\oplus$$ Learnable parameter $e \\in \\mathbb{R} ^ {N^2 \\times L \\times d}$ Learnable temperature scalar $\\tau$ Freezing part $$x^\\prime_{\\theta} = x + \\mathcal{f}_{Attn} (LN(x)|\\theta)$$ $$\\mathbf{F}_\\theta (x) = x^\\prime_{\\theta} + \\mathcal{f}_{Attn} (LN(x^\\prime_{\\theta})|\\theta)$$ LoRA part $$x^\\prime_{\\Delta \\Theta_i} = x + \\mathcal{f}_{Attn} (LN(x)|\\Delta \\Theta_i)$$ The output of each LoRA $$\\mathbf{E} _{\\Delta \\Theta_i} (x) = x^\\prime_{\\Delta \\Theta_i} + \\mathcal{f}_{FFN} (LN(x^\\prime_{\\Delta \\Theta_i})|\\Delta \\Theta_i)$$ The output of all LoRA $$\\mathbf{E}_\\Omega (x) = Normalization(\\mathbf{E}_{\\Delta \\Theta_0} (x) \\oplus \\ldots \\oplus \\mathbf{E}_{\\Delta \\Theta_{N-1}} (x)) \\in \\mathbb{R} ^ {N \\times L \\times d}$$ Flatten and dot product operation $$\\epsilon = Flatten(\\mathbf{E}_\\Omega (x))^T \\centerdot e, \\epsilon \\in \\mathbb{R} ^ N$$ Gate value for each LoRA $$\\mathcal{G} (\\epsilon_i) = \\frac {exp(^{\\epsilon_i} /_ \\tau)} {\\displaystyle\\sum_{j=1}^N {exp(^{\\epsilon_j} /_ \\tau)}} $$ Final output of the gating function $${\\tilde{\\mathbf{E}}_\\Omega (x)} = \\displaystyle\\sum_{i=0}^N {\\mathcal{G} (\\epsilon_i) \\centerdot \\mathbf{E} _{\\Delta \\Theta_i} (x)} , {\\tilde{\\mathbf{E}}_\\Omega (x)} \\in \\mathbb{R} ^ {L \\times d} $$ Final output of Transformer block $$\\mathcal{O}(x) = {\\mathbf{F}_\\theta (x)} + {\\tilde{\\mathbf{E}}_\\Omega(x)} $$ Training # The final loss function used in MoLE is as follows:\nAlpha is a coefficient for weight balancing. Gating Balacing Loss\nAs shown in Figure 5 (a), the average entropy of the distribution probabilities from the gating functions gradually decreases as training progresses. In Figure 5 (b), we can see a gating probability of 64% for LoRA β among the three LoRAs, indicating that the gating function tends to converge to a state where it assigns large weights to well-performing LoRAs in the early stages. This can result in a significantly larger impact from a few specific LoRAs compared to others, potentially leading to biased outcomes. To avoid this, the author created a gating balancing loss. The gating balancing loss helps prevent bias by ensuring that the loss value decreases as the model becomes less biased. See related Symbols M: the nu of blocks where gating functions are placed N: num of LoRAs Domain-specific Loss In V\u0026amp;L, Using a loss in CLIP(Radford et al,20221b) In NLP, Using a loss in FLAN-T5(Chung et al,2022)\nResults # Analyisis and Limitations # "},{"id":9,"href":"/docs/spring24/09_/","title":"09","section":"Spring24","content":" MobileNetV4 - Universal Models for the Mobile Ecosystem # Posted by JoonSeok Kim and DongGyu Kim\nMain Contributions # Universal Inverted Bottleneck (UIB) seach block - Unifies the Inverted Bottleneck (IB), ConvNext, Feed Forward Netowork (FFN), and Extra Depthwise variant Mobile MQA - Attention block tailored for mobile accelerators NAS technique to improve performance Achieves Pareto optimal acorss various devices such as CPUs, DSPs, GPUs, and TPUs Novel distillation technique to boost accuracy. Achieves 87% accuracy on ImageNet-1k and 39x smaller model size Preliminaries - Roofline Model and Hardware Efficiency # Algorithm running on hardware is composed of two parts - memory access and computation. The computation time is determined by the computation requirement and hardware performance. \\[runtime_computation = {Number of Operatins}/{FLOPS}\\] Algorithm runtime can be limited by the memory access bottleneck or communication overhead \\[runtime_communication = {Number of IO Bytes}/{Bandwidth}\\] Hardware performance is determined by the upper bound of the computation time or memory access latency \\[performance = max(runtime_computation, runtime_communication)\\] Below Fig. 1(a) and Fig. 1(b) illustrates the roofline model and its characteristics\nFig. 1 (a) Roofline Model Fig. 1 (b) Roofline Model with Ceiling Hardware-Independent Pareto Efficiency # Fig. 2. Ridge Points and Latency/Accuracy Tradeoffs Fig. 3. Op Cost vs. Ridge Point This research focuses on efficiency on various hardware targets such as DSPs, CPUs, and GPUs. To find whether the hardware is limited by its memory bottlenecked or compute bottlenecked, the Roofline Model of that hardware must be investigated. It is defined by the harware's peak computational throughput and its peak memory bandwidth. The optima of that hardware can be found in its ridge point, which is defined by the hardware's ratio of Peak MACs to Peak memory bandwidth. The algorithm's accuracy and latency are swept by the ridge point on various hardware on Fig. 2, and Fig. 3. The roodline model of MobileNetV4 achieves highest Pareto-optimal performance compared to other MobileNet models. MobileNetV4 is designed to achieve Pareto optimal and hence balances MAC operations and memory bandwidth. The initial layers are designed with high MAC intensity, so as to improve model capacity and downstream accuracy. The end layers use identically-sized FC layers to maximize accuracy. These two initial and end layers are balances so that MobileNetV4 should not see slowdowns at any hardware.\nUniversal Inverted Bottlenecks (UIB) # Fig. 4. Universal Inverted Bottleneck (UIB) blocks The main advantage of UIB is its adaptability and flexibility, that mitigates seach complexity. Optional Depthwise (DW) convolution blocks are inserted before the expansion layer, and between the expansion and projection layer. In the NAS procedure, common components such as the pointwise expansion and projection are shared and DWs are added as search options. UIB has four possible instantiations as follows.\nInverted Bottleneck (IB) : Spatial mixing on the expanded features activations, and provides higher model capacity ConvNext : Cheaper spatial mixing before the expansion with larger kernel size ExtraDW : Inexpensive increase of the network depth and the receptive field. Combined benefits of ConvNext and IB FFN : Stack of two 1x1 pointwise convolutions. Accelerator-friendly operation Mobile MQA # Table 1. Efficiency Gains by MQA This paper considers the Operational Intensity (OI), which is the ratio of arithmetic operations to memory access, to enhance efficiency of vision models on mobile accelerators. Here, Multi-Query Attention (MQA) is proposed instead od Multi-Head Self Attention (MHSA), which is simplified by utilization of shared keys and values across all heads. This sharing of keys and values reduces memory access hence improving OI, especially when the batch size is small. Large language models does not have significant accuracy drop in this MQA case. Table 1 shows that by adding MHSA and MQA, the performace accuracy has increased whereas the inference latency for MQA is approximately x39 lower than that of MHSA. Hence, MQA can accelerate better in the mobile environment, with negligible performance degradation.\nThe Spatial Reduction Attention (SRA) is applied, hence incorporating asymmetric spatial down-sampling, to downscale keys and values, and not queries. In hybrid models, there is a certain correlation between spatially adjacent tokens, hence necessitating spatial mixing convolution filters.\nRefined NAS for Enhanced Architectures # As shown above, the insitantiation of UIB blocks are in the neural architecture search process. TuNAS was adopted for the paper\u0026rsquo;s search strategy. The paper uses a two-stage search operation, the coarse-grained search and fine-grained serach to address the variance in parameter counts between UIB\u0026rsquo;s depthwise layers and other search options. The course-grained search process involves determining optimal filter sizes with fixed parameters. The fine-grained stage searches for the UIB\u0026rsquo;s layer configuration.\nResults # Table 5. Classification results on ImageNet-1k Table 6. Object Detection results on the COCO validation set Results on the classification performance on ImageNet-1k dataset show that the MobileNetV4 achieves the highest performance and smallest latency compareed to other models on various mobile platforms such as CPUs and DSPs of mobile phones. While other models have closely competitive latency with the MobileNetV4 model, their latency is much higher.\nThe effectiveness of MobileNetV4 as backbone networks are tested on the COCO object detection experiment. The number of MACs were set to be similiar, and the Retina framework was used as the object detector. As the same as classification, MobileNetV4 achieves highest performance compared to other mobile-target modles, with the lowest CPU latency. Hence, the ability of MobileNetV4 for mobile devices can be shown.\nConclusion # This paper proposes the MobileNet V4 series, a universal high-efficiency model that operates efficiently across a wide range of mobile environments. By introducing a new Universal Inverted Bottleneck and Mobile MQA layer and applying an enhanced NAS recipe, MobileNet V4 achieves near Pareto-optimal performance on various hardware, including mobile CPUs, GPUs, DSPs, and dedicated accelerators. Additionally, using the latest distillation techniques, it demonstrates cutting-edge performance in mobile computer vision by achieving 87% ImageNet-1K accuracy with a latency of 3.8ms on the Pixel 8 EdgeTPU. The paper also presents a theoretical framework and analysis for understanding the model\u0026rsquo;s universality across heterogeneous devices, providing guidance for future design. "},{"id":10,"href":"/docs/spring24/10_/","title":"10","section":"Spring24","content":" Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length # Authors: Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou Reviewer: Hyunho Kook\n1. Introduction # Recently, Large Language Models (LLMs) have been gaining popularity. The impressive performance and versatility demonstrated by large models above a certain level have started to be utilized in various fields. However, as the size of the models grows, the size of the data that the models are expected to process is also increasing. Examples of this include processing currently open issues by inputting a GitHub repository or translating a large volume of books without losing context. In addition, the ability to maintain context and carry on a conversation for an extended period within a single chat is also sometimes required. The transformer, which is the foundation model of modern LLMs, exhibits vulnerabilities in this regard. Firstly, since it uses KV cache, memory usage increases rapidly as the sequence length grows, and it has a computational complexity proportional to the square of the sequence length.\nTo address this problem, the authors propose a method that inherits and advances MEGA (exponential moving average with gated attention), the predecessor of this paper. The overall contributions are as follows:\nCEMA (Extending Multi-dimensional Damped EMA to Complex Domain), an extension of Exponential Moving Average (EMA) to the complex domain, is proposed. Timestep Normalization, an extension of Group Norm to the timestep domain, is proposed as an alternative to Layer Norm. Normalized Attention, which performs normalization during attention computation, is proposed. 2-hop Residual Connection, which composes residual connections in 2-hop units, is proposed. By employing these methods, the authors have created a transformer architecture that is linear with respect to context length. They have also addressed the issues encountered in the previous research, MEGA, which were (i) low performance and (ii) the need for different architecture structures for each data type or task.\n2. MEGA (exponential Moving avErage with Gated Attention) # 3. Methods # CEMA (Extending Multi-dimensional Damped EMA to Complex Domain) # Timestep Normalization # Normalized Attention # 2-hop Residual Connection # 4. Experiments # 5. Related Works # Mamba\n6. Discussion # In my opinion, there are a few potential limitations that are not extensively discussed in the paper:\nReliance on CEMA for Out-of-Chunk Context: The self-attention mechanism in MEGALODON is applied within each chunk. For data that falls completely outside the chunk boundaries, the model relies solely on CEMA for processing. However, CEMA is a causal mechanism, which means it may struggle to adequately capture the influence of distant future content on earlier parts of the sequence. This limitation could potentially hinder the model\u0026rsquo;s ability to handle long-range dependencies that span across multiple chunks.\nComplexity of the Architecture: Compared to the traditional Transformer layer, the MEGALODON architecture is considerably more complex. It requires the computation of EMA, including the complex domain, for each token. Additionally, several normalization and attention components have been introduced, such as Timestep Normalization, which further increases the complexity of the model compared to the previous works.\nLimited Exploration of Downstream Tasks: While the paper demonstrates the effectiveness of MEGALODON on long-context question answering tasks from the Scrolls dataset, the range of downstream tasks explored is relatively narrow. Evaluating the model\u0026rsquo;s performance on a broader set of tasks, such as summarization, dialogue generation, and composition, would provide a more comprehensive assessment of its capabilities and potential limitations.\nDespite these limitations, MEGALODON presents a promising direction for efficient long-context modeling. In my opinion, this kind of efficent and linear processing of memory can be a breakthrough for long-context LLMs.\n"},{"id":11,"href":"/docs/spring24/11_/","title":"11","section":"Spring24","content":" # "},{"id":12,"href":"/docs/spring24/12_/","title":"12","section":"Spring24","content":" Scaling (Down) CLIP: A Comprehensive # Posted by: Harit Keawmuang, Junkyeong Park\nAuthors: Zichao Li (University of California, Santa Cruz), Cihang Xie (University of California, Santa Cruz), Ekin Dogus Cubuk (Google Deepmind)\nintroduction\nData # Data Quantity # What is CLIP? # CLIP effectively merges the capabilities of natural language processing (NLP) and computer vision. By learning from images and their textual descriptions, CLIP unifies text and image understanding, allowing it to perform various tasks without task-specific training.\nRelated Works # In the recent years, NLP has advanced significantly in pre-training language models on large text datasets. Simultaneously, computer vision has improved by pre-training convolutional neural networks (CNNs) on extensive image datasets. The CLIP model combines these approaches by jointly pre-training on images and text using a contrastive loss function, creating a shared embedding space for both.\nRecent efforts have focused on improving scalability and efficiency of CLIP model. For example, FLIP was introduced to minimize the computation by masking image patches, enabling larger batch sizes without sacrificing performance. Most research has focused on large-scale training with significant computational resources, utilizing ViT large models on extensive datasets. However, less attention has been given to optimizing CLIP for smaller training budgets.\nMethod # Training Pipeline, Dataset, and Hyperparameters # In this paper, they adopted the identical training approach as CLIP, which employs a contrastive loss to simultaneously train the vision and text encoders from scratch. This loss function encourages the encoders to map related image-text pairs to similar feature representations in a shared embedding space by minimizing the distance between positive pairs and maximizing the distance between negative pairs. Key aspects of the training include:\nMinimal data augmentation: Images resized to 224x224 and pixel values normalized to the range of -1 to 1. Optimizer: AdafactorShazeer \u0026amp; Stern with β1 = 0.9 and β2 = 0.999. Batch size: 16k Learning rate: Initial rate of 0.001 with a cosine learning scheduler and weight decay of 0.0001. Evaluation Matrices # Zero-shot transfer evaluation: Assesses the model\u0026rsquo;s ability to generalize to new tasks without fine-tuning. Linear probe evaluations: Freezes the vision encoder and optimizes the fully connected layer\u0026rsquo;s learning rate. Retrieval performance on MSCOCO captions: Ranks text captions based on cosine similarity with image embeddings, reporting Recall@1 for image-to-text retrieval and average results for text-to-image retrieval. Comparison of Network Architectures # To effectively choose the best network architectures, they performed a comparison among the various architectures. Previous studies have explored various vision encoders for CLIP, such as ResNet, MLP-Mixer, and ViT, but some architectures like Swin-Transformer and ConvNext haven\u0026rsquo;t been investigated. Here, they compared CNN and vision transformer architectures with similar computational costs, including ViT-B/32, ResNet-50, ConvNext-T, Swin-T, and Mixer-B/32. In Zero-shot, when considering limited data samples, ResNet-50 performs better initially, but ViT-B/32 achieves superior performance with more samples due to its stronger ability to capture global information (see Figure 1(a)). In linear probing, MLP-Mixer outperforms others with fewer samples, but ViT-B/32 excels with larger datasets. ViT and MLP-Mixer show better robustness, likely due to their lower inductive bias, leading to improved generalization (Figure 1(b)). For retrieval tasks, ResNet-50 is better with smaller sample sizes, but ViT-B/32 surpasses it as sample sizes increase. Mixer-B/32 performs poorly in retrieval tasks, making ViT the preferred choice for CLIP\u0026rsquo;s vision encoder across various tasks.\nFigure\n"},{"id":13,"href":"/docs/spring24/13_/","title":"13","section":"Spring24","content":" Exploring µ-Parameterization in Large-Scale Neural Networks # Paper : A Large-Scale Exploration of µ-Transfer Author : Lucas Dax Lingle produced by Jeonghyun Choi, Minhye Choo Introduction # In the field of artificial intelligence, especially in natural language processing and computer vision, large neural network models have become a cornerstone. However, the initialization and learning rates of these models are often determined through heuristic methods, varying significantly between different studies and model sizes. This inconsistency can lead to suboptimal performance, particularly when scaling up models, resulting in inefficient training processes and less effective models. As models grow larger, the tuning process becomes increasingly costly and time-consuming.\nThe concept of µ-Parameterization (µP) provides a potential solution to this problem. µP offers scaling rules for model initialization and learning rates, enabling zero-shot hyperparameter transfer from small models to larger ones. This technique promises stable training and optimal hyperparameters at scale with minimal cost. Despite its potential, µP has not been widely adopted due to its complexity and the need for further empirical validation.\nFigure 1 : In the default parameterization in PyTorch, the graph on the left, the activation scales diverge in width after one step of training. But in µP, the graph on the right, the activation scales change by a consistent amount regardless of width for any training step. The y-axis shows the change of network activation scales on a fixed input after t=0, 1, 2, 3, and 4 steps of training as the width of the model varies, which is shown along the x-axis. .\nIn this blog post, we delve into the details of µ-Parameterization, its underlying principles, and its practical applications as explored in the paper \u0026ldquo;A Large-Scale Exploration of µ-Transfer\u0026rdquo; by Lucas Dax Lingle.\nWhat is µ-Parameterization? # Concept and Principles # µ-Parameterization (µP) is a set of rules for initializing neural networks and setting learning rates that allows for the seamless transfer of hyperparameters from small proxy models to larger target models. This approach is grounded in a Gaussian Process interpretation of deep neural networks, where the width of the network (number of neurons per layer) is a critical factor.\nThe core idea is to scale the initialization and learning rates based on the width of the network. The general formulation of µP when training with the Adam optimizer and using an i.i.d. Gaussian initialization is as follows:\nInitialization Variance: Parameters are initialized with a variance that scales inversely with the width of the layer. For example, if the width of a layer is M, then the initialization variance for weight matrices WAQ, WAK, WAV is \\(\\frac{1}{M}\\) .\nLearning Rate: The learning rate for each parameter is scaled based on the width of the network. For instance, the learning rate for the same weight matrices WAQ, WAK, WAV is \\(\\frac{αP}{M}\\) , where α is the base learning rate and P is a fixed proxy model width.\nThese scaling rules ensure that the behavior of small and large models remains consistent, facilitating the transfer of optimal hyperparameters across different model sizes.\nPractical Implementation # In practical terms, µP involves specific scaling rules for various components of a transformer model, which is a popular architecture in NLP and computer vision. For example, the initialization variance for the weight matrices in the attention mechanism and MLP blocks is set according to the width of the model. Similarly, the learning rate is adjusted to maintain consistent training dynamics across different scales.\nBelow is an example of µP scaling rules for transformers:\nParameter Initialization Variance Adam Learning Rate WE 1 α WAQ 1/M αP/M WAK 1/M αP/M WAV 1/M αP/M WAO 1/(HD) αP/M WFI 1/M αP/M WFO 1/F αP/M WU 1/M² αP/M Table 1: µP scaling rules for transformers.\nIn this table, M represents the model width, H the number of heads, D the head width, F the hidden width of the MLP, and α the base learning rate.\nµ-Transfer # Figure 2 : Illustration of µ-Transfer\nOne of the significant advantages of µ-Parameterization is the concept of µ-Transfer. This method allows hyperparameters, such as learning rates, found optimal in small models to be transferred directly to larger models without the need for extensive re-tuning. This process is particularly beneficial for scaling models efficiently and maintaining consistent performance across different model sizes.\nSteps in µ-Transfer # Training a Small Proxy Model: Begin by training a small proxy model, which is easier and less expensive to experiment with. Perform hyperparameter tuning on this model to find the optimal learning rate and other hyperparameters. For instance, let\u0026rsquo;s denote the optimal learning rate found for this small model as α.\nScaling the Hyperparameters: Use the scaling rules provided by µP to adapt the hyperparameters for a larger model. The key scaling rule here is that the learning rate should be adjusted based on the ratio of the widths of the large model to the small model. For example, if the small model has a width 128 and the large model has a width 2048, the scaled learning rate for the large model would be \\(\\frac{α2048}{128}\\) .\nApplying the Scaled Hyperparameters: Implement these scaled hyperparameters in the larger model. This involves adjusting the initialization variance and learning rates according to the µP rules to ensure that the training dynamics remain stable and consistent.\nBenefits of µ-Transfer # Efficiency: By using µ-Transfer, researchers can avoid the costly and time-consuming process of hyperparameter tuning on large models. Instead, they can perform this tuning on smaller models and scale the results.\nConsistency: µ-Transfer helps maintain consistent training dynamics across different model sizes. This consistency is crucial for achieving optimal performance as models scale up.\nSimplicity: The process of scaling hyperparameters using µP is straightforward once the initial tuning on the small model is complete. This simplicity can significantly reduce the complexity of managing large-scale model training.\nAblation Experiment # Setup \u0026amp; Objective # The objective of these experiments is to examine how various methods that can enhance performance affect the actual transfer of learning rates from smaller width models to larger models with µP and their impact on performance in reality. While the existing µP aimed at transferring initialization and learning rates, this experiment focused on the learning rate, an important hyperparameter in large transformer models. The experiments were implemented using Jax/Flax on TPU V3, and the optimal learning rate was determined by measuring the validation loss. Models with widths of \\(M\\) = {128, 512, 2048} have parameters ranging from 4.7M to 1.2B, with the depth fixed at L = 24. The reason for focusing solely on width and not depth is that in the case of depthwise µP, only one linear layer is used per residual block, whereas transformers use at least two layers. So, in this experiment, width is the main change to control the # of parameters.\nBaseline \u0026amp; Summary # The baseline represents the experimental results used as a reference for performance improvement or degradation across various experimental settings. In the baseline using µP, it is confirmed that the optimal learning rate for the smallest model is also the optimal learning rate for larger models that are 4x wider (16x larger). The experimental results can be categorized into three main groups:\nTransfer O, Performance Improvement O Cases where both learning rate transfer and performance improvement is observed. Examples: Zero query initialization, Multiplicative Nonlinearities, SP unembedding initialization, Multi-query attention, batch size(4x) Transfer O, Performance Improvement X Cases where learning rate transfer, but there was no improvement in performance. Examples: Projection biases, Embedding normalization, Cosine schedule Transfer X Cases where learning rate does not transfer. Examples: RMS Norm gain, Decoupled weight decay, SP attention scale Cases where learning rate does not transfer are not separately classified with performance improvement, as performance improvement in these cases is not significant.\nProjection Biases # Adding a bias vector to the linear layer does not guarantee an improvement in model performance. In fact, experimental results showed that the performance was similar to that of the baseline and learning rate transfer across the model size and width under µP.\nRMS Norm gain(vector \u0026amp; scalar) \u0026amp; Embedding normalization # RMSNorm is a normalization method that uses the root mean square instead of the mean and standard deviation. \\[\\bar{a_i}=\\frac{a_i}{\\mathrm{RMS(a)}}g_i, \\text{where } \\mathrm{RMS(a)}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^na_i^2}\\] To re-scale the standardized summed inputs, a trainable gain \\(g_i\\) and bias \\(b_i\\) were used. The gain can be implemented in two forms: a vector gain and a scalar multiplier. Similar to projection bias, the use of a trainable gain does not guarantee performance improvement. The results showed that transfer does not occur in any case, vector gain and scalar multiplier, and performance degradation was observed in the models with the largest width. On the other hand, using normalized embedding with RMSNorm without a trainable gain did not improve performance, but it was observed that the learning rate transfer was successful.\nQuery Initialization # For query projection, µP initialization typically uses a Gaussian distribution with variance \\(\\Theta(1/M)\\) , but zero-initialization has been proposed to facilitate transfer. Transfer occurs with zero-initialized query, and there was a slight improvement in performance compared to the baseline, traditional Gaussian initialization.\nCosine schedule # Adjusting the learning rate over iterations is an open problem with no definitive solution, with methods like power and exponential scheduling. This experiment use cosine scheduling, which is the method that periodically decreases and increases the learning rate to prevent convergence to local minima. This approach can help the model escape suboptimal points and potentially find better solutions.The baseline used a linear schedule, but switching to cosine scheduling did not negatively impact transfer. However, a slight performance degradation was observed with cosine scheduling compared to the baseline.\nDecoupled weight decay # When optimizing hyperparameters with Adam, the decoupled weight decay method separates the weight decay from the optimization step, allowing independent exploration of the learning rate and weight decay factor. Experimental results using decoupled weight decay showed that optimal learning rate transfer was not achieved. A large \\(\\lambda\\) value was suggested as a potential cause for this issue. In this experiment, \\(\\lambda = 0.1\\) was used. The smaller difference in optimal learning rates between small and large models, compared to other transfer failures, suggested that reducing \\(\\lambda\\) may help resolve the transfer problem.\nMultiplicative Nonlinearities # To enhance the performance of transformers, multiplicative nonlinearity activation functions such as SwiGLU and Squared ReLU can be utilized. SwiGLU is an activation function created by combining the Swish activation function with the Gated Linear Unit (GLU) and is considered to get better performance compared to traditional activation methods. . The formulas for each are as follows: \\(Swish(x) = x\\sigma (\\beta x)\\) ( \\(\\sigma\\) : sigmoid function, \\(\\beta\\) : trainable parameter ) \\(GLU(x,W,V,b,c) = \\sigma(xW\u0026#43;b)\\otimes (xV\u0026#43;c)\\) ( \\(W,V\\) : trainable tensor, \\(b,c\\) : trainable tensor bias, \\(\\otimes\\) : element-wise multiplication) \\(SwiGLU(x,W,V,b,c, \\beta) = Swish_\\beta(xW\u0026#43;b)\\otimes(xV\u0026#43;c)\\) Similarly, Squared ReLU, which is obtained by squaring the ReLU activation function, is known to help improve performance. Experimental results showed that they allow µ-transfer of the learning rate across model sizes and unlike the RMSNorm gain, there was a performance improvement from the perspective of multiplicative interaction.\nStandard Parameterization # Yang et al. (2021) stateed that µP models perform better than SP models, and our various experiments with SP settings confirmed that some of the µP settings offer advantages in terms of transfer and model performance.\nattention scale Usual attention scaling is \\(\\tau^{-1} = 1/\\sqrt{D}\\) , while µP proposes \\(\\tau^{-1} = \\Theta(1/D)\\) , and baseline experiment was implemented using a simple \\((1/D\\) ) scaling. In this experiment, attention scaling was \\(1/\\sqrt{D}\\) to check the SP setting. For the \\(M = 128\\) model, the optimal learning rate was \\(2^{-8}\\) , but for larger models, the optimal learning rate was changed to \\(2^{-6}\\) . This means that transfer did not occur, and performance slightly deteriorated compared to the original baseline. unembedding initialization The initialization of µP\u0026rsquo;s unembedding matrix follows a Gaussian distribution with a variance of \\(\\Theta(1/M^2)\\) , while standard parametrization (SP) uses \\(1/M\\) . Experiments using the original SP method with \\(1/M\\) showed that transfer was maintained and there was a slight improvement in performance for larger models. To compare the result of the SP and µP, this experiment was implemented using SP and compared the result with baseline's. The differences between the baseline and SP included using trainable biases in linear layers, trainable gains in RMSNorm layers, attention scale \\(1/\\sqrt{D}\\) , and unembedding initialization variance \\(1/M\\) . All other hyperparameters remained the same. The combined results for SP transformers showed that transfer does not occur, and the optimal loss is lower in performance compared to the baseline. Lion Optimizer # The Lion optimizer is known for being more than twice as memory-efficient as Adam while delivering similar performance in transformers. This optimizer restricts updates to \\(\\{-1, 1\\}\\) for each coordinate, yielding a coordinate size of \\(\\Theta(1)\\) per step. Consequently, it seems suitable to use the existing \\(\\Theta(1/M)\\) transfer rule as it is. However, experimental results showed that the learning rate transfer was not successful, indicating the need for further research on the transfer rule.\nMulti-query attention # Transformer LLMs can use multi-query attention and group generalization to increase inference speed by sharing keys/values across multiple heads. Experimental results showed that these methods lead to significant performance improvements compared to other methods, and transfer also occurred effectively.\nBatch Size(4x larger, 4x smaller) # By adjusting the batch size while keeping the number of training tokens constant, it is possible to reduce training time or determine the minimum batch size required for operation. In this case, the learning rate formula is adapted by using twice the specified value for 4x larger batch sizes and half the value for 4x smaller batch sizes. The results showed that learning rate transfer effectively in both cases, though further research is needed to determine the optimal batch size.\nLarge-scale Transfer Experiement # To verify if transfer is possible over a larger scale difference, experiments was implemented by reducing \\(L\\) to 12 and setting the width to \\(\\{128, 512, 2048, 8192\\}\\) , resulting in models with 2M, 40M, 600M, and 10B parameters(5000x). Zero query and Squared ReLU were used, which showed good performance and did not negatively impact transfer. The results confirmed that, despite a 5000x scale difference, the learning rate transfer well. Related Works # To summarize the contributions of the paper and consider the future works, this section summarizes the contents of a relevant research paper and explains its works and limitations.\nTensor Programs V: Tuning large neural networks via zero-shot hyperparameter transfer The performance of µ-Transfer, which reduces computation by tuning a small model and then transferring the obtained hyperparameters to a large model instead of tuning the large parameter model directly, was demonstrated experimentally. Figure : (1) MLP with SP (2) MLP with µP (3) transformer with SP (4) transformer with µP\nThe paper experimentally confirmed that when Standard Parameterization was applied to MLP and Transformer models, the hyperparameter stability was low. However, using µ Parameterization enabled stable transfer by using width-128 network and width-8192 network. The experiement sweeped the width and depth by changing only one of the hyperparameters—learning rate, output weight multiplier, initialization standard deviation, and learning rate schedule to check if they transfer across scale dimensions. Optimal hyperparameters transfered well across scale dimensions when the minimum width, depth, batch size, sequence length, and training steps were met. However, unlike the changes in width where hyperparameter transfer occured to wider models, the transfer to deeper models was less effective. The reason is same as the main paper's fixed depth and the experiments in this paper also adjusted the scale by fixing the depth and only varying the width. In this paper, experiments were conducted to evaluate the efficiency and performance of µ-Transfer using various models: from a 10M to 40M Transformer on IWSLT14 De-En, from a 15M to 211M Transformer on WMT14 En-De, from a 13M to 110M BERT and to 350M BERT-large. The largest model tested was the GPT-3, where the width is shrunk from 4096 to 256, resulting in a scale difference from 40M to 6.7B, representing a 168x scale difference. The total tuning cost was only 7% of total pretraining cost and the hyperparameter was stable across the scale. Contribution of the main paper : The performance of µ-Transfer, which has been proven effective with a scale difference of up to 168x, was experimentally confirmed to be reliable even with a scale difference of up to 5000x. The paper also conducted experiments not only comparing SP (Standard Parameterization) and µP (µ Parameterization) overall but also by changing specific elements such as attention scale.\nConclusion # The paper demonstrates that the transfer properties observed with µ-Parameterization (µP) can be maintained across most scenarios. µP outperforms standard parameterization (SP) and confirms the efficacy of part-by-part transfer for elements like attention scale and unembedding initialization, validating µP\u0026rsquo;s superiority. Additionally, it shows that transfer is feasible for models ranging from 2M to 10B parameters (5000x), suggesting applicability to larger models. However, some issues are identified where optimal learning rate transfer does not occur, or there is a performance decline in large models. For instance, trainable RMSNorm gain and decoupled weight decay do not function properly for learning rate transfer. Although transfer is observed with projection biases and cosine scheduling, there is no performance improvement or even a decline. The impressive performance of µ-Transfer demonstrated in this paper is expected to be highly beneficial in the current AI landscape, where model sizes are continually increasing. However, tuning hyperparameters for large models is not always feasible, indicating a need for further research. The suggested further research areas are as follows:\nDetailed explanation and analysis of each experiment, investigating the causes of transfer failures and potential solutions through the expansion of each experiment (e.g., experiments on specific SP elements, experiments with various batch sizes). Scaling adjustment through depth rather than width and exploring its transferability. Transferability of other hyperparameters. Personal Insights and Future Directions # The significance of this paper lies in summarizing the impact of various methods on performance and transfer under µ through ablation experiments. However, it is limited by focusing on many ablations without additional experiments for detailed results, leading to a lack of comprehensive analysis. For instance, where transfer fails, minor differences in optimal learning rates between small and large models suggest potential for improvement, but no further experiments were conducted. Structural issues in transformers make scaling adjustments through depth challenging, and research on architectures that allow adjustments through both depth and width could increase flexibility. Additionally, expanding the scope of transfer to include other hyperparameters like the output weight multiplier, initialization standard deviation, and learning rate schedule is necessary.\nWe also believe µ-Parameterization offers a promising solution to one of the significant bottlenecks in training large neural networks. Its potential to standardize and simplify the hyperparameter tuning process is a substantial step forward. However, the complexity of its implementation and the need for more empirical validation remain hurdles to its widespread adoption.\nTo fully realize the benefits of µP, the following future research directions are proposed:\nEmpirical Validation: Conduct large-scale studies across various types of neural networks and applications to validate µP’s effectiveness and identify any limitations. Tool Development: Create user-friendly tools and libraries that automate the µP scaling process, making it more accessible to practitioners. Hybrid Approaches: Explore combining µP with other optimization methods like LAMB to further enhance training efficiency and performance. By addressing these future research directions, the full potential of µ-Parameterization can be unlocked, paving the way for more efficient and effective neural network training methodologies.\nReferences # Lingle, L. D. (2024). A Large-Scale Exploration of µ-Transfer. arXiv preprint. Retrieved from arXiv:2404.05728. Yang, G., \u0026amp; Hu, E. J. (2021). Tensor Programs V: Tuning large neural networks via zero-shot hyperparameter transfer. Advances in Neural Information Processing Systems. Biao Zhang and Rico Sennrich. Root mean square layer normalization. CoRR, abs/1910.07467, 2019. URL http://arxiv.org/abs/1910.07467. Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. CoRR, abs/1711.05101, 2017. URL http://arxiv.org/abs/1711.05101. Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/abs/2002.05202. Noam Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019. URL https://arxiv.org/abs/1911.02150. "},{"id":14,"href":"/docs/spring24/14_/","title":"14","section":"Spring24","content":" # "},{"id":15,"href":"/docs/spring24/15_/","title":"15","section":"Spring24","content":" # "},{"id":16,"href":"/docs/spring24/16_/","title":"16","section":"Spring24","content":" Accelerating Transformers via Conditional Computation: As Aspect of Mixture of Depths # Posted by: Inkwan Hwang, Minjae Park\nThis image was generated by DALL·E 3. Introduction # “Choice and concentration” is an effective strategies for achieving success in problems. Sometimes, it is not necessary to consume same amount of effort and time into all problems. Expending energy on trivial issues may fail to concentrate on what truly matters. Similarly, in language models, there is a technique that does not focus equally on all tokens but allocates less budget to non-essential tokens. This technique is called conditional computation.\nIn this post, We will explain conditional computation strategies for Transformers, focusing on a technology announced this year called Mixture-of-Texture.\npaper: Mixture-of-Depths: Dynamically allocating compute in transformer-based language models Let\u0026rsquo;s dive in!\nUnderstanding the problem: Uniform computation in Transformers # These days, most language models are based on Transformers, and we stack these blocks to make big models. When given an input sequence, tokens pass through these blocks to predict the next token. The problem is that the models spread computations uniformly across input sequences. Transformers use the same amount of computation for essential tokens as for non-essential ones. For instance, predicting a token within a sentence is cheaper than predicting the first token of the next sentence. Researchers want to address this issue by making Transformers focus on important tokens by allocating less computing resources.\nConditional computation for Transformers # Early exiting Instead of passing through all layers, the model can stop early if it is confident enough about its prediction. This saves computation time and resources. Large pre-trained models like BERT can use early exiting ot maintain performance while reducing computational load. CoLT5\nMixture of Experts (MoE)\nMoE is an model which consists of parallel expert models which is fitted to certain domains. Like MoD, token-level routing decisions are made across the network depth. Difference between MoD is, MoD chooses path to transformer or to residual connection, MoE chooses path to transformer(Expert) or to transformer(Expert) or both.\nOverview to Mixture-of-Depths (MoD) # Self-attention + MLP, Residual Connection 중 고른다는 내용. MoE는 넓이를 줄였지만, MoD는 깊이에 해당한다는 내용.\nMoE is an model which consists of parallel expert models which is fitted to certain domains. Like MoD, token-level routing decisions are made across the network depth. Difference between MoD is, MoD chooses path to transformer or to residual connection, MoE chooses path to transformer(Expert) or to transformer(Expert) or both.\nRouting schemes # Routing implementation is the most crucial part of MoD. The authors compare three routing schemes, demonstrating that MoD is an efficient approach.\nToken-choice routing # Token-choice routing is a method where each tokens select the path it will follow. The router produces probability distributions for each token across the computational paths. Based on this distribution, each token chooses its preferred path at each layer.\nIn token-choice routing, tokens have the flexibility to select their path, allowing for dynamic processing. However, this can lead to path balancing issues as all tokens might preger on the same path. It causes potential overloads on specific paths. To mitigate it, auxility loss is used to ensure that most tokens do not prefer on a single path.\nExpert-choice routing # Expert-choice routing is the reverse of token-choice routing. Similar to token-choice routing, the router produces a probability distribution for each token. In expert-choice routing, instead of tokens selecting their paths, each path selects the top-k tokwns based on the tokens\u0026rsquo; preferences.\nUsing this method ensures that each paths receives k tokens, maintauing balance among the paths. However, some tokens may not be selected beacuse there might be common tokens that multiple paths prefer.\nExpert-choice MoD # This method applies expert-choice routing but uses only a single expert. Since only a single path is utilized, if $k$ is less than the sequence length, not all tokens need to undergo self-attention and MLP computation.\nRouting scheme에는 다음과 같은 고려해야할 사항이 있다:\n연산 효율성 Auxiliary balancing loss가 필요 없다 구현의 단순함 Router의 weight 순서대로 가장 큰 것을 고르면 된다. 명확한 기준 top-k 연산이 router weight의 magnitude에 depend하기 때문에 가장 중요한 토큰이 연산되는 것을 보장할 수 있다. 토큰에게 주어진 경우가 두 가지이므로(slef-attention + MLP, residual connection) top-k는 명확하게 token을 두 set으로 나눌 수 있다. 다음과 같은 이유로 저자들은 Expert-choice routing을 사용하기로 하였고, single path만을 사용하기로 하였다.\nImplementation # MoD Transformers는 다음과 같은 방식으로 작동한다.\nCalculate routing weights \\[x^{l\u0026#43;1}_i=\\begin{cases}r^{l}_i f_i(\\tilde{X}^l)\u0026#43;x^{l}_i, \u0026amp; \\text{if } r^{l}_i \u0026gt; P_\\beta(R^l)\\\\x^{l}_i, \u0026amp; \\text{if }r^{l}_i \u0026lt; P_\\beta(R^l)\\end{cases}\\] ssss backward path 및 오차보정 More details # Capacity # Autoregressively sampling # The issue with autoregressively sampling is that it lacks information about future tokens, which means it cannot determine whether a token will be in the top-k when it passes through the router. Therefore, to solve this problem, the paper proposes two methods.\nSimple auxiliary loss The first method is introducing an auxiliary loss. By designing an additional binary cross-entropy loss function at the router's output, the value of tokens in the top-$k$ is guided to be greater than 0.5, while the value of tokens are not in the top-$k$ is guided to be less than 0.5. Through its process, when the token passes through the router, it is considered to be in the top-$k$ if its value is higher than 0.5, and it passes through the self-attention and MLP layer. Otherwise, it passses through the residual path. Designing such a function impacts the primary language modeling objective about 0.2-0.3%. We believe this likely refers to the extent to which performance and inference time are affected. Small auxiliary MLP predictor\nThe second method does not affect the primary language modeling objective at all. The authors design a new MLP layer that functions as a binary classifier to determine wheather a token is in top-$k$ during the training process. This classifer is trained to make these demterminations, and it is used in real-time during the autoregressive sampling process.\nWith these methods, authors can sample autoregressively by choosing to route tokens to or around a block based on the router\u0026rsquo;s outer which is not depends on the future tokens. They provide empirical result that auxiliary task achieved 99% accuracy.\nOpen source MoD (not official) # The followuing is an implementation of MoD that supports various LM such as Mixtral, LLama3 and BLOOM. It implements MoD using PyTorch and Hugging Face Transformers library.\nLINK: https://github.com/astramind-ai/Mixture-of-depths\nThe code operates in the following steps:\nToken Weight Calculation\nThe TokenRouter module caculates weights for each token based on its embedding. This is done using a lnear layer appleid to the embeddingsm resulting in a weight value for each token.\nSelective Processing\nThe processing occurs in the MoD module\u0026rsquo;s forward pass\nFirst token weights are calculated using TokenRouter By a capacity paratmter, the number of tokens are determined. They undergo self-attention and MLP computation. Application to Hugging Face Models\napply_mod_to_hf function applies the MoD mechanism to an existing Hugging Face model.\nMore detail explanations are HERE. 개인 페이지에 더 자세한 코드 분석 작성.\nResults # 실험결과\n## **Conclusion and discussion** 결론론 + 내 생각 Some resources # 참고문헌 정리\n"},{"id":17,"href":"/docs/spring24/17_/","title":"17","section":"Spring24","content":" QuaRot : Outlier-Free 4-Bit Inference in Rotated LLMs # Author : Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li\nPosted by MyeongJi Yun, JungGyu Min, POSTECH\nThis post assumes that the reader has a structural understanding of Transformer and Llama models. If you need a detailed understanding of these models, please refer to the Transformer, LLaMa.\nLarge Language models ( LLMs ) like GPT-2, LLaMa have become increasingly important due to their countless applications. However, their inference requires a significant amount of computation, memory, and energy. Quantization is among the most important techniques to solve both memory and compute issues in LLM inference.\nOutlier makes quantization difficult # Recent research has shown that LLMs have large outliers and make quantization more difficult, especially in 4-bit case. Also, they mentioned that the activations have more outliers, which makes quantization harder. There are three main streams to solve this problem.\nWeight only quantization LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale, 2022 NeurIPs Weight quantization can ease the memory budget for saving the model. However, since activations are not quantized, the computation still involves integer and float operations, making it difficult to address compute issues. Remain outlier in higher bitwidth QUIK: Towards End-to-End 4-Bit Inference on Generative Large Language Models, 2023 Weight quantization can ease the memory budget for saving the model, and since most operations are integer X integer, compute issues are largely resolved. However, some operations still involve integer X float, and the occurrence of float values is irregular, leaving some compute issues unresolved. Use calibration set and normalize activation SmoothQuant: Accurate and Efficient Post-Training Quantization for LLM, 2023 ICML Accuracy is guaranteed up to 8-bit quantization, but it is not assured with 4-bit quantization. In “ QuaRot : Outlier-Free 4-Bit Inference in Rotated LLMs”, the author introduces a new method for quantizing LLM models end-to-end, by utilizing “computational invariance” to all weights and activation and optimizing the computing process.\nRandom Hadamard transform doesn’t change the result # In the concept of computational invariance theorem, small changes in input parameters do not cause the output difference if the algorithm is stable. When applying this to a transformer-based large language model (LLM), it implies that rotating the coordinate system of activations between weight and computation blocks using an orthogonal matrix does not alter the model\u0026rsquo;s output. According to this theory, instead of using any matrix X that constitutes the transformer, you can use X′=UXV where U and V are orthogonal matrices, and the computational results will remain unchanged.\nIf the number or proportion of outliers in 𝑋′ is less than that in 𝑋, the information loss during quantization can be reduced. QuIP demonstrates that multiplying a matrix by orthogonal matrices on both sides reduces the value of max⁡(𝑋)/mean(𝑋). This means that the presence of extreme values relative to the average is diminished, leading to a more uniform distribution of values within the matrix. However, performing 𝑈𝑋𝑉 also incurs overhead, so selecting orthogonal matrices 𝑈 and 𝑉 that minimize this overhead is essential.\nQuaRot uses Random Hadamard transformation because the result PPL is lower, so random Hadamard transformation is better than random matrix.\nLLama2-7B LLama2-7B LLama2-7B QuaRot ( Random ) 7.45 5.84 4.07 QuaRot (Hadamard) 6.10 5.40 3.79 Random Hadamard transformation matrix H is described below : \\[ H_{2} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{bmatrix}, \\quad H_{2^n} = H_2 \\otimes H_{2^{n-1}} \\] \\[H\u0026#39; = H \\cdot \\mathrm{diag}(s), \\quad s \\sim \\mathrm{Uniform}(\\{-1, \u0026#43;1\\})\\] This transformation pairs elements to perform simultaneous computations, allowing the matrix-vector multiplication between matrix 𝐻 and vector 𝑥 to be executed using only 𝑂(𝑑log⁡𝑑) addition operations without any multiplications, as illustrated below:\n--- QuaRot demonstrates that using this technique reduces the number of outliers. By applying the random Hadamard transformation, the distribution of activations is more uniform, which decreases the number of extreme values or outliers, thereby minimizing information loss during quantization.\nStep by Step modification and quantization # Step 1 involves applying the new schemes proposed by QuaRot to significantly reduce outliers in weights and activations, thereby minimizing accuracy degradation due to quantization held in Step 2. The key technique is to apply the Hadamard transform to each activation and weight in both attention blocks and FFN. This is done by merging operations through the use of two different Hadamard transform matrices across consecutive layers, creating an optimal computational flow.\nStep 1-a. Weight Modification # Note that the multiplication of two orthogonal matrices generates identical matrix, so inserting Q and Q^T between linear layers doesn’t change any output.\n\\[I = Q Q^T, XQQ^TW = XW\\] Considering LayerNorm or RMSNorm at the start of the transformer multiplying some orthogonal matrices does not change output. Also, we can fuse the scaling operation of RMSNorm’s : diag(a) into an adjacent weight matrix.\n\\[RMSNorm(X) = x_i \\leftarrow \\frac{x_i}{||x_i||} = ( \\frac{x_i * Q}{||x_i||} ) Q^T = RMSNorm (XQ^T)Q\\] So for all weights after the RMSNorm layer, the weight becomes :\n\\[W \\leftarrow Q^T diag(a) W, Q = Hdiag(s)\\] Step 1-b. Rotate FFN # Inserting online Hadamard operation can ease the activation value’s quantization difficulty within each block. This operation is implicitly reserved by fusing a Hadamard matrix into the next matrix of the network.\nStep 1-c. Attention Value Projection # This step applies Hadamard transformations to the value and output projection matrices in the attention block throughout both offline weight modification and online activation transformation. Since value and output projection weight are multiplied in each head, two matrices can be transformed using the Hadamard matrix without changing the result of attention.\n\\[W_v^{(h)} \\leftarrow W_v^{(h)}H_{d_h} \\\\ W_{out}^{(h)} \\leftarrow H_{d_h} W_{out}^{(h)} \\] This transformation can be represented with Kronecker multiplication in the point of full attention computation view.\n\\[W_v \\leftarrow W_v(I\\otimes H_{d_h})\\\\W_{out}\\leftarrow (I\\otimes H_{d_h}) W_{out} \\] The following simple lemma defines the remaining Hadamard operation after modification.\n\\[H_{a\\times b}= (I\\otimes H_{b}) (H_{a}\\otimes I )\\] This defines the remaining Hadamard operation as the later term of the upper lemma, which results in a modification of the online forward path.\n\\[Z \\leftarrow Z(H_{n_h} \\otimes I)\\] Step 1-d. Key Rotation # This step applies Hadamard transformation to the key vectors in the attention module. Utilizing the RoPE method (Su et al., 2021), the positional encoding is directly attended to query and key vectors. This reshapes the attention score computation equation into a modification-convenient form.\n\\[\\text{Score}=\\text{Softmax}(\\alpha \\text{Pos}(Q_h) \\text{Pos}(K_h^T)\\odot M)\\] The Hadamard transformation is applied to both position encoded query and key vectors similar to step 1-c.\n\\[\\text{Pos}(Q) = \\text{Pos}(XW_q) \\leftarrow \\text{Pos}(XW_q)(I\\otimes H_{d_h})\\\\\\text{Pos}(K) = \\text{Pos}(XW_k) \\leftarrow \\text{Pos}(XW_k)(I\\otimes H_{d_h}) \\] Note that this transformation can be applied without changing final attention scores since both queries and keys are rotated, therefore no remaining Hadamard transformation exists.\nStep 2 involves applying various state-of-the-art techniques to quantize weights and activations.\nStep 2-a. Weight Quantization # You can quantize the adjusted weights using GPTQ, or you can use a very simple round-to-nearest (RTN) technique. The paper have shown simpler method(RTN) have shown a slight sacrifice in accuracy.\nStep 2-b. Online Quantization # To quantize the activations, find the scale factor for each row (max(row) / 7), then divide all values by the scale factor and convert them to the nearest 4-bit integer. For dequantization, multiply the 32-bit integer output of GEMM by the scale factors of both the activation and the weight, and convert the result to FP16.\nStep 2-c. Quantized Attention # The significance of storing in 4-bit is greater than performing calculations in 4-bit because attention operations are memory-bound. Thus, to compute attention, keep the query, key, and value in FP16 and use Flash Attention for the softmax computation.\nQuaRot saves runtime \u0026amp; memory # As highlighted in the contributions of the paper, this model demonstrates that it maintains accuracy even with 4-bit quantization, achieving the same level of accuracy as other models with significant computation overhead. Additionally, this paper presents results across various model sizes(7B to 70B) and different tasks(PIQA(PQ), ARC-e(A-e), ARc-c(A-c), HellaSwag(HS), Winogrande(WG), LAMBADA(LA) ), demonstrating that as the model size increases, the quantization error compared to FP16 decreases for all tasks. Regarding the Llama-1-7B model\u0026rsquo;s 4-bit quantization situation, which exhibits the largest difference from the FP16 model, we compared it with other recent papers not mentioned in the original study. It is evident that QuaRot, which has lower computational cost, outperforms the generally best-performing QAT and OmniQuant, which involves some additional training on top of SmoothQuant, in 4-bit quantization. Despite this low cost, QuaRot has the smallest inference accuracy difference from the FP16 model, making it a highly effective quantization technique. Moreover, while the original SmoothQuant may have lower computational cost at the same bandwidth due to its simplicity, as shown in the table below, its inference accuracy in 4-bit quantization is so poor that it necessitates the use of 8-bit, making comparisons with QuaRot unnecessary. The key point of QuaRot is that the process of performing the Hadamard transform for quantization to INT4 should not introduce a large overhead compared to the computational benefits gained from converting to INT4. From the perspective of the runtime of the FFN block, it has been confirmed that the overhead remains minimal regardless of layer size, model size, or batch size. Additionally, the memory saving factor ranges from x3.48 to x3.71, which is very close to the ideal value (4 = FP16 / INT4), demonstrating significant efficiency. This paper is particularly noteworthy for addressing the issue of memory overhead in long sequence scenarios by quantizing the KV cache as well. Discussion and future work direction # Why we limited to symmetric INT4 qunatization?\nNumerous papers discuss the limitations of using symmetric quantization in INT4 format for quantization. For example, ANT demonstrate that, even with the same bitwidth, numeric formats like flint and PoT(power of Two), which divide the representation into exponent and mantissa, can achieve better accuracy due to their ability to represent a wider range of values. In the figure below, the INT-4bit example uses only integers, while the others utilize new data formats. It is evident that the Mean Squared Error (MSE) significantly decreases with these new formats.\nQuaRot considers INT4 format for both weight quantization and activation quantization, likely because modern GPUs support efficient operations with INT4 and INT8 formats. If we could use other formats, it might be possible to maintain accuracy even with formats as small as 3-bit, leading to greater memory savings. However, maintaining computational simplicity is challenging because GPUs are not optimized for operations with custom data types, unlike INT4. Therefore, achieving optimal computation with custom data types would require the development of custom hardware.\nToward Quantization + Pruning\nOne of the authors, Dan Alistarh, has papers on GPTQ and OBS. GPTQ focuses on reconstructing matrices after quantization, while OBS deals with reconstructing models after pruning. Both papers share a common foundation in using the Hessian matrix and employ various optimization techniques such as Wood-Fisher. Combining these two approaches, the OBC study explores methods to preserve the accuracy of networks that undergo both pruning and quantization. SliceGPT similarly achieves effective pruning by employing the concept of computational invariance when multiplying orthogonal matrices. By analyzing the properties of orthogonal matrices in both QuaRot and SliceGPT, I believe it is possible to achieve quantization and pruning simultaneously. Nonlinear layer Quantization\nThis paper discusses performing quantization in an end-to-end manner. However, it lacks detailed explanations regarding operations in layers known to require higher bitwidth, such as the input to the softmax function, gelu, and residual operations in layer normalization. Therefore, future research could potentially extend this approach to include all these operations using only low-bitwidth integer calculations. How to reduce the overhead of online Hadamard transformation\nThe forward path in QuaRot mostly follows the activation-quantized LLM tasks like GPTQ, yet requires the additional task of online Hadamard transformation on attention activation. The online Hadamard transformation can be performed by utilizing existing computational resources by converting the task into a matrix-multiplication form, or tossing the task to a dedicated hardware accelerator. Either way have an optimization point of acceleration, where data scheduling of the Hadamard transformation matrix into GEMM task accelerator, or utilizing various previous works about hardware accelerator Hadamard transformation with dedicated dataflow. "},{"id":18,"href":"/docs/spring24/18_/","title":"18","section":"Spring24","content":" ViTAR: Vision Transformer with Any Resolution # Authors: Qihang Fan, Quanzeng You, Xiaotian Han, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang\nVision Transformers (ViTs) # Vision Transformers (ViT) has recently emerged as a competitive alternative to Convolutional Neural Networks (CNNs) that are currently state-of-the-art in different image recognition computer vision tasks.\nVision Transformers (ViT) is an architecture that utilizes self-attention mechanisms to process images. The Vision Transformer Architecture consists of a series of transformer blocks. Each transformer block consists of two sub-layers: a multi-head self-attention layer and a feed-forward layer.\nThe self-attention layer calculates attention weights for each pixel in the image based on its relationship with all other pixels, while the feed-forward layer applies a non-linear transformation to the output of the self-attention layer. The multi-head attention extends this mechanism by allowing the model to attend to different parts of the input sequence simultaneously.\nViT consists of the following steps.\nSplit an image into patches (fixed sizes) Flatten the image patches Create lower-dimensional linear embeddings from these flattened image patches Include positional embeddings Feed the sequence as an input to a state-of-the-art transformer encoder Pre-train the ViT model with image labels, which is then fully supervised on a big dataset. Fine-tune the downstream dataset for image classification The transformer\u0026rsquo;s encoder has a structure in which L transformer blocks sequentially pass through the Feed Forward, which consists of the Normalization Layer, Multi-head Attention, Normalization Layer, and MLP, as shown on the right of Figure 1.\nChallenge: Multi-Resolution ViT Modeling # Shortcoming of ViT is revealed when receiving multi-resolution images as input. There are limits to its application in actual use environments because ViT cannot process images of various resolutions well.\nThe most common method used to address this problem is to apply interpolation to positional encoding before feeding it into the ViT. This approach allows for some compensation of positional information even when the input resolution changes. However, this method has shown significant performance degradation in image classification tasks.\nRecently, ResFormer proposed adding depth-wise convolution to the existing positional encoding method when performing global-local positional embedding, enabling it to work well even with unseen resolutions. (Chu et al., 2023; Tian et al., 2023).\nHowever, ResFormer has three drawbacks.\nShows high performance only in a relatively small range of resolutions (Degradation significantly when resolution is greater than 892) It cannot be used with self-supervised learning methods like masked auto-encoding (MAE). Computation cost increases as input resolution increases, which has a negative impact on the training and inference process. ViTAR: Vision Transformer with Any Resolution # In this section, we introduces two key innovations to address this issue. Firstly, we propose a novel module for dynamic resolution adjustment, designed with a single Transformer block, specifically to achieve highly efficient incremental token integration. Secondly, we introduce fuzzy positional encoding in the Vision Transformer to provide consistent positional awareness across multiple resolutions, thereby preventing overfitting to any single training resolution.\n1. Adaptive Token Merger (ATM Module) # The ATM Module separates input tokens in the form of a grid of $G_h \\times G_w$. When the size of all tokens is $H \\times W$, all tokens are separated in grid form to have tokens of $G_{th}\\times G_{tW}$ size. Each Grid is processed through a special operation called Grid Attention. Grid Attention is carried out only between tokens within the Grid. Average Pooling of all Tokens is performed as a Query, and Attention operation is performed by setting each Token as Key and Value. When this is performed for the entire Grid, it is reduced to $G_h \\times G_w$, which is equal to the number of Grids. Afterwards, it passes through the FeedForward network and repeats. Through this iterative process, even when the resolution of the image is large, the number of tokens can be effectively reduced, and through a sufficient process, this size can be reduced to the size of the grid of 1. This has the advantage of being computationally efficient because when performing the subsequent MHSA calculation, a token of the same size is always input as input, regardless of resolution.\nIn our opinion, Grid Attention appears to add an inductive bias similar to Convolution. It appears that Tokens in adjacent locations in the actual image should be contained within the same Grid. The order of grid patching may have an effect.\n2. Fuzzy Positional Encoding (FPE) # Existing ViT Models generally use learnable positional encoding or sin-cos positional encoding. However, they do not have the ability to handle various input resolutions because these methods are sensitive to input resolution. In response to this, ResFormer attempted to solve this problem through convolution-based positional embedding.\nHowever, convolution-based positional embedding is not suitable for use in self-supervised learning such as masked auto-encoding (MAE). This is because the method can extract and utilize the complete spatial feature only if it has all adjacent patches, but in the case of MAE, some of the image patches are masked. This makes it difficult for the model to conduct large-scale learning.\nFuzzy Positional Encoding(FPE) differs from the previously mentioned methods. It enhances the model\u0026rsquo;s resolution robustness without introducing specific spatial structures like convolutions. Therefore, it can be applied to self-supervised learning frameworks. This property enables ViTAR to be applied to large-scale, unlabeled training sets for training, aiming to obtain a more powerful vision foundation model.\nInitially, the learnable positional embedding is randomly initialized and used as the model\u0026rsquo;s positional embedding. At this time, FPE provides only fuzzy positional information and experiences changes within a certain range. Specifically, assuming that the exact coordinates of the target token are (i, j), the fuzzy positional information is (i + s1, j + s2). s1 and s2 satisfy -0.5 ≤ s1, s2 ≤ 0.5 and follows uniform distribution.\nDuring training, randomly generated coordinate offsets are added to the reference coordinates during the training process, and grid samples for learnable location embeddings are performed based on the newly generated coordinates to generate fuzzy location encoding.\nIn case of inference, precise positional encoding is used instead of FPE. When there is a change in input resolution, interpolation is performed on learnable positional embedding. This has strong positional resilience because it was somehow seen and used in the FPE used in the training phase.\nExperiments # Result of Image Classification Task # ViTAR is trained on ImageNet-1K form scratch and it demonstrates excellent classification accuracy across a considerable range of resolutions. Especially, when the resolution of the input image exceeds 2240, ViTAR is capable of inference at lower computational cost. In contrast, traditional ViT architectures (DeiT and ResFormer) cannot perform high resolution inference due to computational resource limitations.\nResult of Object Detection Task # Effect of Adaptive Token Merger (ATM) Module # Effect of Fuzzy Positional Encoding (FPE) # "},{"id":19,"href":"/docs/spring24/19_/","title":"19","section":"Spring24","content":" # "},{"id":20,"href":"/docs/spring24/20_/","title":"20","section":"Spring24","content":"##Background\n"},{"id":21,"href":"/docs/spring24/21_/","title":"21","section":"Spring24","content":" A Unified Framework for Model Editing # Authors: Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli\nPosted by Donggeun An, Jonghyun Chae\nIntroduction # In the rapidly evolving field of artificial intelligence and machine learning, keeping large language models (LLMs) up-to-date with the latest information is crucial. This paper presents a comprehensive framework, Equality-constrained Mass Model Editing for Transformers (EMMET), that integrates two major model editing techniques: Rank-One Model Editing (ROME) and Mass Editing Memory in Transformer (MEMIT). The proposed framework focuses on the retention-memory objective, which aims to inject new knowledge into the model while maintaining the fidelity of existing information.\nFigure 1. A diagrammatic representation of the preservation-memorization objective. Step 1: Find Keys to Preserve Identify key vectors \\(k_0\\) representing existing knowledge, ensuring they remain intact by processing with the weight matrix \\(W_0\\) to produce output vectors \\(v_0\\) . Step 2: Find a Fact to Memorize\nLocate new information to be added, represented by key vector \\(k_e\\) and output vector \\(v_e\\) , ensuring the model generates the correct new fact. Step 3: Update Weight Matrix\nModify \\(W_0\\) to \\(\\hat{W}\\) , preserving existing key vectors \\(k_0\\) while ensuring \\(k_e\\) produces \\(v_e\\) , thus integrating the new information accurately. Model Editing Evaluation Metrics # The success of model editing is measured using standard metrics.\nEfficacy Score (ES): Indicates if an edit has been successfully made to a model. It is measured as the percentage of edits where the probability of the new fact is greater than the probability of the old fact for a given query prompt. Paraphrase Score (PS): Represents the generalization ability of the model under an edit. It measures the percentage of edits where the probability of the new fact is greater than the probability of the old fact for paraphrases of the query prompt. Neighborhood Score (NS): Represents the locality of model editing. It measures whether editing a fact affects other facts stored inside a model. NS indicates the percentage of facts in the neighborhood of the edited fact that remain unaltered post-edit. Generation Entropy (GE): Represents the fluency of a model post-edit. It is calculated by measuring the weighted average of bi-gram and tri-gram entropies of text generated by an edited model. This value drops if the generated text is repetitive, a common failure case of model editing. Score (S): A composite metric defined to represent a combination of edit success, generalization, and locality. It is the harmonic mean of ES, PS, and NS. ROME and MEMIT: Overview # Figure 2: Figure shows a diagrammatic representation of a transformer layer. The layer being edited by ROME, MEMIT and EMMET is the projection weight matrix inside the MLP layer ( \\(W_{proj}\\) ). To further understand how model editing techniques like ROME, MEMIT, and EMMET work, it's essential to look at how they interact with the layers of a transformer model. Input Representation ( \\(h_{l-1}\\) ): The input to the transformer layer, \\(h_{l-1}\\) , is either the output from the previous layer or the initial input embedding. Attention Mechanism (Attn): The input \\(h_{l-1}\\) passes through the attention mechanism, which calculates attention scores and generates a context vector by attending to different parts of the input sequence. Feed-Forward Layer: The transformed input then goes through the feed-forward layer, consisting of a fully connected layer ( \\(W_{fc}\\) ) producing an intermediate representation, followed by a non-linear activation ( \\(\\sigma\\) ) like ReLU or GELU. Key Vector Generation ( \\(k\\) ): After the non-linearity, the intermediate representation is used to generate key vectors \\(k\\) , crucial for storing and retrieving the model\u0026rsquo;s knowledge. Projection Weight Matrix ( \\(W_{proj}\\) ): The projection weight matrix \\(W_{proj}\\) projects the key vectors into the final output space and is the focus of edits in ROME, MEMIT, and EMMET. Output Vector Generation ( \\(v\\) ): The projection weight matrix \\(W_{proj}\\) transforms the key vectors \\(k\\) into output vectors \\(v\\) , integrating the edits made to the model. Layer Output ( \\(h_{l}\\) ): The final output of the transformer layer, \\(h_{l}\\) , serves as the input for the next layer or as the model\u0026rsquo;s final output if it is the last layer. ROME (Rank-One Model Editing) # ROME is a method that facilitates direct modification of model parameters to incorporate new factual knowledge or modify existing information. ROME works by enforcing equality constraints that ensure precise alignment between the output of the updated model and the intended new knowledge. This method uses first-order updates to model parameters. This is expressed mathematically as adding a single outer product of the two vectors to the existing weight matrix. This approach is highly targeted, modifying the weights in a way that exactly matches the new facts with minimal changes elsewhere, making it ideal for precision-critical applications. ROME is effective for single edits or small batches, but because the method strictly adheres to equality constraints, it does not scale well for large edits, potentially leading to inefficiencies or long computation times in batch scenarios.\nMEMIT (Mass Editing Memory in Transformer) # MEMIT is designed for batch updates and is known for its flexibility and scalability in model editing tasks. Unlike ROME, MEMIT uses a least-squares constraint that provides more flexibility in how edits are implemented. This method optimizes a relaxed objective where the goal is to minimize the overall error across a batch of edits rather than achieving exact matches for each individual update. MEMIT’s strength lies in its ability to handle large batches of edits simultaneously, making it particularly useful for applications that require frequent and extensive updates to the stored knowledge. The algorithm adjusts the model\u0026rsquo;s parameters by calculating a closed-form solution that distributes the edits across the parameters in a way that balances the introduction of new facts with the preservation of existing knowledge.\nTable 1. Comparison between ROME and MEMIT when editing only a single layer for CounterFact dataset. The comparison between ROME and MEMIT reveals that both techniques are highly effective at model editing, with each having its strengths. ROME generally excels in generalization and efficacy, while MEMIT performs slightly better in maintaining locality and fluency, especially for larger models like Llama-2. EMMET (Unifying ROME and MEMIT) # Introducing EMMET # EMMET unifies ROME and MEMIT under the preservation-memorization objective. EMMET uses an equality constraint for batched edits, providing a balanced approach that leverages the strengths of both ROME and MEMIT.\nEMMET\u0026rsquo;s Closed-Form Solution # EMMET uses a closed-form solution to implement the equality constraints across batch edits. This solution involves modifying the weight matrix of a transformer model in such a way that the edits are distributed across the parameters efficiently, ensuring that each targeted update is reflected accurately in the model\u0026rsquo;s output. The key formula for EMMET’s update is given by: \\[\\Delta = (V_E-W_0K_E)(K_E^TC_0^{-1}K_E)^{-1}K_E^TC_0^{-1}\\] Here V_E represents the vector of desired outputs, \\(W_0\\) is the original weight matrix, \\(K_E\\) is the key vector representing the input associated with each fact, and \\(C_0\\) is the covariance matrix derived from the existing model parameters. EMMET operates under the preservation-memorization objective, which aims to preserve the integrity of the model’s existing knowledge while accurately incorporating new information. The algorithm is carefully designed to balance these objectives, ensuring that the updates enhance the model\u0026rsquo;s utility without introducing errors or biases. EMMET is designed to incorporate batch edits under equality constraints. This approach is similar to ROME\u0026rsquo;s method of applying precise updates but is scaled to handle multiple edits simultaneously. EMMET ensures that each edit precisely matches the desired update without adversely affecting the existing knowledge encoded in the model. One of the features of EMMET is its ability to perform large-scale batch edits, which can include up to 10,000 edits in a single batch. This is a significant enhancement over traditional methods that typically handle edits one at a time or in smaller batches. EMMET’s batch processing capability makes it particularly valuable for applications requiring frequent and extensive updates to model data.\nExperiments and Results # Figure 3. Single layer editing performance of EMMET as a function of batch size when compared to MEMIT on the CounterFact dataset. Figure 4. Performance comparison of EMMET and MEMIT when distributing the edit over multiple layers using the MEMIT edit-distribution algorithm on the CounterFact dataset. The effectiveness of EMMET has been validated through extensive testing on standard model compilation datasets, including evaluations on various models such as GPT2-XL, GPT-J, and Llama-2-7b. These experiments demonstrated that EMMET matches and sometimes exceeds the performance of MEMIT in terms of editing success rate, maintaining data integrity, and generalization ability across multiple datasets and model architectures. Conclusion # This unified approach allows for a comprehensive comparison of the two methods, showing they can optimize similar objectives through different constraints. The introduction of EMMET, a new algorithm for batch editing under equality constraints, demonstrates the ability to handle large updates efficiently, maintaining performance on par with existing methods. The paper confirms the robustness of these techniques through extensive empirical testing and establishes a solid theoretical foundation for understanding model editing dynamics.\n"},{"id":22,"href":"/docs/spring24/22_/","title":"22","section":"Spring24","content":" Larimar: Large Language Models with Episodic Memory Control # Posted by: Sunggyu Jang, Hyeonwoo Park\nAuthors: Payel Das (IBM AI Research), Subhajit Chaudhury (IBM AI Research) et.al\n1. Background # Large Language Model (LLM) is one of the most popular topics in these days, due to their outstanding performance on various Natural Language Processing (NLP) tasks. However, LLM has faced a lot of challenges at the same time. In this report, we especially focus on the \u0026ldquo;knowledge edit\u0026rdquo; problem.\nKnowledge edit in LLM research # Knowledge edit problem can be summarized as \u0026ldquo;constantly updating the knowledge of pre-trained LLMs to keep models fact-relevant, safe, and ethical after deployment.\u0026rdquo; [1] The point is that, we have to update the knowledge on the pre-trained model accurately and quickly. Figures below illustrate why do we need knowledge update.\nTo update new knowledge To mitigate context length generalization problem To erase sensitive data Fig1. Knowledge update: New knowledge should be injected constantly [2] Fig2. Context length generalization: The ability to quickly update the LLM can help with \"input context length generalization problem\" [3] Fig3. Selective fact forgetting: LLMs should forget personal \u0026 sensitive data [4] Memory network # However, knowledge edit is not so simple as it sounds. Pre-training LLMs requires substantial computational cost due to thier unprecedented amounts of parameters. Considering the fact that we have to introduce new knowledge into the pre-trained model frequently, re-training the whole model is not a feasible solution [2].\nTo tackle the problem, \u0026ldquo;memory network\u0026rdquo; was proposed. The main point of memory network is \u0026ldquo;to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to.\u0026rdquo; [5]\nFor example, let\u0026rsquo;s assume that you\u0026rsquo;re providing new information to a pre-trained LLM. What you expect to the model is to answer the following questions based on the facts you mentioned. In this case, the model can do the job by writing the knowledge from you into a memory and reading the relevant one from the memory to answer the question. This problem is called as \u0026ldquo;Question Answering (QA).\u0026rdquo;\nFig4. Example of QA [5] Variational auto encoder (VAE) # To implement the idea of memory network, concepts from variational auto encoder are usually used. VAE is a kind of generative model to generate an output similar to real data. To be specific, it aims to approximate the true distribution of input data with three components - encoder, decoder, and latent space.\nIn this post, we assume that readers have knowledge about VAE. For details, please refer to [6] and [7].\nFig5. VAE Structure [7] Neocortex-Hippocampus interactions # This paper imitates the role of brain. Humans can rapidly update their knowledge after encountering the first relevant instance. In the brain, this process is facilitated through interactions between the neocortex and the hippocampus. The hippocampus is the site for storing long-term memories, while the neocortex integrates long-term and short-term memories to relay the results to the body.\nFig6. Neocortex and the Hippocampus The Complementary Learning Systems (CLS) theory proposes a model that combines these complementary learning systems of the hippocampus and neocortex. The interaction between the neocortex and hippocampus in the brain is known to promote adaptive behavior through memorization and generalization. Furthermore, it is suggested that memory consolidation from the hippocampus to the neocortex is facilitated by the activation synchronized with multiple exact or false replays of the encoded experience in the hippocampus. This implies that the hippocampus functions as a generative associative network. 2. Contributions # Larimar introduces a class of memory-conditioned language models inspired by complementary learning mechanisms in the brain. This architecture facilitates real-time test-time adaptation without requiring time-intensive gradient-based learning or internal fact tracing, offering a faster method for updating LLMs. Utility Demonstration in Knowledge Editing and Context Generalization:\nThe proposed method is demonstrated on two significant and challenging use cases: knowledge editing and generalizing to longer input contexts. Larimar exhibits fast and accurate training-free adaptation to new inputs in both scenarios, outperforming baseline editing methods and existing language models. Selective Fact Forgetting and Information Leakage Prevention:\nLarimar effectively supports selective fact forgetting and prevents information leakage using its one-shot memory updating mechanism. Recursive Search-Based Solution for Long Context Generalization: A simple recursive search-based approach is provided to enable Larimar\u0026rsquo;s memory to generalize to longer input contexts.\n3. Model architecture # Inspired by human brain (neocortex-hippocampus interactions), authors suggest \u0026ldquo;a class of LLMs augmented with an external episodic memory controller.\u0026rdquo; They utilize an episodic memory to mimic hippocampal fast-learning system, and use LLM as a neocortical slow learning system.\nFig7 below shows the overall architecture of Larimar. Basic idea is to implement VAE with external memory. It consists of three main components: encoder, decoder, and adaptive memory. Comparing the architecture with Fig5 would be helpful. In Larimar, memory corresponds to a latent vector.\nEncoder: Transforms the input into a latent vector Decoder: Generates an answer to the question conditioned on the memory Memory: Stores episodes in encoded form Fig7. Larimar architecture Let\u0026rsquo;s see how it works in two stages.\n3-1. Training # (1) Writing # The memory M in Fig7 has to be trained so as to approximate the distribution of X (X is an exchangeable-order invariant episode: X = { $x_{1}$, \u0026hellip;, $x_{N}$ }, a subset of the input data consisting of N samples). To do so, the model is trained to maximize the conditional log-likelihood of lnp (X|M). In this way, the model learns to compress X in a memory M, which then becomes a distributed associative memory. This process is similar to that of encoder in VAE. (Look at the green arrows in Fig7)\n(2) Reading # The reading weight matrix, W, is a random variable for generative ability of the model. In this paper, authors set a standard Gaussian prior p(W) ~ N(0, $I_{N \\times K}$ ) and posterior q(W) ~ N($\\bar{W}$, $\\sigma^2_{W} \\cdot I_{N \\times K}$), where the mean $\\bar{W}$ is estimated from each episode and $\\sigma_{W}$ is learnable. Memory readouts are obtained as Z$_{readout}$ = WM.\n(3) Summary # Three main components - encoder(e), associative memory(M), and decoder(d) - are jointly trained and optimized for an episode X, using the following loss:\n3-2. Inference # Once M$_{0}$ is trained via backpropagation, the posterior memory M is updated in one-shot by solving a minimization problem below. This problem can be efficiently done with the preudo-inverse of matrix. For more details, please refer to [8].\n4. Memory Operations # In this paper, authors followed the ideas from [8] to combine pre-trained LLMs and memory component for knowledge edit. Fig8 illustrates the single training step of the memory.\nFig8. Basic memory operations [8] On top of that, sequential writing and forgetting is conducted as follows.\nFirst, given an initial set of encodings $Z_{0}$ and writing weights $W_{0}$, memory matrix and key covariance matrix are initialized as below.\nNext, memory $M_{i-1}$ is sequentially updated by adding a new set of encodings $Z_{i}$ or forgetting a previously written set of encodings $Z_{i}$. This process is conducted as below.\nFor instance, $\\alpha_{i}$ is 1 for writing, -1 for forgetting.\n5. Results # Wall Clock time # Fig9. Comparison between different editing methods and the wall clock time for a single edit The experiment was conducted on a single A100 GPU. Comparing the wall clock time for each editing method across 10 single edits, as shown in Fig8, Larimar was found to be approximately 4-10 times faster than the existing ROME and GRACE methods. Additionally, Larimar demonstrates the ability to address sequential edits, batch edits, and forgetting/deletion, which were not previously addressed in existing work. Single Fact Editing # This paper utilizes the CounterFact dataset for comparing Single Fact editing. The CounterFact dataset is designed to test the language model\u0026rsquo;s ability to handle counterfactual edits. It evaluates whether the model accurately learns new facts. It contains a total of 21,919 data points, and the evaluation is conducted using the first 2000 samples. In contrast to training the LLM on edits or causally tracing the original fact within the LLM and updating the relevant parameters, Larimar leverages one-shot memory update for editing. In this approach, the memory posterior is updated as the edit(s) of interest is written, and then the updated memory is queried. The read-out from the memory conditions the decoder to output the edit.\nFig10.Single fact edit performanceon CounterFact dataset comparing with baseline. Top two best systems are highlighted. The results are shown in Fig 9. Edit Success measures the percentage of cases where the edited result has a higher probability than the original result, while Paraphrase evaluates whether the model achieves the same performance using paraphrase prompts. Neighborhood assesses the model's ability to retain knowledge about the original object. Larimar demonstrates comparable performance in editing new facts and handling prompts. Sequential Fact Editing # To check sequential fact editing, Test retention rate(TRR) and edit retention rate(ERR) are used. TRR check how well an edited model retains its performence on tis original testing data. Larminar decoder\u0026rsquo;s perplexity was tested on 1000 random test samples from wikitext using a separate language model. In comparison, baseline models compute TRR from mean F1 scores from 1000 random samples of NQ data. ERR check how well an edited model retains previous edits. This paper, ERR was evaluated by F1 score after 1000 sequential edits when querying the memory with the encoded query Zquery for each written fact.\nFig11. Sequential editing on ZsRE dataset According to the figure 9, Larimar’s comparable ERR performance to GRACE, while preserving its original test set performance.Larimar-1.3B achieves editing speeds approximately 10 or more times faster than GRACE on GPT-2 XL. Selective Forgetting # This results shows that specific fact can be selectively erased from N facts that are have been written in Larimar\u0026rsquo;s memory.\nFig12. Batch editing accuracy on counterfact dataset. Green: MEMIT, Orange: ROME, Magenta: MEND, Black: Larimar. Fig 10 shows many edits can be written at once to memory and accurately retrieve from it. Rewrite accuracy is near 100% for up to 512 edits (the memory size K) and then drops to 82% for 1024 edits. This result shows Larimar's ability to compress more than K facts into its size-K memory. This performance level is higher when compared to baselines like MEND and ROME, but subpar compared to MEMIT, which can accurately handle a very large batch of edits at a cost of reduced editing speed and is also not meant to handle sequential editing. To test the ability of Larimar for selectively forgetting specified facts during inference, write N facts to memory and then forget one fact, and also write to memoty in its place the same fact with the answer replaced with the string \"unknown.\" Then, compare recall for the forgotten fact before and after the forgetting operation. Paper also report the recall on the remaining N −1 facts in memory to demonstrate that forgetting does not compromise other memories. The samples used are from the ZsRE validation set and from the Counterfact test set. Fig13. Fraction of facts with accurate recall, for the Counterfact and ZsRE datasets, after writing N factrs to memory and removing one. As a result, Larimar achived higher performance in forgotten and retained information is all testbench than Basemodel. This shows that Larimar works better on selective forgetting. Recall Performance # Larimar performs fact recall with long context using data that is not present in the base decoders pretraining corpus. Facts are curated from CNN Fast Facts. Recursive search in the latent memory space and using readouts to construct new higher-level memory is performed to process the long context with Larimar’s memory trained on a relative small episode length. It should be noted that memory hierarchy is also found in hippocampus and thought to be implicated in learning.\nThe recall rate, in the context of information retrieval, is a measure of how well a system retrieves all relevant items of a specific class. It represents the proportion of relevant items that the system correctly identifies out of all the relevant items available. For example, in a search engine scenario, the recall rate indicates how many of the relevant documents related to a user's query are successfully retrieved by the system. A high recall rate implies that the system effectively captures most, if not all, of the relevant information. Fig14. Novel fact addition recall rate on FastFacts. Fig 12 shows Larimar’s recall performance does not degrade much with increasing input context length, even compared to some of most competitive baseline LLMs trained with longer training context. We also compare with Supersizing Transformer, which is a memory-augmented model, however it did not show competitive recall performance because it was not trained to perform memory-conditioned generation. Due to memory processing in the latent space, Larimar is also efficient is terms of number of KV cache token computation compared to baseline methods. 6. Conclusion and further improvements # This paper propose enhancing Large Language Models (LLMs) with a dynamically updatable and distributed episodic memory. By leveraging a one-shot memory update mechanism and combining it with memory-conditioned decoding, this framework demonstrates precise, robust, and significantly faster editing performance compared to baselines in both single-fact and challenging sequential editing experiments. Using the same memory update mechanism enable fast and selective fact deletion operations, as well as effective information deletion mechanisms. Additionally, provide a simple approach for handling long input contexts, demonstrating better fact recall from longer input contexts in Larimar\u0026rsquo;s memory space compared to state-of-the-art LLMs trained with much larger training context windows. When compared to cutting-edge LLMs trained with much larger training context windows, Larimar showcases its advantages.\nJust as the interaction between the Neocortex and Hippocampus inspired the design of the memory module in this paper, drawing inspiration from the Corpus Callosum to conceptualize hardware could also be a viable approach. The Corpus Callosum, as a part of the brain, serves as a major connecting structure of the cerebral nervous system responsible for communication between the hemispheres. It spans across the entire brain, situated between the left and right hemispheres, facilitating the exchange and coordination of information between them to harmonize various brain functions. Adjusting all parameters of the model during the process of learning new knowledge in LLMs incurs significant costs. I propose a method to divide the model\u0026rsquo;s parameters into parts and update only the relevant parameters corresponding to the data being trained, thereby reducing costs. Introducing a module performing the role of the Corpus Callosum separately allows for the exchange and adjustment of data between parts, enabling more efficient learning with reduced costs and facilitating the processing of various types of data individually and complex information collectively within the model.\nAnother improvement could be to add a memory module specifically for image processing. The memory module used in this paper accepts natural language as input and produces natural language as output. I propose introducing a separate memory module for processing images, so that when both natural language and images are input simultaneously, the information can be processed and reflected in the output. This would enable more effective processing by LLMs when both images and natural language are provided as input. For example, it could be used to provide a photo of a crime scene and information as input, and obtain clues about the suspect as output.\n7. References # [1] https://arxiv.org/abs/2403.11901 -\u0026gt; Larimar: Large Language Models with Episodic Memory Control\n[2] https://arxiv.org/abs/2310.16218 -\u0026gt; Knowledge Editing for Large Language Models: A Survey\n[3] https://arxiv.org/abs/2207.04901 -\u0026gt; Exploring Length Generalization in Large Language Models\n[4] https://arxiv.org/abs/2402.05813 -\u0026gt; Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models\n[5] https://arxiv.org/abs/1410.3916 -\u0026gt; Memory Networks\n[6] https://arxiv.org/abs/1312.6114 -\u0026gt; Auto-Encoding Variational Bayes\n[7] https://process-mining.tistory.com/161 -\u0026gt; VAE, blog post\n[8] https://openreview.net/forum?id=Harn4_EZBw -\u0026gt; Generative Pseudo-Inverse Memory\nbrain figure\n"},{"id":23,"href":"/docs/spring24/23_/","title":"23","section":"Spring24","content":" Beyond Language Models: Byte Models are Digital World Simulators # Authors: Shangda Wu, Xu Tan, Zili Wang, Rui Wang, Xiaobing Li, Maosong Sun Links: Paper, GitHub, Hugging Face, Official Project Page\nPosted by Dohun Kim and Yeongwoo Kim\nbGPT: from Language Models to Byte Models # Byte models expand traditional language models to the byte level, starting from the premise that all digital data and operations are fundamentally byte-based. These models process data from various modalities such as text, audio, and images uniformly as bytes, increasing their applicability in a wide digital environment.\nFigure 1: The bGPT framework simulates digital systems using native binary data. It integrates diverse data types into a single model by treating everything as a byte sequence.\nIn this paper, bGPT is introduced. bGPT is designed to model digital data at the byte level and is optimized to effectively process byte sequences. It has demonstrated performance comparable to specialized models across various modalities, including text, audio, and images, and offers new possibilities for predicting, simulating, and diagnosing algorithms or hardware operations. Designed to predict and understand bytes, bGPT provides a deeper understanding of and interaction with digital systems.\nThe main contributions of this paper are as follows:\nbGPT, a model with next-byte prediction is presented to simulate digital systems Hierarchical Transformer architecture is adapted to handle byte sequences efficiently. In-depth analysis of bGPT\u0026rsquo;s performance on text, audio, and image data is provided. Novel benchmarks are introduced to show bGPT\u0026rsquo;s capabilities for digital world modeling. Proposed bGPT Framework # Architecture # Figure 2: The hierachical Transformer architecture of bGPT. It segments sequence of bytes into a sequence of patches, to balance the need for long sequences and computational efficiency.\nLearning patterns in digital systems at the byte level provides a unified approach to integrating various data types, but the high resolution of bytes results in long sequences that significantly increase computational costs. This issue is especially pronounced in transformer-based models, limiting the efficiency and scalability of processing binary data. bGPT is equipped with a hierarchical structure designed to efficiently handle entire byte sequences. This structure segments a sequence of byte \\(B = \\{b_1, b_2, \\ldots, b_T\\}\\) of length \\(T\\) into a sequence of patches \\(\\mathcal{P}\\) , where each patch contains exactly \\(S\\) bytes: \\(\\mathcal{P} = [P_1, P_2, \\ldots, P_N]\\) where \\(N = \\left\\lceil \\frac{T}{S} \\right\\rceil\\) is the number of patches,\n\\(P_i = [b_{(i-1)S\u0026#43;1}, \\ldots, b_{(i)S}]\\) for \\(( 1 \\leq i \\leq N)\\) , if \\(T \\mod S \\neq 0\\) , the last patch defined as \\(P_N = [b_{(N-1)S\u0026#43;1}, \\ldots, b_T, \\underbrace{e, \\ldots, e}_{S - (T \\mod S)}]\\) where \\(e\\) represents the \u0026lt;eop\u0026gt; (end-of-patch).\nComponents\nLinear Projection Layer: Each byte patch is mapped to a high-dimensional feature space through a linear projection layer. During this process, each byte is encoded into a 257-dimensional vector, which includes the 256 possible byte values and a special \u0026lt;eop\u0026gt; (end-of-patch) token. Patch-Level Decoder: The embedded patches are processed by a patch-level decoder. This decoder plays a role in predicting the features of the next patch from the embedding of each patch, thereby learning the structural patterns of the entire dataset. Byte-Level Decoder: Based on the predicted patch features, the byte sequence within each patch is reconstructed. The byte-level decoder uses the features of each patch to predict the next byte within that patch, processing the detailed information of the entire byte sequence. Training Objectives # 1. Generative Modeling # This approach requires the model to predict the next byte in a given byte sequence. The model takes the byte sequence \\(B = \\{b_1, b_2, \\ldots, b_T\\}\\) as input and utilizes all previous byte information to predict the next byte \\(b_{i\u0026#43;1}\\) at each position.\nAs a loss function, the negative log likelihood of the next byte at each step is minimized. This encourages the model to maximize the likelihood of the actual occurrence of the next byte. \\(\\mathcal{L}_{\\text{GEN}}(\\theta) = - \\sum_{i=1}^{T-1} \\log p(b_{i\u0026#43;1} \\mid b_1, b_2, \\ldots, b_i; \\theta)\\) 2. Classification # Based on the knowledge acquired through generative modeling, bGPT can also be applied to classification tasks for labeled datasets. In this process, the model takes a byte sequence as input and predicts the category to which that sequence belongs. For classification tasks, the loss function used is the cross-entropy loss, which ensures that the model accurately outputs the prediction probabilities for each category.\n\\(\\mathcal{L}_{\\text{CLF}}(\\theta) = - \\sum_{k=1}^{K} y_k \\log p(y_k \\mid B; \\theta)\\) These training objectives enable bGPT to understand various byte-based data and accurately mimic digital patterns of the real world. The combination of generative approaches and classification capabilities grants the model the flexibility to tackle a diverse range of problems. Through this, the model can go beyond simple pattern recognition to play a crucial role in predicting and analyzing the operations of complex digital systems.\nApplications # 1. Digital Media Processing\nbGPT is used for processing various types of digital media data such as text, audio, and images. This model performs learning targeted at media files through generative modeling, transforming the data into features and subsequently performing classification tasks based on these features.\nFor example, audio files are converted to and processed in WAV format, while images are processed at a low resolution in BMP format. By utilizing these standardized datasets, bGPT can develop a generalized understanding of various media types.\n2. Algorithm and Hardware Simulation\nbGPT is particularly useful for tasks such as data conversion and CPU state modeling. This means bGPT can learn the digital conversion process and simulate the operation of CPUs to predict the state of the CPU after various commands are executed.\nFor example, in the task of converting the music data format from ABC notation to MIDI format, bGPT learns to transform text-based music scores in ABC notation into binary performance signals in MIDI. Additionally, this model is also capable of performing the reverse conversion from MIDI back to ABC notation.\nExperiment # 1. Processing Various Digital Media\nExperiment Overview To assess the flexibility and versatility of the bGPT model, experiments with various types of digital media data were conducted. This involved handling a wide range of file types including text, audio, and image data, with the aim to measure the model\u0026rsquo;s ability to process these types and to see how well bGPT generalizes compared to specialized models. The experiment included both generative modeling and classification tasks.\nExperimental Data The datasets used in the experiment included:\nText: Wikipedia data and AG news dataset. Audio: LibriSpeech and Speech Commands v2 dataset. Images: ImageNet and CIFAR-10 dataset. These datasets are ideal resources for evaluating the diverse media processing capabilities of bGPT.\nExperimental Setup The bGPT model was trained under various settings:\nbGPTimage: Trained exclusively with image data (ImageNet). bGPTlibri: Trained exclusively with text data (Wikipedia). bGPTmix: Trained with a mix of all the above datasets. Each model was fine-tuned for specific types of classification and generative tasks post-pretraining, providing a direct comparison of each model\u0026rsquo;s performance.\nResult and Analysis\nText Processing: bGPTwiki showed high classification accuracy on the AG News dataset, indicating bGPT\u0026rsquo;s strong performance in text-based tasks. Audio Processing: bGPTlibri demonstrated excellent performance on the Speech Commands v2 dataset, showcasing its high potential in audio processing. Image Processing: bGPTimage recorded high accuracy on the CIFAR-10 dataset but showed somewhat lower performance on ImageNet. This suggests that while bGPT works well with relatively simple images, it may have limitations with more complex images. Model | AG News (4 classes) | CIFAR-10 (10 classes) | Speech Commands v2 (36 classes) | BPB Acc (%) BPB Acc (%) BPB \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; bGPT_random 1.3496 84.74 3.4928 76.73 1.5414 bGPT_wiki 1.0639 92.49 3.6663 77.02 1.5719 bGPT_image 1.4179 83.16 3.1234 88.69 1.5326 bGPT_libri 1.3993 83.59 3.3345 83.51 1.4818 bGPT_signal 1.4058 83.80 3.1554 87.65 1.4898 bGPT_mix 1.0935 91.75 3.2279 84.32 1.5086 Baselines 0.9237 94.50 — 98.13 — 2. Algorithm and Hardware Simulation\nExperiment Overview One of the unique capabilities of the bGPT model is its ability to simulate the operations of algorithms and hardware. This experimental section assesses how bGPT handles complex data conversion processes and CPU state modeling tasks. These capabilities are particularly significant in the fields of cybersecurity, system diagnostics, and hardware optimization.\nExperiment Methods bGPT\u0026rsquo;s performance was evaluated in the following two key areas:\nData Conversion: This experiment evaluates whether bGPT can learn the process of converting ABC music notation into MIDI format. The task tests how bGPT models complex algorithms and their ability to convert actual music files. CPU State Modeling: CPU state modeling assesses how bGPT predicts and updates the state of a CPU based on a given set of machine instructions. This is particularly useful for understanding and predicting hardware operations. Results and Analysis\nData Conversion Performance: bGPT performed the conversion between MIDI and ABC notation with high accuracy. Notably, it also showed high accuracy in converting MIDI back to ABC notation, indicating that bGPT successfully learned the inherent structures and patterns of the data. CPU State Modeling Performance: bGPT accurately predicted the resulting state of CPUs from an initial state across a variety of CPU instructions. It achieved over 99% accuracy even with complex instruction sequences, demonstrating bGPT\u0026rsquo;s detailed understanding of the internal workings of hardware. Conclusion and Future Work # bGPT has proven to be a powerful model capable of effectively processing various types of digital media data. Particularly, this model can be flexibly applied to different types of data and has demonstrated performance that can compete with models pretrained on specific datasets. These results show that bGPT can be extremely useful in solving a wide range of real-world problems.\nMoreover, bGPT has proven its ability to go beyond simply processing data, successfully modeling and simulating the operations of complex algorithms and hardware. This capability will be particularly valuable in fields related to technical problem solving and new hardware design.\nbGPT extends deep learning to binary data processing through byte prediction. Experiments have demonstrated bGPT\u0026rsquo;s strong scalability in native binary data modeling.\nFuture research directions for byte models include:\nReducing training costs to make byte model training more feasible. Expanding the model and dataset sizes to accommodate a wider range of native binary data and handle larger digital media files such as high-resolution images and videos. Improving performance in underexplored tasks involving native binary data across various application domains. References # Beyond Language Models: Byte Models are Digital World Simulators (arXiv) "},{"id":24,"href":"/docs/spring24/24_/","title":"24","section":"Spring24","content":" # "},{"id":25,"href":"/docs/spring24/25_/","title":"25","section":"Spring24","content":" Merging Text Transformer Models from Different Initializations # Posted by: Kyungtae Kim, Minwoo Kim\nAuthors: Neha Verma (Johns Hopkins University), Maha Elbayad (Meta)\n[paper link].\nAlthough recent works on model merging have exhibited low- or zero-barrier mode connectivity between models with different initialization, model merging on transformer architecture has not yet been studied extensively. The application of previous merging techniques on the transformer structure is limited due to its unique structural characteristics, such as residual connection, multi-head attention (MHA), and sequential input. The paper merges separate transformer minima, proposing a new model merging technique to investigate the relationship between the pre-trained models\u0026rsquo; minima in the loss landscape. Using permutation-based model merging, authors found lower loss barriers between minima compared to other model merging techniques such as model averaging. The results showed that the model has less sharp and isolated minima than previously expected.\nThe contributions of the researchers are listed as follows:\nThey introduced a new transformer merging algorithm based on model permutation. They showed that the technique leads to decreased loss barriers between masked language models trained from different initializations compared to other merging methods. They extended their approach to fine-tuned models and showed consistently smaller loss barriers between models compared to vanilla merging. Background # Transformer # Figure 1. Basic structure of a transformer. Transformer is a type of sequence-to-sequence (seq2seq) models that takes a sequence of tokens as an input, and computes according to the input token. Unlike previous seq2seq models where a certain input token had a hard time affecting every output tokens, transformer uses self-attention, which allows all tokens to affect every output tokens. This allows for better performance in data where the distance between tokens has low relationship to the importance of the tokens\u0026rsquo; importance. For more details on transformers and attention, see the paper \u0026lsquo;Attention is All You Need\u0026rsquo;.\nLoss Landscape # Loss landscape is a representation of the loss values around the weight space of the network. Loss landscape helps researchers see how well a neural network has been trained and gives researchers new insights on their models.\nFigure 2. An example of a loss landscape of a deep learning model. DNNs are trained by optimizing a loss function with an stochastic gradient descent (SGD) variant. The loss landscapes of these networks have been shown to contain infinitely many global minimizers that are reachable with the help of SGD. One reason for the abundance of minima is overparameterization, which leads different functions to behave similarly on the training data. Permutation and scaling invariances also lead to functionally identical minima that differ in the weight space. Prior works stated that the optima of loss functions are connected by simple curves over which training and test accuracy are nearly constant (no loss barrier). This is called mode connectivity. Other researchers conjectured that if the permutation invariances of neural networks are taken into account, these optima are linearly mode connected, i.e. the linear path connecting these two models has no loss barrier. In the paper, the authors pay attention on how permutation between models could lead to similar or identical loss landscapes.\nModel Interpolation # Model interpolation is a technique that blends two or more models to create an intermediate model. This process is mostly done by averaging the model weights. Researchers found out that if fine-tuned models lie in a single low error basin, then the weight averaging performs similarly to ensembling, which combines the output of multiple fine-tuned model to hopefully obtain a better result. It is however not guaranteed that fine-tuned models (starting from the same initialization) will reside in the same loss basin. Prior work on linear interpolation-based model merging has focused on improving the algorithms used to bring the hidden units of two networks into alignment, in order to reduce the barrier to interpolation between them.\nPermutation-based Merging # Feed-Forward Layers # In this section, we explain how the authors of the paper used permutation to find the similarities between two distinct models and merge them. In short, permutation order that maximizes the cross-correlation of the models is calculated and the permutation is used to change the weight ordering.\nIn more details, given two models \\(\\theta_A\\) and \\(\\theta_B\\) trained from distinct initializations, the authors compute post-activation features for each layer or sublayer parameter \\(\\text{W}_l\\subset \\theta\\) in order to compute the similar parts across models. The researchers compute \\(d\\) -dimensional activations across \\(n\\) tokens from both models \\(\\text{X}_A, \\text{X}_B\\in \\mathbb{R}^{n\\times d}\\) . Then, the feature relatedness via cross-correlation is computed as\n\\[C=\\text{corr}(\\text{X}_A, \\text{X}_B)=\\frac{\\mathbb{E}[(\\text{X}_A-\\boldsymbol{\\mu}_{\\text{X}_A})^\\text{T}(\\text{X}_B-\\boldsymbol{\\mu}_{\\text{X}_B})]}{\\boldsymbol{\\sigma}_{\\text{X}_A}\\boldsymbol{\\sigma}_{\\text{X}_B}},\\] where \\(\\boldsymbol{\\sigma}\\) is a standard deviation vector, and \\(\\boldsymbol{\\mu}\\) is a mean vector. The features are standardized since the magnitude of features values can vary greatly depending on the initialization. Next, the permutation that gives the highest correlation score is computed, and is declared as the optimal computation. More specifically, given \\(C\\in\\mathbb{R}^{d\\times d}\\) and a permutation mapping \\(\\pi\\) , the optimal permutation is computed as follows:\n\\[\\text{arg}\\max_\\pi \\sum_{i=1}^{d} C(i, \\pi(i)).\\] cf. The above problem is solved using the Jonker-Volgenant algorithm.\nNext, the permutation mapping \\(\\pi\\) is converted to a permutation matrix \\(\\text{P}\\) . The matrix is then multiplied to the original weight matrix of \\(B\\) denoted as \\(\\text{W}_l^B \\subset \\theta_B\\) . Then the permuted weight matrix \\(\\text{P}\\text{W}_l^B\\) closely resembles the weight \\(A\\) , denoted as \\(\\text{W}_l^A \\subset \\theta_A\\) . Denoting the modified model parameters as \\(\\theta_B\u0026#39;\\) , the final merged model is computed as \\(\\lambda\\theta_A\u0026#43;(1-\\lambda)\\theta_B\\) for some \\(\\lambda\\in[0,1]\\) .\ncf. If permutation matrix \\(\\text{P}\\) is multiplied in layer \\(l\\) , then \\(\\text{P}^{\\text{T}}=\\text{P}^{-1}\\) is applied in the next layer to unpermute the ordering, i.e.,\n\\[\\text{W}_{l\u0026#43;1}^{B\u0026#39;} \\leftarrow \\text{W}_{l\u0026#43;1}^{B}\\text{P}^{\\text{T}}.\\] Multi-head Attentions # Figure 3. Different head alignments occuring from different initialization. Though the 5 heads of the multi-head attention all connect to the same 5 features, the order of the heads may differ. As aformentioned, multi-head attention mechanism can be challenging to deal with due to its unique properties. The authors propose using permutation on each attention head separately and not permuting features between attention heads.\nMore specifically, multi-head attention parameters include parameters from key, query, value, and linear layer each denoted as \\(\\text{W}_K\\) , \\(\\text{W}_Q\\) , \\(\\text{W}_V\\) , and \\(\\text{W}_O\\) . For each key, query, and value weights, the whole parameter \\(\\text{W} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}\\) is partitioned into \\(H\\) attention heads each of output dimension \\(d_k = d_{\\text{model}}/H\\) . Permutation should be operated on each attention head separately, in order to apply a permutation to full weight matrices and maintain the functional equivalence of the overall model. This is because the final hidden vector from MHA reflects a concatenation of the result from each head, which are computed separately with weights \\(\\text{W}_{K_i} ,\\text{W}_{Q_i} , \\text{W}_{V_i}\\) for head \\(i\\) . As can be seen from figure 123, since the models are trained from different initializations, the correspondence of their attention heads may differ in addition to the correspondence of features within each head. The features are extracted just after the attention computation and before the linear layer. The features are used to compute \\(C\\) , and then the correlation matrix is partitioned by heads into \\(d_k \\times d_k\\) correlation matrices, for each potential attention head pair. Next, optimal permutation for each unique head pair \\((j, k)\\) is computed. Each head\u0026rsquo;s internal permutation is computed and stored, and the cost is computed as\n\\[\\text{cost}(j,k)=\\max_\\pi \\sum_{i=1}^{d_k} C_{jk}(i,\\pi(i)),\\] where \\(C_{jk}\\) refers to the specific partition of the overall correlation matrix. The outer head correspondence permutation is computed as\n\\[\\pi_{\\text{outer}}=\\text{arg}\\max_\\pi \\sum_{h=1}^{H} \\text{cost}(h,\\pi(h)).\\] The algorithm returns a permuting matrix \\(\\text{P}_{\\text{MHA}}\\) , which is applied to each of \\(\\text{W}_V\\) , \\(\\text{W}_K\\) and \\(\\text{W}_Q\\) .\nResidual Connections # Each transformer layer comes with two residual connections, as can be seen from Figure 1. The residual connections can be formulated as follows:\n\\[\\begin{aligned} x_a^r\u0026amp;=\\text{LN}(\\text{W}_O \\text{MHA}(x) \u0026#43; x),\\\\ x_f^r\u0026amp;=\\text{LN}(\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; x_a^r). \\end{aligned}\\] The input and output of both sublayers are added to create a new output. This implies that if a permutation operation is applied to the output state, the permutation should be the same for both addends. Also, since the inputs passes through the LayerNorm module, the permutation to the output should also permute the features of the LayerNorm module also. Ignoring the parameters of the LayerNorm,\n\\[\\begin{aligned} \\text{P}x_f^r\u0026amp;=\\text{P}(\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; x_a^r)\\\\ \u0026amp;=\\text{P}\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; \\text{P}x_a^r\\\\ \u0026amp;=\\text{P}\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; \\text{P}(\\text{W}_O \\text{MHA}(x) \u0026#43; x) \\end{aligned}\\] Since the input to each layer must be permuted ( \\(\\text{P}x\\) ), and the output of each layer is also permuted ( \\(\\text{P}x_f^r\\) ), the entire transformer architecture uses the same \\(\\{\\text{P}, \\text{P}^{\\text{T}}\\}\\) matrices for all weights involved in residual connections.\nModels, Tasks, Datasets and Evaluation settings # In this work, the authors investigated 5 different BERT models from the MultiBERTs reproductions seeds 1 through 5 (See \u0026lsquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u0026rsquo; \u0026amp; \u0026lsquo;The MultiBERTs: BERT Reproductions for Robustness Analysis\u0026rsquo;). Each model has the following properties:\nEach model comes from a bert-base-uncased checkpoint. Different random initialization and random ordering. Same original BERT vocabulary and tokenizer. To test the baseline method, they used the masked language modeling task while employing the validation set of the Wikitext-103 benchmark as the evaluation data. Next, they extracted over one million sentences from the Books corpus. In classification tasks, they employed fine-tuned models with randomly initialized classification head with pooling layer and classification layer weights. The authors kept the head initializations the same across the models. They used the General Language Understanding Evaluation (GLUE) benchmark excluding WNLI. As a baseline for comparison, vanilla averaging is defined as:\n\\[\\theta_{avg} = \\frac{1}{2}(\\theta_A\u0026#43;\\theta_B)\\] In this work, they defined new evaluation definitions. They defined loss-barriers as (\u0026lsquo;M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations\u0026rsquo;): \\[\\max_{\\lambda} \\mathcal{L}(\\lambda\\theta_A \u0026#43; (1 - \\lambda)\\theta_B) - \\frac{1}{2}(\\mathcal{L}(\\theta_A) \u0026#43; \\mathcal{L}(\\theta_B))\\] A masking probability of \\(p = 0.15\\) across block sizes of 128 tokens were used to compute MLM loss/pseudo-perplexity. For \\(N\\) masked samples in the text \\(\\textbf{W}\\) , pseudo-perplexity is defined as:\n\\[\\mathrm{Pseudo-PPL}(\\textbf{W};\\theta) = 2^{-\\frac{1}{N} \\sum_{i=1}^{N}\\log_{2}\\,p_\\theta(\\omega_i|\\textbf{W}_{\\backslash{i}})}\\] Results # By component # First, they found that the merging all feed-forward sublayers and/or merging all multi-headed attention sublayers reduces the pseudo-perplexity compared to the baseline. Remakably, combination of them leads to the reducuction of the perplexity by about 7 times at \\(\\lambda = 0.5\\) (See Figure 4). The reduced barrier suggests that a lower loss path has formed among these models, indicating a connection between the minima with a barrier similar to what they report.\nFigure 4. Results of pseudo-perplexity scores of 10 MultiBERTs with vanilla averaging, merging all feed-forward sublayers, and merging all multi-headed attention sublayers and all multi-headed attention sublayers. Next, they investigated how well these Transformers learn similar representations of the model. The average feature correlations of both the Feed-Forward layer and the attention pre-merged/merged with our method are calculated. The aligned models show higher average feature correlations than the orignal models. However, these values are no more than 0.3 because some pre-trained transformers can be sparsely activated and be pruned heavily leading to lower average feature correlations (Li et al.,2023; Dalvi et al., 2020).\nFigure 5. Results of average feature correlations between 10 masked language model pairs. Multi-headed attention # Next, they investigated loss barrier of Head-Permutation multi-headed attention approach. It is worth noting that this approach maintains head structure while allowing different head correspondences (Head-Perm). The proposed method exhibits lower loss barrier than method using simple attention averaging (Vanilla Attention Avg.), method that ignores the multiheaded structure of the weight parameters (Ignore-Heads), and method that does not allow for different head correspondences across different models (Monotonic) while exhibiting clear attention head boundaries of the correlation matrix (See Figure 5 and Table 1).\nMethod Loss Barrier\u0026darr; Std. Err. Vanilla Attention Avg. 4.31 0.21 Monotonic Head Alignment 4.13 0.20 Ignore-Heads 3.97 0.25 Head-Perm 3.71 0.23 Table 1. Loss Barriers of 10 MultiBERTs merged with feed-forward and attention components merged. Figure 6. Results of a correlation matrix between the first multi-headed attention layer from two different MultiBERTs models. Residual Stream # Next, the effect of the permutation alignment involving residual connection parameters is discussed. Repeated Add/Norm components sharing the permutation operations reduce the permutation symmetries and available residual stream parameters. The identity permutation which uses the identity matrix \\({I_d}\\) exhibits the lowest loss barrier because only one pair of \\({\\{P, P^T\\}}\\) is in the residual stream. We note that the seperate permutation approach, despite it having the largest loss barrier and no valid symmetry, has largest degrees of freedom.\nMethod Loss Barrier\u0026darr; Std. Err. Identity 4.95 0.38 First 7.58 0.19 Last 7.41 0.18 All 7.34 0.22 Seperate 9.38 0.49 Table 2. Loss Barriers of merged MultiBERTs with only residual components merged. Amount of Data # Moreover, they investigate the effect of the amount of sentences on the loss barrier. Despite the combination of feed-forward and attention layers, there is no strong directional relationship between the amount of data and the loss barrier. It seems that some variations are attributed by the quality of the data (See Figure 7).\nFigure 7. Results of loss barrier respect to the amount of sentences. GLUE results # Finally, they compared the loss barriers of their method to those of vanilla averaging approach for eight different GLUE tasks including residual permutations (See Figure 8 and Table 3). Vanilla averaging (STS-B) exhibits the highest loss, but some tasks show that the vanilla averaging outperforms their approach. They observe inconsistent loss reduction, with lower loss barriers than those of the masked language modeling setting. They also observe that lower loss pattern than either parent model at about \\(\\lambda = 0.15\\) and \\(\\lambda = 0.85\\) . Interestingly, M-shaped curve can be found in some vanilla merges between these fine-tuned models. In this perspective, their method could be extended to explore lower loss paths between finely-tuned minima. However, the selection of optimal data for the lowest loss, understanding of fine-tuned models and pre-connectivity in the loss landscape are remained for future work.\nFigure 8. Loss barrier curves for 8 GLUE tasks for vanilla interpolation and our strategy. Vanilla averaging Proposed Barrier Error Barrier Error MNLI-mm 0.61 0.03 0.72 0.08 QQP 1.37 0.09 1.20 0.11 QNLI 0.64 0.04 0.77 0.06 SST-2 0.42 0.04 0.36 0.07 CoLA 1.31 0.14 1.11 0.13 STS-B 5.15 0.44 4.24 0.35 MRPC 2.74 0.08 1.93 0.11 RTE 0.53 0.04 0.41 0.05 Table 3. Comparison of loss bariers between fine-tuned BERT model across 8 GLUE tasks for vanilla interpolation and our strategy. Conclusion # In this work, the authors develop a new strategy for model mergring based on permutation mapping and demonstrates reduced loss barriers between masked languaged models with different initialziation compared to vanilla merging. Next, they extend their approach to fine-tuned models. The authors suggest that understanding the connectedness between models lead to achieving sharpness of minima and smoothness Transformer loss space. Moreover, it can open up new possibilities for improving design optimization methods, ensembles of models, and additional merging techniques. Specifically, this paper shows that permutation invariances of Transformer model is considered to characterize the geometric features of minima. Finally, they shad the light on the relationships between fine-tuned models, Transformer width and loss barriers, and the data for characterize the relationship between Transformer minima.\nFurther research is possible, examining the proposed techniques on a more complicated or larger dataset. Also, more sophisticated inspection on the aformentioned M-shaped loss curve is needed.\nReferences # Paper: https://arxiv.org/abs/2403.00986\nAttention mechanism: https://arxiv.org/abs/1706.03762\nLoss landscape explanation: https://arxiv.org/abs/1712.09913\nBERT: https://aclanthology.org/N19-1423/\nMultiBERTs: https://arxiv.org/abs/2106.16163\nWikitext-103 benchmark: https://arxiv.org/abs/1609.07843\nBooks corpus: https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html\nGLUE: https://arxiv.org/abs/1804.07461\nWNLI: https://arxiv.org/abs/1810.04805\nLoss barrier: https://arxiv.org/abs/1803.0363\n"}]