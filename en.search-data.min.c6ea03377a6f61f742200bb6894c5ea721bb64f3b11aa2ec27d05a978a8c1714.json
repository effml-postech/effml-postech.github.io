[{"id":0,"href":"/docs/spring24/00_taco_example/","title":"00 Taco Example","section":"Spring24","content":" Example : Content # This paper propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss. By doing so, they avoid decoding based on text-guided generative models\u0026mdash;known for high generative diversity\u0026mdash;and effectively utilize the semantic information of text at a global level.\nExample : Using KaTeX for math equation # KaTeX shortcode let you render math typesetting in markdown document. See KaTeX\nHere is some inline example: \\(\\pi(x)\\) , rendered in the same line. And below is display example, having display: block \\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi\\] Text continues here!!!\n"},{"id":1,"href":"/docs/spring24/01_/","title":"01","section":"Spring24","content":" # "},{"id":2,"href":"/docs/spring24/02_/","title":"02","section":"Spring24","content":" # "},{"id":3,"href":"/docs/spring24/03_/","title":"03","section":"Spring24","content":" # "},{"id":4,"href":"/docs/spring24/04_/","title":"04","section":"Spring24","content":" # "},{"id":5,"href":"/docs/spring24/05_/","title":"05","section":"Spring24","content":" # "},{"id":6,"href":"/docs/spring24/06_/","title":"06","section":"Spring24","content":" # "},{"id":7,"href":"/docs/spring24/07_/","title":"07","section":"Spring24","content":" # "},{"id":8,"href":"/docs/spring24/08_/","title":"08","section":"Spring24","content":" # "},{"id":9,"href":"/docs/spring24/09_/","title":"09","section":"Spring24","content":" # "},{"id":10,"href":"/docs/spring24/10_/","title":"10","section":"Spring24","content":" # "},{"id":11,"href":"/docs/spring24/11_/","title":"11","section":"Spring24","content":" # "},{"id":12,"href":"/docs/spring24/12_/","title":"12","section":"Spring24","content":" # "},{"id":13,"href":"/docs/spring24/13_/","title":"13","section":"Spring24","content":" # "},{"id":14,"href":"/docs/spring24/14_/","title":"14","section":"Spring24","content":" # "},{"id":15,"href":"/docs/spring24/15_/","title":"15","section":"Spring24","content":" # "},{"id":16,"href":"/docs/spring24/16_/","title":"16","section":"Spring24","content":" Mixture-of-Depths: Dynamically allocating compute in transformer-based language models # Authors: Dativd Raposo(Google DeepMind) and Adam Santoro(Google DeepMind) et.al\nThis paper insists that all problems do not require same amount of time to solve in real world and also in language models like Transformer.\nBut, Transformer models spread FLOPs \u0026lsquo;uniformly\u0026rsquo; across input sequences, which is inefficient!\nTherefore there are many efforts like \u0026ldquo;early exiting\u0026rdquo; to reduce total FLOPs \u0026amp; \u0026lsquo;dynamically\u0026rsquo; allocate compute budgets. However, these methods do not work well due to hardware constraints.\nSo, the methods should be sophistically addressed like harmonious with current hardware stack \u0026amp; known tensor sizes that are selected to maximize hardware utilization.\nTherefore, the things that authors of this paper contributed in this paper are listed as follows:\nSuggestion of method(Mixture-of-Depths, MoD) which limits the total FLOPs by choosing only k tokens which process into Attention + mlp layer. Comparing this method with Vanilla transformer(isoFLOP) Comparing this method with Mixture-of-Experts(MoE) and Combine to MoDE Background # What is MoE? # MoE\nImplementing Mixture-of-Depth Transformers # # Results \u0026amp; Discussion # References # "},{"id":17,"href":"/docs/spring24/17_/","title":"17","section":"Spring24","content":" # "},{"id":18,"href":"/docs/spring24/18_/","title":"18","section":"Spring24","content":" # "},{"id":19,"href":"/docs/spring24/19_/","title":"19","section":"Spring24","content":" # "},{"id":20,"href":"/docs/spring24/20_/","title":"20","section":"Spring24","content":" # "},{"id":21,"href":"/docs/spring24/21_/","title":"21","section":"Spring24","content":" # "},{"id":22,"href":"/docs/spring24/22_/","title":"22","section":"Spring24","content":" Larimar: Large Language Models with Episodic Memory Control # Authors: Payel Das (IBM AI Research), Subhajit Chaudhury (IBM AI Research) et.al\nBackground # Pre-trained Large Language Models (LLMs) have demonstrated outstanding performance on various Natural Language Processing (NLP) tasks. However, LLMs have faced a lot of challenges at the same time. Especially, in this post, we focus on the \u0026ldquo;knowledge update\u0026rdquo; problem.\nKnowledge update in LLM research # In order to keep pre-trained LLMs fact-relevant, safe, and ethical after deploymemnt, the knowledge of the LLM needs to be constantly updated. To be specific, model editing is mandatory to remove the undesired, incorrect, or obsolete facts from the LLM\u0026rsquo;s \u0026ldquo;memory\u0026rdquo;, and optionally replace it with desired outcome. Firgures below shows why do we need knowledge update with detailed examples.\n(Fig1. Knowledge update: New knowledge should be injected constantly) (Fig2. Context length generalization: The ability to quickly update the LLM can help with \"input context length generalization problem\") (Fig3. Selective fact forgetting: LLMs should forget personal \u0026 sensitive data) Autoencoder # TODO : This content can be erased\n(Fig4. Autoencoder) Memory network # Neocortex-Hippocampus interactions # This paper imitates the role of brain. Humans can rapidly update their knowledge after encountering the first relevant instance. In the brain, this process is facilitated through interactions between the neocortex and the hippocampus. The hippocampus is the site for storing long-term memories, while the neocortex integrates long-term and short-term memories to relay the results to the body.\nThe Complementary Learning Systems (CLS) theory proposes a model that combines these complementary learning systems of the hippocampus and neocortex. The interaction between the neocortex and hippocampus in the brain is known to promote adaptive behavior through memorization and generalization. Furthermore, it is suggested that memory consolidation from the hippocampus to the neocortex is facilitated by the activation synchronized with multiple exact or false replays of the encoded experience in the hippocampus. This implies that the hippocampus functions as a generative associative network. Contributions # Larimar introduces a class of memory-conditioned language models inspired by complementary learning mechanisms in the brain. This architecture facilitates real-time test-time adaptation without requiring time-intensive gradient-based learning or internal fact tracing, offering a faster method for updating LLMs. Utility Demonstration in Knowledge Editing and Context Generalization:\nThe proposed method is demonstrated on two significant and challenging use cases: knowledge editing and generalizing to longer input contexts. Larimar exhibits fast and accurate training-free adaptation to new inputs in both scenarios, outperforming baseline editing methods and existing language models. Selective Fact Forgetting and Information Leakage Prevention:\nLarimar effectively supports selective fact forgetting and prevents information leakage using its one-shot memory updating mechanism. Recursive Search-Based Solution for Long Context Generalization: A simple recursive search-based approach is provided to enable Larimar\u0026rsquo;s memory to generalize to longer input contexts.\nModel architecture # Larimar consists of three main components: encoder, decoder, and adaptive memory.\nEncoder: Transforms the input into a latent vector Decoder: Generates an answer to the question conditioned on the memory Memory: Stores episodes in encoded form (Fig6. Larimar architecture) Memory Operations # Results # Conclusion # References # https://arxiv.org/abs/2310.16218 -\u0026gt; Knowledge Editing for Large Language Models: A Survey\nhttps://arxiv.org/abs/2207.04901 -\u0026gt; Exploring Length Generalization in Large Language Models\nhttps://arxiv.org/abs/2402.05813 -\u0026gt; Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models\nhttps://arxiv.org/abs/2403.11901\nbrain figure\nhttps://openreview.net/forum?id=Harn4_EZBw -\u0026gt; Generative Pseudo-Inverse Memory\n"},{"id":23,"href":"/docs/spring24/23_/","title":"23","section":"Spring24","content":" Beyond Language Models: Byte Models are Digital World Simulators # Authors: Shangda Wu, Xu Tan, Zili Wang, Rui Wang, Xiaobing Li, Maosong Sun\nByte models expand traditional language models to the byte level, starting from the premise that all digital data and operations are fundamentally byte-based. These models process data from various modalities such as text, audio, and images uniformly as bytes, increasing their applicability in a wide digital environment.\nIn this paper, bGPT is introduced. bGPT is designed to model digital data at the byte level and is optimized to effectively process byte sequences. It has demonstrated performance comparable to specialized models across various modalities, including text, audio, and images, and offers new possibilities for predicting, simulating, and diagnosing hardware operations. Designed to predict and understand bytes, bGPT provides a deeper understanding of and interaction with digital systems.\nThe bGPT framework simulates digital systems using native binary data. It integrates diverse data types into a single model by treating everything as a byte sequence. Exploring bGPT # Architecture\nLearning patterns in digital systems at the byte level provides a unified approach to integrating various data types, but the high resolution of bytes results in long sequences that significantly increase computational costs. This issue is especially pronounced in transformer-based models, limiting the efficiency and scalability of processing binary data. bGPT is equipped with a hierarchical structure designed to efficiently handle entire byte sequences. This structure segments a sequence of byte \\(B = \\{b_1, b_2, \\ldots, b_T\\}\\) of length \\(T\\) into a sequence of patches \\(\\mathcal{P}\\) , where each patch contains exactly \\(S\\) bytes: \\(\\mathcal{P} = [P_1, P_2, \\ldots, P_N]\\) where \\(N = \\left\\lceil \\frac{T}{S} \\right\\rceil\\) is the number of patches,\n\\(P_i = [b_{(i-1)S\u0026#43;1}, \\ldots, b_{(i)S}]\\) for \\(( 1 \\leq i \\leq N)\\) , \\(P_N = [b_{(N-1)S\u0026#43;1}, \\ldots, b_T, \\underbrace{e, \\ldots, e}_{S - (T \\mod S)}]\\) where \\(e\\) represents the \u0026lt;eop\u0026gt; (end-of-patch).\nComponents\nLinear Projection Layer: Each byte patch is mapped to a high-dimensional feature space through a linear projection layer. During this process, each byte is encoded into a 257-dimensional vector, which includes the 256 possible byte values and a special \u0026lt;eop\u0026gt; (end-of-patch) token. Patch-Level Decoder: The embedded patches are processed by a patch-level decoder. This decoder plays a role in predicting the features of the next patch from the embedding of each patch, thereby learning the structural patterns of the entire dataset. Byte-Level Decoder: Based on the predicted patch features, the byte sequence within each patch is reconstructed. The byte-level decoder uses the features of each patch to predict the next byte within that patch, processing the detailed information of the entire byte sequence. Model Training\n1. Generative Modeling\nThis approach requires the model to predict the next byte in a given byte sequence. The model takes the byte sequence B = {b_1, b_2, \\ldots, b_T} as input and utilizes all previous byte information to predict the next byte \\(b_{i\u0026#43;1}\\) at each position.\nAs a loss function, the negative log likelihood of the next byte at each step is minimized. This encourages the model to maximize the likelihood of the actual occurrence of the next byte. \\(\\mathcal{L}_{\\text{GEN}}(\\theta) = - \\sum_{i=1}^{T-1} \\log p(b_{i\u0026#43;1} \\mid b_1, b_2, \\ldots, b_i; \\theta)\\) 2. Classification\nBased on the knowledge acquired through generative modeling, bGPT can also be applied to classification tasks for labeled datasets. In this process, the model takes a byte sequence as input and predicts the category to which that sequence belongs. For classification tasks, the loss function used is the cross-entropy loss, which ensures that the model accurately outputs the prediction probabilities for each category.\n\\(\\mathcal{L}_{\\text{CLF}}(\\theta) = - \\sum_{k=1}^{K} y_k \\log p(y_k \\mid B; \\theta)\\) These training objectives enable bGPT to understand various byte-based data and accurately mimic digital patterns of the real world. The combination of generative approaches and classification capabilities grants the model the flexibility to tackle a diverse range of problems. Through this, the model can go beyond simple pattern recognition to play a crucial role in predicting and analyzing the operations of complex digital systems.\nApplications # 1. Digital Media Processing\nbGPT is used for processing various types of digital media data such as text, audio, and images. This model performs learning targeted at media files through generative modeling, transforming the data into features and subsequently performing classification tasks based on these features.\nFor example, audio files are converted to and processed in WAV format, while images are processed at a low resolution in BMP format. By utilizing these standardized datasets, bGPT can develop a generalized understanding of various media types.\n2. Algorithm and Hardware Simulation\nbGPT is particularly useful for tasks such as data conversion and CPU state modeling. This means bGPT can learn the digital conversion process and simulate the operation of CPUs to predict the state of the CPU after various commands are executed.\nFor example, in the task of converting the music data format from ABC notation to MIDI format, bGPT learns to transform text-based music scores in ABC notation into binary performance signals in MIDI. Additionally, this model is also capable of performing the reverse conversion from MIDI back to ABC notation.\nExperiment # 1. Processing Various Digital Media\nExperiment Overview To assess the flexibility and versatility of the bGPT model, experiments with various types of digital media data were conducted. This involved handling a wide range of file types including text, audio, and image data, with the aim to measure the model\u0026rsquo;s ability to process these types and to see how well bGPT generalizes compared to specialized models. The experiment included both generative modeling and classification tasks.\nExperimental Data The datasets used in the experiment included:\nText: Wikipedia data and AG news dataset. Audio: LibriSpeech and Speech Commands v2 dataset. Images: ImageNet and CIFAR-10 dataset. These datasets are ideal resources for evaluating the diverse media processing capabilities of bGPT.\nExperimental Setup The bGPT model was trained under various settings:\nbGPTimage: Trained exclusively with image data (ImageNet). bGPTlibri: Trained exclusively with text data (Wikipedia). bGPTmix: Trained with a mix of all the above datasets. Each model was fine-tuned for specific types of classification and generative tasks post-pretraining, providing a direct comparison of each model\u0026rsquo;s performance.\nResult and Analysis\nText Processing: bGPTwiki showed high classification accuracy on the AG News dataset, indicating bGPT\u0026rsquo;s strong performance in text-based tasks. Audio Processing: bGPTlibri demonstrated excellent performance on the Speech Commands v2 dataset, showcasing its high potential in audio processing. Image Processing: bGPTimage recorded high accuracy on the CIFAR-10 dataset but showed somewhat lower performance on ImageNet. This suggests that while bGPT works well with relatively simple images, it may have limitations with more complex images. 2. Algorithm and Hardware Simulation\nExperiment Overview One of the unique capabilities of the bGPT model is its ability to simulate the operations of algorithms and hardware. This experimental section assesses how bGPT handles complex data conversion processes and CPU state modeling tasks. These capabilities are particularly significant in the fields of cybersecurity, system diagnostics, and hardware optimization.\nExperiment Methods bGPT\u0026rsquo;s performance was evaluated in the following two key areas:\nData Conversion: This experiment evaluates whether bGPT can learn the process of converting ABC music notation into MIDI format. The task tests how bGPT models complex algorithms and their ability to convert actual music files. CPU State Modeling: CPU state modeling assesses how bGPT predicts and updates the state of a CPU based on a given set of machine instructions. This is particularly useful for understanding and predicting hardware operations. Results and Analysis\nData Conversion Performance: bGPT performed the conversion between MIDI and ABC notation with high accuracy. Notably, it also showed high accuracy in converting MIDI back to ABC notation, indicating that bGPT successfully learned the inherent structures and patterns of the data. CPU State Modeling Performance: bGPT accurately predicted the resulting state of CPUs from an initial state across a variety of CPU instructions. It achieved over 99% accuracy even with complex instruction sequences, demonstrating bGPT\u0026rsquo;s detailed understanding of the internal workings of hardware. Conclusion and Future Work # bGPT has proven to be a powerful model capable of effectively processing various types of digital media data. Particularly, this model can be flexibly applied to different types of data and has demonstrated performance that can compete with models pretrained on specific datasets. These results show that bGPT can be extremely useful in solving a wide range of real-world problems.\nMoreover, bGPT has proven its ability to go beyond simply processing data, successfully modeling and simulating the operations of complex algorithms and hardware. This capability will be particularly valuable in fields related to technical problem solving and new hardware design.\nbGPT extends deep learning to binary data processing through byte prediction. Experiments have demonstrated bGPT\u0026rsquo;s strong scalability in native binary data modeling.\nFuture research directions for byte models include:\nReducing training costs to make byte model training more feasible. Expanding the model and dataset sizes to accommodate a wider range of native binary data and handle larger digital media files such as high-resolution images and videos. Improving performance in underexplored tasks involving native binary data across various application domains. "},{"id":24,"href":"/docs/spring24/24_/","title":"24","section":"Spring24","content":" # "},{"id":25,"href":"/docs/spring24/25_/","title":"25","section":"Spring24","content":" Merging Text Transformer Models from Different Initializations # Authors: Neha Verma (Johns Hopkins University), Maha Elbayad (Meta)\nAlthough recent works on model merging have exhibited low- or zero-barrier mode connectivity between models with different initialization, model merging on transformer architecture has not yet been studied extensively. The application of previous merging techniques on the transformer structure is limited due to its unique structural characteristics, such as residual connection, multi-head attention (MHA), and sequential input. The paper merges separate transformer minima, proposing a new model merging technique to investigate the relationship between the pre-trained models\u0026rsquo; minima in the loss landscape. Using permutation-based model merging, authors found lower loss barriers between minima compared to other model merging techniques such as model averaging. The results showed that the model has less sharp and isolated minima than previously expected.\nThe contributions of the researchers are listed as follows:\nThey introduced a new transformer merging algorithm based on model permutation. They showed that the technique leads to decreased loss barriers between masked language models trained from different initializations compared to other merging methods. They extended their approach to fine-tuned models and showed consistently smaller loss barriers between models compared to vanilla merging. Background # Transformer # Transformer is a type of sequence-to-sequence (seq2seq) models that takes a sequence of tokens as an input, and computes according to the input token. Unlike previous seq2seq models where a certain input token had a hard time affecting every output tokens, transformer uses self-attention, which allows all tokens to affect every output tokens. This allows for better performance in data where the distance between tokens has low relationship to the importance of the tokens\u0026rsquo; importance. For more details on transformers and attention, see the paper \u0026lsquo;Attention is All You Need\u0026rsquo;.\nLoss Landscape # Loss landscape is a representation of the loss values around the weight space of the network. Loss landscape helps researchers see how well a neural network has been trained and gives researchers new insights on their models.\nDNNs are trained by optimizing a loss function with an stochastic gradient descent (SGD) variant. The loss landscapes of these networks have been shown to contain infinitely many global minimizers that are reachable with the help of SGD. One reason for the abundance of minima is overparameterization, which leads different functions to behave similarly on the training data. Permutation and scaling invariances also lead to functionally identical minima that differ in the weight space. Prior works stated that the optima of loss functions are connected by simple curves over which training and test accuracy are nearly constant (no loss barrier). This is called mode connectivity. Other researchers conjectured that if the permutation invariances of neural networks are taken into account, these optima are linearly mode connected, i.e. the linear path connecting these two models has no loss barrier. In the paper, the authors pay attention on how permutation between models could lead to similar or identical loss landscapes.\nModel Interpolation # Model interpolation is a technique that blends two or more models to create an intermediate model. This process is mostly done by averaging the model weights. Researchers found out that if fine-tuned models lie in a single low error basin, then the weight averaging performs similarly to ensembling, which combines the output of multiple fine-tuned model to hopefully obtain a better result. It is however not guaranteed that fine-tuned models (starting from the same initialization) will reside in the same loss basin. Prior work on linear interpolation-based model merging has focused on improving the algorithms used to bring the hidden units of two networks into alignment, in order to reduce the barrier to interpolation between them.\nPermutation-based Merging # Feed-Forward Layers # In this section, we explain how the authors of the paper used permutation to find the similarities between two distinct models and merge them. Given two models $\\theta_A$ and $\\theta_B$ trained from distinct initializations, the authors compute post-activation features for each layer or sublayer parameter $\\text{W}_l\\subset \\theta$ in order to compute the similar parts across models. The researchers compute $d$-dimensional activations across $n$ tokens from both models $\\text{X}_A, \\text{X}_B\\in \\mathbb{R}^{n\\times d}$. Then, the feature relatedness via cross-correlation is computed as\n$$ C=\\text{corr}(\\text{X}_A, \\text{X}_B)=\\frac{\\mathbb{E}[(\\text{X}_A-\\boldsymbol{\\mu}_{\\text{X}_A})^\\text{T}(\\text{X}_B-\\boldsymbol{\\mu}_{\\text{X}_B})]}{\\boldsymbol{\\sigma}_{\\text{X}_A}\\boldsymbol{\\sigma}_{\\text{X}_B}}, $$\nwhere $\\boldsymbol{\\sigma}$ is a standard deviation vector, and $\\boldsymbol{\\mu}$ is a mean vector. The features are standardized since the magnitude of features values can vary greatly depending on the initialization. Next, the permutation that gives the highest correlation score is computed, and is declared as the optimal computation. More specifically, given $C\\in\\mathbb{R}^{d\\times d}$ and a permutation mapping $\\pi$, the optimal permutation is computed as follows:\n$$ \\text{arg}\\max_\\pi \\sum_{i=1}^{d} C(i, \\pi(i)). $$\ncf. The above problem is solved using the Jonker-Volgenant algorithm.\nNext, the permutation mapping $\\pi$ is converted to a permutation matrix $\\text{P}$. The matrix is then multiplied to the original weight matrix of $B$ denoted as $\\text{W}_l^B \\subset \\theta_B$. Then the permuted weight matrix $\\text{P}\\text{W}_l^B$ closely resembles the weight $A$, denoted as $\\text{W}_l^A \\subset \\theta_A$. Denoting the modified model parameters as $\\theta_B\u0026rsquo;$, the final merged model is computed as $\\lambda\\theta_A+(1-\\lambda)\\theta_B$ for some $\\lambda\\in[0,1]$. cf. If permutation matrix $\\text{P}$ is multiplied in layer $l$, then $\\text{P}^{\\text{T}}=\\text{P}^{-1}$ is applied in the next layer to unpermute the ordering, i.e.,\n$$ \\text{W}_{l+1}^{B'} \\leftarrow \\text{W}_{l+1}^{B}\\text{P}^{\\text{T}}. $$\nMulti-head Attentions # Multi-head attention parameters include parameters from key, query, value, and linear layer each denoted as $\\text{W}_K$, $\\text{W}_Q$, $\\text{W}_V$, and $\\text{W}_O$. For each key, query, and value weights, the whole parameter $\\text{W} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}$ is partitioned into $H$ attention heads each of output dimension $d_k = d_{\\text{model}}/H$. Permutation should be operated on each attention head separately, in order to apply a permutation to full weight matrices and maintain the functional equivalence of the overall model. This is because the final hidden vector from MHA reflects a concatenation of the result from each head, which are computed separately with weights $\\text{W}_{K_i} ,\\text{W}_{Q_i} , \\text{W}_{V_i}$ for head $i$. In the paper's case, since the models are trained from different initializations, the correspondence of their attention heads may differ in addition to the correspondence of features within each head. The features are extracted just after the attention computation and before the linear layer. The features are used to compute $C$, and then the correlation matrix is partitioned by heads into $d_k \\times d_k$ correlation matrices, for each potential attention head pair. Next, optimal permutation for each unique head pair $(j, k)$ is computed. Each head's internal permutation is computed and stored, and the cost is computed as\n$$ \\text{cost}(j,k)=\\max_\\pi \\sum_{i=1}^{d_k} C_{jk}(i,\\pi(i)), $$\nwhere $C_{jk}$ refers to the specific partition of the overall correlation matrix. The outer head correspondence permutation is computed as\n$$ \\pi_{\\text{outer}}=\\text{arg}\\max_\\pi \\sum_{h=1}^{H} \\text{cost}(h,\\pi(h)). $$\nThe algorithm outputs a permuting matrix $\\text{P}_{\\text{MHA}}$, which is applied to each of $\\text{W}_V$, $\\text{W}_K$ and $\\text{W}_Q$.\nResidual Connections # Each transformer layer comes with two residual connections, as can be seen from FIgure 1. The residual connections can be formulated as follows:\n$$ \\begin{align} x_a^r\u0026amp;=\\text{LN}(\\text{W}_O \\text{MHA}(x) + x),\\ x_f^r\u0026amp;=\\text{LN}(\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) + x_a^r). \\end{align} $$\nThe input and output of both sublayers are added to create a new output. This implies that if a permutation operation is applied to the output state, the permutation should be the same for both addends. Also, since the inputs passes through the LayerNorm module, the permutation to the output should also permute the features of the LayerNorm module also. Ignoring the parameters of the LayerNorm,\n$$ \\begin{align} \\text{P}x_f^r\u0026amp;=\\text{P}(\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) + x_a^r)\\ \u0026amp;=\\text{P}\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) + \\text{P}x_a^r\\ \u0026amp;=\\text{P}\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) + \\text{P}(\\text{W}_O \\text{MHA}(x) + x) \\end{align} $$\nSince the input to each layer must be permuted ($\\text{P}x$), and the output of each layer is also permuted ($\\text{P}x_f^r$), the entire transformer architecture uses the same ${\\text{P}, \\text{P}^{\\text{T}}}$ matrices for all weights involved in residual connections.\nResults # Conclusion # References # https://arxiv.org/abs/2403.00986\nLoss landscape figure\n"}]