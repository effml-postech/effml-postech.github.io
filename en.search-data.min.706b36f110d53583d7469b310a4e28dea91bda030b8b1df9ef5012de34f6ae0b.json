[{"id":0,"href":"/docs/spring24/00_taco_example/","title":"00 Taco Example","section":"Spring24","content":" Example : Content # This paper propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss. By doing so, they avoid decoding based on text-guided generative models\u0026mdash;known for high generative diversity\u0026mdash;and effectively utilize the semantic information of text at a global level.\nExample : Using KaTeX for math equation # KaTeX shortcode let you render math typesetting in markdown document. See KaTeX\nHere is some inline example: \\(\\pi(x)\\) , rendered in the same line. And below is display example, having display: block \\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi\\] Text continues here!!!\n"},{"id":1,"href":"/docs/spring24/01_/","title":"01","section":"Spring24","content":" Is Bigger Edit Batch Size Always Better? - An Empirical Study on Model Editing with Llama-3 # Authors: Junsang Yoon, Akshat Gupta, Gopala Anumanchipalli\nPosted by Jin Hyun, Gyuhyun Jung\nBackground # What is model editing? # Fig 1. Concept of model editing. The rapidly evolving field of artificial intelligence faces the challenge of keeping large language models (LLMs) up-to-date with new information, as traditional retraining methods are time-consuming and resource-intensive. As shown in figure, an alternative is model editing proposed in (Sinitsin et al., 2020). It enables data-efficient alterations to the behavior of models.\nFig 2. Example of model editing in case of MEMIT. Model editing modifies stored facts within a model and corrects inaccuracies without retraining. Techniques such as ROME (Rank-One Model Editing) (Meng et al., 2022a), MEMIT (Mass Editing Memory in Transformer) (Meng et al., 2022b), and EMMET (Equality-constrained Mass Model Editing algorithm for Transformers) (Gupta et al., 2024), known as \u0026ldquo;locate-and-edit\u0026rdquo; algorithms, have emerged to optimize the preservation-memorization (PM) objective. These methods directly modify specific areas of the model and are applicable to any transformer-based LLMs, offering a more efficient way to update models without retraining.\nHow model editing works? # For a relation \\((s,r,o)\\) expressed as a tuple in the form of (subject, relation, object). In model editing, we aim to update the memory of the existing model with new facts by learning about a new object \\((s,r,o^*)\\) . Model editing directly reform the weight by objective function, called the preservation-memorization objective. This objective consists of two parts, a preservation term and a memorization term. Below equation shows how ROME works with preservation term and memorization term.\n\\( \\argmin_{\\hat{W}} \\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\quad \\text{s.t.} \\quad \\hat{W} k_e = v_e \\\\Preservation\\_term=\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\\\ Memorization\\_term=\\hat{W} k_e = v_e \\) Where W represents the weights of the feedforward layer we want to edit, k is a key-vector representative of a fact, \\(v_e\\) is the desired output, and \\(K_0 =[k_1^0 |k_2^0 |\\cdots| k_0^N]\\) is a matrix consisting of facts we want to preserve. Above equation is optimized by follwing gradient.\n\\(\\hat{W} = W_0 \u0026#43; \\Delta \\quad \\text{where} \\\\ \\Delta = (v_e - W_0 k_e) \\frac{k_e^T C_0^{-1}}{k_e^T C_0^{-1} k_e} \\) For MEMIT model editing. it optimizes same objectives with ROME, but performance memorization using a least-square constraint, which allows for a closed-form solution. It has similar form with ROME method, but it multiplies \\(\\lambda\\) term, which is hyperparameter, to preservation term. Also, it combines memorization term for minimize target\n\\(\\argmin_{\\hat{W}} \\lambda\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \u0026#43; \\left\\| \\hat{W} K_E - V_E \\right\\|\\\\Preservation\\_term=\\lambda\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\\\ Memorization\\_term=\\hat{W} K_E - V_E \\) \\(V_E\\) is stacked matrix of \\(v_e\\) vectors, and fact is represented by a pair of vectors denoted as key ( \\(k_e\\) ) and value ( \\(v_e\\) ). This objective has similar solution of ROME, followed by below equations.\n\\(\\hat{W} = W_0 \u0026#43; \\Delta \\quad \\text{where} \\\\ \\Delta = (V_E - W_0 K_R)K^T_E (\\lambda C_0 \u0026#43; K_E^T K_E^T)^{-1} \\) In EMMET, it shows model editing is possible with batched facts. It is possible by allowing memorization happens using an equality-constraint. EMMET objective and gradient solution is followed by below equations.\n\\(\\argmin_{\\hat{W}} \\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\|\\quad \\text{s.t.} \\hat{W} k_i^e = v_i^e \\quad \\forall i \\in [1, 2, \\cdots, E] \\\\Preservation\\_term=\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\\\ Memorization\\_term=\\hat{W} k_i^e = v_i^e \\quad \\forall i \\in [1, 2, \\cdots, E] \\\\ \\hat{W} = W_0 \u0026#43; \\Delta \\quad \\text{where} \\\\ \\Delta = (V_E - W_0 K_R)(K_E^T C_0^{-1}K_E)^{-1}K_E^TC_0^{-1} \\) How model editing performance is estimated? # Model performance is estimated with 4 main scores, and these scores are bsed on how model editing works with expressions of correct facts in \\((s,r,o^{c})\\) and false facts in \\((s,r,o^{*})\\) .\nEfficacy Score (ES) # ES measures if the new fact, which we want to edit, is successfully edited to model. It is measured by percentage where \\(\\mathbb{P}[o^*] \u0026gt; \\mathbb{P}[o^{c}]\\) , which means the portion of correct edition result from predictions.\nParaphrase Score (PS) # PS measures model\u0026rsquo;s ability to generalize following an edit. It is measured by where P(new fact) \u0026gt; P(old fact) under paraphrases of the query prompt.\nNeighborhood Score (NS) # NS represents the specificity of model editing. To measure NS, we collect a set of nearby subjects \\(s_n\\) for which \\((s_n,r,o^{c})\\) holds true. Then we test \\(\\mathbb{P}[o^*] \u0026gt; \\mathbb{P}[o^{c}]\\) , reporting the success fraction asn NS.\nComposite Score (S) # S represents the overall performance. It combines aspect of edit success, generalization, and specificity. It is calculated as the harmonic mean of Edit Success (ES), Paraphrase Score (PS), and Neighborhood Score (NS). It provies overall efficacy of model edits.\nExperiments \u0026amp; Results # What is the Optimal Layer for Model Editing? # Investigating the effectiveness of hidden states in LLMS for recalling facts using causal tracing showed that subject’s last token within the feed-forward networks at intermediate layer plays a significant role. (Meng et al., 2022b)\nMotivation : Later work showed that layers deemed important during causal tracing did not always translate to model editing performance. Therefore, this work focused on finding the optimal layer for model editing layer empirically.\nSteps for finding optimal layer\nMake 1000 non-sequential edits from the CounterFact (Meng et al., 2022a) dataset at each layer of the Llama-3 model. Calculate various model metrics(ES, PS, NS, S) to evaluate their impact. The layer that achieves the highest score is selected as the most suitable for targeted interventions. Fig 3. Post-edit performance of various metrics for Llama3-8b model using MEMIT and ROME on various layers. Eqch layer is edited with 1000 facts, one at a time and non-sequentially. Fig 4. Post-edit performance of various metrics on Llama2-7b for MEMIT on various layers. Evaluation results showed that layer 1 for Llama-3 outperformed on numerous metrics. Furthermore this trend was also shown in previous version, Llama-2, as seen in Figure 6. Here, MEMIT and ROME have very similar performance for model editing across layer of a model.\n→ Why? : Both algorithms optimize for the same objective with difference in the memorization constraints. This shows that memorization constraints plays minor effect on editing performance.\nOptimal way of Scaling Up model editing? # After finding the optimal layer, scaling of model editing on the same model can happen in two ways : batch editing \u0026amp; sequential-batched editing.\n1. Batch Editing :\nA large number(batch size) of knowledge edits are performed on the model with the same update. This work stick to editing a single layer of the model.\nExperimental settings\nTargeting layer1 in Llama-3 with batch size 16, 64, 256, 1024, and 4096 for Batched editing. Evaluation Results of Batch Editing\nFig 5. Various metric results (PS, NS, ES, S) after a batch edit (16, 64, 256, 1024, 4096) on MEMIT and EMMET respectively. For both MEMIT \u0026amp; EMMET editing, metrics are seen to consistently fall with larger batches, with NS being the most pronounced to fall. ES is most resilient metric to edits. PS, only metric to do so, seen to increase dramatically between batch sizes of 16 and 64. The similar trend between two editing techniques reflect the similarity in their optimization objectives.\n2. Sequential-batched Editing :\nSequential Editing is an alternate way to scale up model editing where facts are added sequentially to a model.\nThis work proposes optimal way to scale model editing that strikes a balance between Batch Editing \u0026amp; Sequential Editing.\nSequential-batched editing sequentially edit many batch of facts at a time. And the experiment was conducted going from batch size of 1 up to 4096. (1, 64, 256, 1024, 4096)\nFig 6. Single layer sequential editing performance for various batch sizes on MEMIT and EMMET respectively. Experimental results according to figures above showed that larger batch sizes are actually worse for model performance than sequential edits with smaller batches. In contrast, larger batch sizes seem to be better for metrics in NS : while batch edits are less successful in general, it is better in preserving locality of edits. This results were concluded to optimal batch size of 1024 for both MEMIT and EMMET. Increasing batch-size beyond that lead to larger model degradation and better editing results can be achieved by sequential-batched editing with smaller batch sizes.\nConclusion # This work examines several model editing techniques in the context of the newly released Llama-3 model and there are some conclusion as follows:\nEarlier layers may be more optimal intervention points. Model editing techniques that share same optimization objectives shows similar trends in layer and editing. Smaller, frequent sequential batch size edits have a superior performance. Batch size of 1024 for MEMIT and EMMET is optimal batchsize with sequential-batched editing. The authors argue that the current trend of pushing towards bigger edit batch sizes for scaling model editing may have limitations. Instead, they propose that future research should focus on methods that combine both batched and sequential editing to optimize performance while minimizing model degradation. Also, future work was proposed for experiments on multi-layer intervention for edits, as well as experiments against other popular models and algorithms, including methods that are hyper-network based.\nDiscussions, and research direction proposal from post writers # The paper empirically analyzes the performance of model editing based on batch size. It would be more beneficial for model editing research if the theoretical reasons behind the overall metrics decreasing as batch size increases are elucidated, rather than just empirically.\nWhile the work presents a hybrid format combining sequential editing and batch editing, it lacks in-depth analysis of the strengths and weaknesses of both approaches. Additionally, it is important to ensure that the individual characteristics of techniques such as ROME, MEMIT, and EMMET are appropriately integrated into editing optimization.\nAnalyzing the reasons behind the improvement in performance when layers are edited later in the network (NS) and the improvement when batch size is increased (PS) could help in identifying the optimal point for multi-layer editing\nIt seems necessary to investigate how many layers should be edited in multi-layer editing to achieve effective results beyond single-layer editing.\nReferences # Implementation code: Link.\nYunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang. 2023. Editing large language models: Problems, methods, and opportunities. arXiv preprint arXiv:2305.13172.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, Artem Babenko. 2020. Editable neural networks. arXiv preprint arXiv:2004.00345.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022a. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359–17372.\nKevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. 2022b. Massediting memory in a transformer. arXiv preprint arXiv:2210.07229.\nAkshat Gupta, Dev Sajnani, and Gopala Anumanchipalli. 2024. A unified framework for model editin. arXiv preprint arXiv:2403.14236.\n"},{"id":2,"href":"/docs/spring24/02_/","title":"02","section":"Spring24","content":" Spectrally Pruned Gaussian Fields with Neural Compensation (SUNDAE) # *Authors: Yang, Runyi, et al\n*Team: Donggeon Lee, Chiho Yoon\nSummary # 3D representation, which is the basis for many VR/AR and robotics applications, has long been an area of interest in computer vision and graphics. With the advent of neural radiation fields (NeRFs) Link, several methods have emerged to improve the quality and efficiency of NeRFs.\nConventional 3D Gaussian Splatting (3DGS) # Fig. 1. Comparison of 3D Gaussian Splatting to previous NeRF technologies. One of the recent hot topics in the NeRF field, 3D Gaussian Splatting (3DGS), demonstrates high quality and particularly fast, near real-time rendering speeds (about 100FPS).\nPros: Superior rendering speed and quality Cons: High memory consumption Proposed SUNDAE # Fig. 2. Comparison of 3D gaussian splatting and proposed SUNDAE It constructs a memory-efficient Gaussian field using spectral pruning and neural compensation. It considers the relationship between primitives, reducing memory usage while maintaining rendering quality. It significantly reduces memory consumption while preserving high rendering quality. Code: https://runyiyang.github.io/projects/SUNDAE/. Introduction # Fig. 3. Conceptual illustration of vanilla 3DGS, SUNDAE spectral pruning technique, and neural compensation. 3D Gaussian Splatting (3DGS) Link # Recently, 3DGS has been proposed as a novel 3D scene representation, utilizing a set of 3D positions, opacity, anisotropic covariance, and spherical harmonic (SH) coefficients to represent a 3D scene (left panel of Fig. 2). 3DGS demonstrates notable advantages in rendering speed, rendering quality, and training time. But it requires a large storage.\nSpectral graph pruning # Gaussian fields utilize a collection of Gaussian primitives as the representation of the scene. As these primitives are irregularly distributed in 3D space, they propose a graph-based data structure, rather than regular structures like grids, to capture the relationship between these primitives (middle panel of Fig. 2).\nNeural compensation # To address an inevitable decrease in rendering quality, they employ a neural compensation head to compensate for this quality loss (right panel of Fig. 2).\nContributions # A newly proposed primitive pruning framework for Gaussian fields based upon the spectrum of primitive graphs. A novel feature splatting and mixing module to compensate for the performance drop caused by the pruning. State-of-the-art results, in terms of both quality and speed, on various benchmarks with low memory footprint. Methods # We have 4 steps for the method\n3D Gaussian Splatting Warm Up Spectral Graph Pruning Neural Compensation Continuous Pruning as a Strategy The overall framework is in Fig.\nFig. 4. The overall framework of SUNDAE with pipeline and graph-based pruning. Let’s see how each step works.\n1. 3D Gaussian Splatting Warm up # SUNDAE initialize Gaussian centers of the vanilla 3D Gaussian Splating as the first step for generating a dense representation using Gaussian primitives. Then, an effective densification strategy is used to increase the primitives.\n- Gaussian Primitive Initialization # The point cloud $P_c$ is the first input for representing the 3D scene using Gaussian primitives $P$. Then they turn 3D coordinates $x \\in P_c$ into Gaussian primitives $p \\in P$ by the following equation:\n$p(x)=exp(-1/2(x)^T\\Sigma^-1(x))$ where the $\\Sigma$ is defined as 3D covariance matrix. They also use an ellipsoid configuration. The decomposition of $\\Sigma$ is achieved with scaling matrix $S$ and a rotation matrix $R$, as expressed in the equation:\n$\\Sigma=RSS^TR^T$ - Gaussian Primitive Densification # They optimize all parameters of Gaussian primitives and integrate a densification strategy to improve representation power during the training process.\n2. Spectral Graph Pruning # After warm-up, a dense representation incurs significant storage consumption. To efficiently prune redundant primitives, they used the graph signal processing theory and construct a graph based on Gaussian primitives.\n- Graph Signal Processing Preliminaries # Graph shift: Graph shift represents the connections between nodes in a weighted graph, typically represented by a weighted adjacency matrix. It quantitatively describes the relationships between nodes using the weights of edges.\nGraph signal: Graph signal is a mapping assigning values to each node in a graph, utilized to model interactions between nodes.\nGraph Fourier Transform: Graph Fourier Transform is the process of expanding a graph signal using the eigenbasis of the graph shift, enabling analysis of the structure and interactions within the graph.\n- Graph Construction # Given a set of Gaussian Primitives %P% , they construct a nearest neighbor graph with the adjacent matrix $W$ of the graph:\n$$ W_{ij} = \\begin{cases} \\exp\\left( -\\frac{\\|x_i - x_j\\|^2}{2 \\sigma^2} \\right), \u0026 \\text{if } \\|x_i - x_j\\|^2 \u003c \\tau \\\\ 0, \u0026 \\text{otherwise} \\end{cases} $$ where $x_i$ and $x_j$ are central points in $P$, $\\tau$ is a hyperparameter, and $\\sigma$ is the variance of the distance matrix.\n- Graph Filtering and Sampling # SUNDAE propose a band-limited graph filter that combined with a high-frequency filter and low-frequency filter. By doing this, they can catch both the detailed information and general information. Design of filters are Haar-like.\nThey also prune the abundant primitives according to the response magnitude of the high-pass filter.\n3. Neural Compensation # There is a decrease in rendering quality for large pruning ratio. To address this, they employ a neural compresation network to model the relationship between primitives in the 2D domain.\nThey render the 3D Gaussian primitives into neural images in a differentiable manner, using the differentiable 3D Gaussian renderer from 3DGS with feature rendering instead of RGB rendering. The center of each Gaussian primitive is projected using a standard point rendering method, and the covariance in the neural image space is calculated.\nThe neural image is then computed using the feature vectors of Gaussian primitives. A lightweight neural network (U-Net with skip connections) is used to compensate for the quality drop after spectral pruning.\nThe overall optimization process is based on the difference between the rendered images and the ground truth images from the dataset. The compensation network and the 3D Gaussian primitives are optimized simultaneously during training, using a loss function that combines L1 loss and D-SSIM loss.\n4. Continuous Pruning as a Strategy # In addition to the training-then-pruning strategy, a continuous pruning strategy is explored. Continuous pruning periodically removes a specific number or percentage of primitives during the training process. This aims to lower peak memory usage and allow training on GPUs with lower memory. However, it can result in less predictable final memory usage, as the reduction may vary across different scenes. Therefore, continuous pruning is considered an alternative strategy when needed.\nResults # Quantitative Results # Table 1. Quatitative evaluation of SUNDAE. SUNDAE demonstrates strong performance across various metrics, including PSNR, SSIM, FPS, and memory usage.\nCompared to existing methods on the MipNeRF360 dataset, SUNDAE achieves a balance between rendering quality and efficiency, maintaining high FPS rates while significantly reducing memory consumption. Even at low sampling rates, SUNDAE remains competitive with established approaches, showcasing the effectiveness of its spectral pruning and neural compensation techniques in managing Gaussian primitive relationships and retaining scene information. Overall, SUNDAE represents scenes more compactly while maintaining high quality rendering. Qualitative Results # Fig5. Qualitative results of SUNDAE. The qualitative results demonstrate that SUNDAE achieves comparable novel view synthesis quality with significantly lower memory consumption (1% or 10%).\nThe graph effectively captures primitive relationships, while the neural compensation head preserves rendering quality. Spectral pruning notably removes outliers near the camera, enhancing scene coherence. Ablation Study # Fig6. Ablations experiment on the ratio 𝛾 of the bandlimited filter of graph based pruning. Band-limited ratio of Graph-based pruning: The band-limited filter\u0026rsquo;s ratio, represented by 𝛾, significantly impacts rendering quality, with a 𝛾 value of 50% yielding the most favorable outcomes, emphasizing the advantage of spectral pruning in preserving important high-frequency details and low-frequency background (Fig. 4). Table 2. Ablations of neural compensation module size. Fig. 7. Visualization with and without neural compensation. The compensation performance of the network: Employing the neural compensation module enhances performance across all sampling rates evide(Table 2, Fig. 5), highlighting its compensatory capability in mitigating performance drops caused by spectral pruning and effectively modeling the relationship between primitives. Neural Compensation Module Size: Increasing the size of the neural compensation module does not necessarily enhance rendering quality (Table 2), aligning with findings from ADOP and indicating a balance between quality and memory usage. Conclusion # They propose SUNDAE, a novel approach to spectrally prune Gaussian fields with neural compensation, efficiently capturing the relationship between Gaussian primitives using graph signal processing and blending information to offset pruning-induced information loss. By leveraging spatial information among Gaussian primitives to construct a graph and spectrally pruning less significant ones, they employ a lightweight neural network to compensate for quality degradation post-pruning. Experimental findings demonstrate SUNDAE\u0026rsquo;s ability to maintain the efficiency of 3DGS while significantly reducing its size across various scenarios. "},{"id":3,"href":"/docs/spring24/03_/","title":"03","section":"Spring24","content":" Boosting Efficiency in Deep Learning: Introducing Unit Scaling for Low-Precision Training # Paper : Unit Scaling Out-of-the-Box Low-Precision Training Author : Charlie Blake, Douglas Orr, Carlo Luschi posted by Seongrok Moon, Changyoung Ju Introduction # The significant advances in deep learning over the past decade have largely relied on the development of algorithms that efficiently leverage available hardware. As the size of state-of-the-art models increases, hardware efficiency becomes crucial for reducing training costs, which have grown substantially in terms of money, time, and environmental impact. However, with the end of Moore\u0026rsquo;s Law and Dennard scaling, increased transistor density alone cannot provide a straightforward path to greater efficiency. The use of low-precision number formats is a promising alternative. These formats offer substantial gains in compute, memory, and bandwidth efficiency, making them valuable in the context of modern deep learning.\nBackground # Floating-Point Formats for Deep Learning # Traditionally, floating-point numbers are defined by the IEEE 754 standard, which specifies the number of exponent bits (E) and mantissa bits (M). Common floating-point formats used in machine learning include FP32, TF32, BFLOAT16, and FP16. Recently, two types of FP8 formats (E4 and E5) have been proposed.\nTable A.1. Common floating point formats for deep learning Advantages and Disadvantages of Low-Precision Training # Disadvantages: FP16 and BFLOAT16 offer different trade-offs. FP16 has higher precision, but BFLOAT16 has a wider range. FP8 formats reduce both range and precision. The use of low-precision formats can introduce quantization noise and other issues. Advantages: Using low-precision formats can significantly improve efficiency in terms of memory usage, bandwidth usage, compute performance, and cross-device communication costs. Figure 2. The signal to noise ratio (SNR) of samples from a normal distribution, quantised in FP16 and FP8, as a function of the distribution’s scale Techniques for Low-Precision Training # Mixed Precision: This technique uses multiple number formats with different bit-widths, placing most activations, weights, and gradients in FP16 without loss of accuracy.\nLoss Scaling: To overcome the limited range of FP16 and FP8, the loss can be multiplied by a scalar to increase the scale of gradients. This method requires empirically finding a suitable loss scale:\n\\[ \\text{scaled\\_loss} = \\text{loss} \\times \\text{scale\\_factor} \\] \\[ \\text{scaled\\_gradients} = \\text{gradients} \\times \\text{scale\\_factor} \\] Automatic Loss Scaling: This dynamically adjusts the loss scale during training, removing the need to sweep for an initial loss scale.\nPer-Tensor Scaling: This system locally rescales based on runtime statistics to address scaling difficulties in FP8 training.\nTable 1. A comparison of techniques for low precision training Analysis # Ideal Scaling # The ability to predict the scale of tensors at the start of training is crucial. We argue that unit variance ( \\(\\sigma = 1\\) ) is an optimal balance among various competing factors. This approach helps concentrate values within the representable range, reducing clipping errors during training.\nIn floating-point formats, values are represented as: \\[ \\text{value} = (-1)^{b_{\\text{sign}}} \\times 2^{\\text{exponent}} \\times \\left(1 \u0026#43; \\frac{b_{\\text{mantissa}}}{2^M}\\right) \\] where \\(b_{\\text{sign}}\\) , \\(b_{\\text{exponent}}\\) , and \\(b_{\\text{mantissa}}\\) represent the sign, exponent, and mantissa bits, respectively.\nFigure 1. Above: Unit scaling of an FFN layer. We multiply each tensor by a fixed scalar to achieve consistent scale, no longer requiring a loss scale to control the scale of gradients. Below: A histogram of exponent values at initialisation for the above FFN Predictable Scaling # If we can predict the scale of tensors in a deep learning model, we can effectively address clipping errors. At initialization, parameters are drawn from known distributions, allowing us to analytically or empirically derive the scale of each tensor.\nFor example, by considering the scaling factors for each operation in the neural network, we can perform scaled operations:\n\\[ y = \\alpha \\cdot f(x) \\] where \\(\\alpha\\) is the scaling factor and \\(f\\) represents the operation.\nUnit Scaling # Unit scaling is proposed to address the limitations of existing methods for managing scale in typical models. A model is considered unit-scaled if its activations, weights, and gradients have approximately unit variance at initialization. This is achieved by inserting scaling factors into the forward and backward passes. Unlike loss scaling, which requires an empirically determined hyperparameter or an adaptive algorithm, unit scaling determines these scales based on a set of rules for each operation, approximately preserving the variance of the inputs. This leads to global unit scaling throughout the model, ensuring tensor values are centered within the exponent range at initialization, providing headroom during training to avoid going out of range.\nA framework for scaling computational graphs # Computational Graphs\nRepresent model by the differentiable function \\(f_{model}(x_1,...,x_m)\\) Describe the structure of such a model using a directed acyclic graph (DAG) denoted \\(\\mathcal{G} =(\\mathcal{V}, \\mathcal{E}) \\) This kind of graph is commonly known as a computational graph, with vertices as nodes and their corresponding functions as ops. Forward and backward graphs\nWe refer to the computational graph corresponding to \\(f_{model}\\) as the forward graph In deep learning we typically apply reverse-mode automatic differentiation to the forward graph to create a second computational graph whose output nodes represent the partial derivatives of the model with respect to its inputs: \\( \\frac{\\partial f_{model}}{\\partial x_i}, \\forall i \\in[1 . . m] \\) . We call this the backward graph Scaled ops\nGiven an op \\(f\\left(x_1, \\ldots, x_k\\right)\\) , we define the scaled op \\( f^*\\left(x_1, \\ldots, x_k, \\alpha, \\beta_1, \\ldots, \\beta_k\\right) \\) with scaling factors \\( \\alpha, \\beta_1, \\ldots, \\beta_k \\in \\mathbb{R}^{\u0026#43;} \\) , such that \\( f^{*} \u0026amp; \\triangleq \\alpha \\cdot f(x_1, \\ldots, x_k)\\) \\( f_{\\text {grad }}^{*}\\left(x_1, \\ldots x_k, g\\right)_i \u0026amp; \\triangleq \\beta_i \\cdot f_{\\text {grad }}\\left(x_1, \\ldots x_k, g\\right)_i, \\forall i \\in[1 . . k] \\) Scaled computational graph\nA scaled computational graph is one where every op \\(f\\) in the forward graph is replaced by a scaled equivalent \\(f^{*}\\) , with the backward graph then generated to produce \\(f^{*}_{grad}\\) grad for each \\(f_{grad}\\) , using any choice of scaling factors. Constraint-scaled computational graphs\nA constraint-scaled computational graph is a scaled computational graph where we restrict the scaling factors of ops that consume non-cut-edge variables in the following way: for any edge \\(e \\notin \\mathcal{C}\\) , we require the op consuming the variable \\(x_e\\) to have scaling factors \\(\\alpha = \\beta_e f\\) . Proposition 5.1\nFor any scaled op, there is an equivalent unscaled op with the same training dynamics under a firstorder optimiser.\nTheorem 5.2\nA constraint-scaled computational graph itself represents a scaled op.\nA scaling strategy for unit variance # Unit scaled computational graphs\nInitially set aside any scale constraints, and calculate the scaling factors that give each op expected unit variance outputs (this process is covered below). Now resolve any scale constraints by taking each constrained group \\( {\\alpha, \\beta_1, \\ldots, \\beta_l } \\) and selecting the geometric mean \\( \\left(\\alpha, \\beta_1, \\ldots, \\beta_l \\right)^\\frac{1}{l\u0026#43;1} \\) Selecting scaling factors\nAssuming unit-scaled inputs to \\( y = f(x_i,\\ldots,x_k) \\) , derive the output scale \\( \\sigma_Y \\) and set the forward scaling factor \\( \\alpha = 1/\\sigma_Y \\) . Repeat this process for \\( x_i\u0026#39;=f_{grad}(\\ldots)_i, \\forall i \\in[1 . . k] \\) , to obtain the gradient scale \\( \\sigma_{x_i\u0026#39;} \\) i and set the backward scaling factor \\( \\beta_i = 1/\\sigma_{x_i\u0026#39;} \\) . Weighted addition # When tensors of different scales, such as those in residual layers, losses, and positional encodings, are added, simply adding them can adversely affect performance. To address this, we propose using weighted_add. In this approach, we can maintain unit scale while performing operations using a scaled identity function.\nRecipe # We now outline a high-level recipe for a unit-scaled model:\nInitialise non-bias parameters with unit variance. Calculate scaling factors for all scaled ops. Identify non-cut-edges, and constrain the ops consumingthem to have \\( \\alpha = \\beta \\) by taking the geometric mean. Replace adds with weighted adds. Example # Using the unit scaling recipe, we first build a scaled op, and then a full scaled layer. Consider a scaled projection op with learnable weights:\n\\( \\operatorname{matmul}^*(X,W) =\\alpha \\cdot X W \\) \\( \\operatorname{matmul}_{\\text {grad }}^*(X, W, G)_1 = \\beta_1 \\cdot G W^{\\top} \\) \\( \\operatorname{matmul}_{\\text {grad }}^*(X, W, G)_2 = \\beta_2 \\cdot X^{\\top} G \\) for input \\( X \\in \\mathbb{R}^{b \\times m} \\) , weight \\( W \\in \\mathbb{R}^{m \\times n} \\) , output \\( \\mathbb{R}^{b \\times n} \\) and incoming gradients \\( G \\in \\mathbb{R}^{b \\times n} \\) We show code for the above in Figure 3, which also gives a scaled layer for the Transformer FFN\nFig3. PyTorch examples Results # Character language modelling\nExperimental Setup: Train causal language models on WikiText-103 raw character language modeling, using cross-entropy loss during training and evaluating on bits per character (BPC). Below the product of these settings, we compare the performance of regular (baseline) and unit scaling in both FP32 and FP16.\nSequence layer type: Attention, RNN and Convolution Norm placement: PreNorm, PostNorm and NoNorm Residual scaling: default, fixed and running-mean Results\nFirst, these demonstrate the need for scaling when using FP16. This is due to gradient underflow, since loss scaling with a factor of 2048 resolves the issue. Second, they demonstrate that unit scaling, despite changing the training behaviour of the model beyond just numerics, matches or even slightly improves upon baseline performance in almost all cases. Finally, they show that no tuning is necessary when switching unit scaling to FP16. suggest that running-mean or fixed are reasonable choices when using unit scaling Fig4. Character language modelling, showing validation bits per character over a wide range of models Masked language modelling\nExperimental Setup\nTo evaluate the advantages of unit scaling, we assess BERTBASE and BERTLARGE models, which typically struggle with loss scaling. Results\nTable2. Downstream performance of regular and unit-scaled BERT models Related Work # Variance scaling analysis\nVariance scaling and residual networks, along with normalization variants, complement unit scaling, which considers both activation and gradient norms. The reparameterization implied by unit scaling, utilized in analyzing deep network training dynamics, applies scaling factors locally throughout the compute graph, akin to training hyperparameter scaling. FP8 inference\nFP8 training lacks hardware support, yet accelerated 8-bit inference is becoming more prevalent through integer quantization to INT8. While this process often leads to reduced accuracy, recent efforts aim to enhance efficient INT8 quantization. FP8 adoption allows accelerated inference in the same format as training, promising significant improvements in the simplicity and accuracy of 8-bit inference. Discussion # Compute overhead\nUnit scaling introduces minimal compute overhead by adding scaling operations that can be fused into preceding operations, resulting in negligible memory-access cost. While basic loss scaling operates similarly, automatic loss scaling may incur additional overhead due to occasional batch discards, particularly noticeable in FP8. Proposed automatic per-tensor scaling schemes may introduce overhead, depending on software and hardware characteristics, as they trade off accuracy for complexity. In contrast, unit scaling with fixed precomputed scaling factors offers a simpler alternative without such complexities. Broader impact\nWith the potential for unit scaling to effectively train larger models, concerns arise about issues such as toxicity, misinformation, privacy concerns, and environmental damage. To address these challenges, various methods have been proposed, including AI feedback, anti-experts, and baked-in safety models. Conclusion Unit scaling has demonstrated to address the complexities of low-precision training, providing a simpler and more granular solution, even enabling the training of BERTLARGE without loss scaling for the first time, even in FP8.\n"},{"id":4,"href":"/docs/spring24/04_/","title":"04","section":"Spring24","content":" Better \u0026amp; Faster Large Language Models via Multi-token Prediction # Posted by Jinoh Cho and Seonghyeon Park\nAuthors: Gloeckle et al. Institution : FAIR at Meta, CERMICS Ecole des Ponts ParisTech and LISN Universite Paris-Saclay Motivation # Conventional LLM trained with a next-token prediction loss latches on local patterns and overlooks “hard” decisions. Author wants to alleiviate this issue by pre-training LLM with multi-token prediction loss.\nPreliminaries # Language Modeling and Next-Token Prediction Task # Learning through a next-token prediction task has been a mainstream for language modeling. The goal of a next-token prediction task is to maximize the probability of the next token $x_{t+1}$, given the history of previous tokens $x_{t:1} = x_1, \\ldots, x_t$. This can be formulated as follow:\n$$ L_1 = - \\sum_{t} \\log P_{\\theta}(x_{t+1} \\mid x_{t:1}), $$\nwhere $P_{\\theta}$ represents large language model under training.\nCore Idea # Multi-Token Prediction Task # In this work, authors propose to learn language modeling from a multi-token prediction rather than a next-token prediction. At each position of the training corpus, the model is instructed to predict $n$ future tokens at once. Thus, the training objective is changed as follow:\n$$ L_n = - \\sum_{t} \\log P_{\\theta}(x_{t+n:t+1} \\mid x_{t:1}) = - \\sum_{t}\\sum_{i=1}^{n} \\log P_{\\theta}(x_{t+i} \\mid x_{t:1}). $$\nMemory-Efficient Implementation # Directly training language models by minimizing the multi-token prediction loss could result in high GPU memory usage, severly limiting the allowable batch-size. Thus, authors propose to carefully adapt the sequence of forward and backward operations for each prediction head rather than operating forward and backword operations simultaneusly for all heads. This could result in reducing peak GPU memory usage $O(nV+d)$ into $O(V+d)$. Here, the $n$ and $V$ denote the number of head and vocabulary size, respectively. Note that $d$ is the vector dimension of shared transformer trunk.\nFaster Inference with Self-Speculative Decoding # For speed up in inference time, authors utilize self-speculative decoding (Stern et al., 2018) scheme. Specifically, instead of iteratively predicting a next single token for the given token sequence, authors directly generate n-token using n independent output heads in a single step. This significantly speed up the decoding stage.\nResult # Learning global patterns with multi-byte prediction # To show using multi-token prediction loss helps to capture global pattern than using next-token prediction loss, they include experiment using extreme case of byte-level tokenization. Notably, as shown in the table 1, multi-token prediction (8-byte prediction) models significantly solve more problem in the case of trained on small number of data.\nCoding Benchmarks # Pre-trained model with multi-token prediction loss maintains an edge on that with next-token prediction loss. At the beginning, they pre-train the 7B parameter models with multi-token prediction loss or next-token prediction loss. (Use the pre-trained model on MBPP, HumanEval and APPS) Then, they finetune the models with CodeContests dataset (Li et al., 2022) with multi-token head or next-token head.\nNatural Language Benchmarks # Choice Tasks : Multiple token training with 7B models doesn’t improve performance on choice tasks Summarization : Multi-token prediction models with both n = 2 and n = 4 improve over the next-token baseline in ROUGE-L F1 scores for both training dataset sizes, with the performance gap shrinking with larger dataset size. Mathematical Reasoning : After 200B tokens, the 2-token prediction model has a clear advantage over the next-token baseline but the order reverses after 500B tokens. The 4-token prediction model is worse throughout. Why does it work, Athors\u0026rsquo; Speculations? # Lookahead reinforces choice points # Multi-token prediction assigns weights to training tokens based on their correlation with successors. Difficult-to-predict choice points receive higher weights compared to inconsequential transitions. The weighting system allocates n(n+1) points to correlated tokens and n points to inconsequential ones.\nInformation-Theoretic View # Next token prediction loss involves $H(X) = H(X | Y) + I(X; Y)$, while multi-token prediction loss involves $H(X) + H(Y) = H(X | Y) + 2I(X; Y) + H(Y | X)$. This indicates that multi-token prediction places twice the emphasis on the mutual information term compared to next token prediction. Essentially, 2-token prediction encourages models to precompute features useful for predicting the next token ( Y ), thereby increasing the importance of the mutual information term in the loss calculation.\nCompare with similar works # Qi et al. (2020) argue that multi-token prediction encourages planning, improves representations and prevents the overfitting on local patterns that can result from teacher-forced training. However, their technical approach replicates the residual stream n-fold while ours allows for compute-matched comparisons and makes the residual representations participate more directly in the auxiliary loss terms. Stern et al. (2018) and Cai et al. (2024) propose model finetunings with multi-token prediction for faster inference but do not study the effects of such a loss during pre-training. Conclusion # Author propose using multi-token prediction loss instead of next-token prediction loss for pre-training language models. This pre-training scheme has shown improvements across various tasks, notably in code tasks.\nDiscussion # For each task and dataset, the optimal number of heads $n$ varies. I would like to see more correlations between dataset characteristics and the optimal $n$. I would like to see more evidence of the global capturing ability of multi-token prediction loss in other experimental settings. Authors demonstrate cases including $(n=4, n\u0026rsquo;=1)$, $(n=4, n\u0026rsquo;=1)$, and $(n=1, n\u0026rsquo;=1)$ on the code task, where $n$ and $n\u0026rsquo;$ denote number of pre-training head and that of finetuning head. However, I would like to see the $(n=1, n\u0026rsquo;=4)$ setting result on the code task. Conducting this experiment would help determine if the $n=4$ case still outperforms the $(n=1, n\u0026rsquo;=4)$ setting. If so, the author\u0026rsquo;s argument for pre-training with the multi-task prediction scheme would be further substantiated. I would like to study more about multi-token prediction head experiments on multi-token prediction loss with stride=2,4,8,16 \u0026hellip; (This paper shows only with stride=1 setting, predicting consecutive tokens) or extremely large number of multi-token prediction head setting. Reference # Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Block-wise parallel decoding for deep autoregressive models, 2018.\nWeizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou. Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training, 2020.\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads, 2024.\n"},{"id":5,"href":"/docs/spring24/05_/","title":"05","section":"Spring24","content":" Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting # Author: Liu F, Tang Y, Liu Z, Ni Y, Han K, Wang Y Written by Nayoung Kwon and Jiwoong Im\nIntroduction # The growing demand for rapid and efficient inference in large language models (LLMs) faces a significant bottleneck.\nDecoding \\(N\\) tokens requires \\(N\\) sequential runs of the model. ⇒ LLM inference is slow.\nTo address this issue, speculative decoding has been introduced as a promising approach to accelerate LLM inference without altering the output quality. This method leverages two key observations about LLM inference:\nMany tokens can be predicted with minimal computational overhead. LLM inference is predominantly constrained by memory bandwidth rather than arithmetic computations. Speculative decoding reduces the need for frequent memory operations on their parameters by focusing computational efforts on validating pre-drafted tokens, thus enhancing inference efficiency. However, existing speculative decoding such as Medusa [4] and Lookahead [3] still face limitations, such as high inference latency and suboptimal token acceptance rates. The paper proposes Kangaroo to address this challenge.\nBackgrounds # Speculative Decoding # Speculative decoding is an approach to accelerate LLM inference.\nDraft model: Additional model to accelerate inference (also known as drafter).\nVerifier or target model: Original LLM.\nFig. 1. Contrast between autoregressive decoding and speculative decoding. Left model: The target LLM generates \\(N\\) tokens in \\(N\\) forward steps, which is a \u0026ldquo;serial\u0026rdquo; process.\nRight model: The drafter generates tokens in parallel. Each generated token is then verified with a verification step.\nSpeculative decoding can be implemented through methods such as independent drafting and self-drafting.\nIndependent Drafting: This approach uses a small language model (LM) from the same series as the target LLM.\nRequires additional training and increases computational complexity by integrating separate target and drafting models. Self-Speculative Decoding: This method utilizes the target LLM itself.\nEmploys techniques such as Blockwise Decoding, Medusa, and early exiting to reduce computational burden. Computational efficiency can also be achieved through layer skipping. Kangaroo: Self-speculative decoding # Kangaroo refers to the self-speculative decoding method, utilizing a fixed shallow sub-network of the original (target) large LLM.\nFig. 2. Comparison of various self-drafting speculative decoding methods. In each decoding step, drafted tokens must be verified in parallel to ensure alignment with the target LLM, which determines the token acceptance rate. High token acceptance rates are crucial for the efficiency of this process. However, methods like Medusa have yet to achieve satisfactory token acceptance rates, as evidenced by performance metrics (see left graph). On the other hand, the Lookahead method achieves a high token acceptance rate but has a very low speedup ratio (see right graph). Addressing these trade-offs, **Kangaroo** offers a solution by training a lightweight and efficient adapter module integrated with a fixed subnetwork of the target LLM, enhancing both the acceptance rate and overall speedup. Kangaroo # The author has proposed a novel self-speculative decoding framework, named Kangaroo. Kangaroo utilizes double early exiting mechanisms, layer early exiting and draft early exiting.\nLayer early exiting suggests the equivalent self-draft small model exiting early from the fixed shallow layers of the large LLM and connecting to an adapter network to generate draft tokens. While this strategy is commonly used for self-speculative decoding frameworks, Kangaroo has further investigated suitable architectures of the adapter module and offered a low-cost approach to train a lightweight model. Draft early exiting uses early exiting at suitable points during the drafting phase to avoid unnecessary computational overhead on more challenging tokens. Fig. 3. The framework of Kangaroo. Fig. 3 illustrates the framework of Kangaroo.\nThe lightweight drafting model \\( \\mathcal{M}^s \\) , including shallow sub-network and adapter network, predicts the draft tokens autoregressively until draft early exiting occurs. \\(\\mathcal{M}^s\\) stops drafting once the confidence level of the current draft token falls below a certain threshold, e.g., \\(\\mathcal{M}^s(x_3\u0026#39;) \\leq \\eta\\) . The hidden states, computed in shallow sub-network, are processed in the remaining layers of LLM to generate prediction results. The draft tokens and the original prediction results are compared for verification, and once a draft token is rejected, the subsequent tokens are discarded. The token \\(x_4\\) that has resulted in an uncertain drafting is also predicted at the verification step. If all draft tokens are accepted, the next decoding step starts with the prediction result of the last token \\(x_4\\) . (It is not necessary to predict \\(x\u0026#39;_4\\) once more.) Evaluation Metrics # Speculative decoding is often evaluated using two primary metrics: walltime speedup ratio and compression rate. Given a speculative decoding algorithm, we assume that \\(N\\) tokens should be generated via the drafting model. As the drafting model predicts multiple tokens in each decoding step and multiple tokens can be accepted by the large model in a step, we record the number of accepted tokens per step as a list \\( S = [s_1, s_2, \\dots, s_{|S|}] \\) , where \\( \\sum_k s_k = N \\) and \\( |S| \\) denotes the number of steps. Then, the compression rate (CR) is defined as: \\[\\text{CR} = \\frac{1}{|S|} \\sum_k s_k.\\] However, once a draft token is rejected during the verification, all subsequent tokens sampled from the drafting model will be discarded. Therefore, CR does not accurately reflect the acceptance levels for tokens at varying distances, and the author has proposed a new evaluation metric named consistent token acceptance rate.\nThe consistent token acceptance rate \\( \\text{CTAR}(w) \\) is calculated as: \\[\\text{CTAR}(w) = \\frac{1}{|S|} \\sum_k \\mathbb{I} (s_k - w \u0026gt; 0),\\] where \\(\\mathbb{I}(\\cdot)\\) denotes an indicator function and \\( w \\) denotes a window size. CTAR can be interpreted as a rate of the number of steps to accept over \\( w \\) tokens. Fig. 2 represents the empirical CTARs for \\(w = 1,2,\\dots,6 \\) of self-drafting speculative decoding frameworks including Kangaroo on the mathematical reasoning subtask of Spec-Bench [1].\nAdapter Network as Self-Draft Model # We assume the target LLM has \\( L \\) layers and the self-draft model \\( \\mathcal{M}^s \\) consists of shallow sub-network \\( \\mathcal{M}^b[:l] \\) , which is first \\( l \\) layers of the target LLM \\(\\mathcal{M}^b\\) , and a adapter network \\( \\mathcal{A} \\) . The drafting model reuses the LM head of the target LLM, and the overall parameters of the target LLM, such as shallow sub-network, remaining layers of LLM, and LM head, are frozen during the training of the adapter network. In Kangaroo, the adapter \\( \\mathcal{A} \\) only encompasses one multi-head attention and two normalization layers. The author emphasizes the feed-forward network (FFN) of the transformer block is too heavy in parameters but redundant, which is presented in the ablation study of the adapter architecture in the Experiments Section.\nThe training loss of the self-draft model is formulated by the cross-entropy loss: \\[\\mathcal{A}^* = \\text{arg}\\min_\\mathcal{A} \\sum_t \\sum_n -\\mathcal{M}_n^b (x_t) \\log \\mathcal{M}_n^s (x_t).\\] Draft Early Exiting # While speculative decoding typically drafts a fixed number of tokens per step, it leads to several problems of local optima, waste on challenging samples, and prolonged inference latency of the draft model. Therefore, the author proposes draft early exiting, which stops drafting once the top- \\(1\\) confidence on the self-draft model is below a predefined threshold \\(\\eta\\) , i.e., \\[\\max_n \\mathcal{M}_n^s (x) \\leq \\eta.\\] Experiments # The author has conducted experiments on Vicuna [2] models with sizes of 7B and 13B and compared Kangaroo with other self-speculative decoding approaches, Lookahead [3], Medusa [4], and REST [5], in Fig. 2 and Table 1.\nTable 1. Comparison of self-speculative decoding frameworks in terms of compression ratio (CR) and walltime speedup ratio (Speedup). Ablation Studies # Fig. 4. Ablation study for appropriate exit layer. Fig. 4 demonstrates the trade-off between the compression rate and the walltime speedup varying by the early exit layer \\(l\\) . Later exit layer leads to larger sub-network \\(\\mathcal{M}^b[:l]\\) and smaller remaining layers \\(\\mathcal{M}^b[l:]\\) , so an adapter network easily imitates remaining LLM layers but the self-draft model \\(\\mathcal{M}^s\\) becomes heavy. The paper set \\(l=2\\) for Vicuna-7B and \\(l=3\\) for Vicuna-13B.\nFig. 5. Ablation study for appropriate threshold. Fig. 5 demonstrates the effect of draft early exiting comparing the optimal threshold \\(\\eta\\) . \\(\\eta=0\\) points represent a typical self-speculative scheme with a fixed step strategy, which achieves the maximum compression rate but leads to sub-optimal end-to-end walltime speedup.\nTable 2. The architecture of the adapter module for Vicuna-7B. Table 2 demonstrates an ablation study on the architecture of the adapter module. In Table 2, removing the FFN and sharing the LM head of LLM (Kangaroo + Head) is most effective.\nDiscussion and Conclusion # Kangaroo with a double early-exit mechanism ensures both efficiency and high performance.\nSeveral advantages:\nLow-Cost Training: The shared KV cache and computation between the self-speculative draft model and the large LLM → only the adapter network requires additional deployment. Efficiency: Experiments on Spec-Bench demonstrate that Kangaroo achieves up to 1.7× speedup, outperforming existing methods with significantly fewer additional parameters (67M compared to 591M for Medusa). Flexibility: By focusing on reducing inference latency and optimizing token acceptance rates, Kangaroo ensures that performance remains robust across various tasks without incurring substantial overhead. Compare with others:\nKangaroo\u0026rsquo;s performance surpasses other speculative decoding methods, such as Medusa and Lookahead, particularly in terms of end-to-end speedup and token acceptance rates (see Fig. 2 in the backgrounds). The double early-exit mechanism plays a crucial role in maintaining this balance by efficiently handling easier tokens and exiting early when confidence is lower than the predefined threshold, thus minimizing latency.\nLimitations and future work: # Enhanced Confidence Metrics: Although Kangaroo introduces a confidence-based mechanism, it retains the original issue of discarding generated tokens that do not meet the confidence threshold. Currently, confidence is measured only on the top-1 prediction. Future work could explore alternative metrics such as entropy or other measures to provide a more robust assessment of token validity. Alternative Networks for Adapting: The use of an adapter network in Kangaroo shows promising results. However, experimenting with different network architectures could yield even better performance. Future research could investigate various types of networks to replace the adapter, potentially improving both the efficiency and accuracy of the speculative decoding process. Adapter network parameter can also be reduced by using model compression or quantization method. Adaptive Early-Exit Mechanisms: The current implementation of early exits in Kangaroo could be refined by dynamically adjusting the confidence thresholds based on the context or specific tasks. This adaptation could further reduce unnecessary computations and improve the overall efficiency of the model. References # [1] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851, 2024.\n[2] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna.lmsys.org (accessed 14 April 2023), 2(3):6, 2023.\n[3] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm inference using lookahead decoding. arXiv preprint arXiv:2402.02057, 2024.\n[4] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao. Medusa: Simple LLM inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024.\n[5] Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. Rest: Retrieval-based speculative decoding. arXiv preprint arXiv:2311.08252, 2023.\n"},{"id":6,"href":"/docs/spring24/06_/","title":"06","section":"Spring24","content":" # "},{"id":7,"href":"/docs/spring24/07_/","title":"07","section":"Spring24","content":" VeRA: Vector-based Random Matrix Adaptation [1] # EECE695E_2024_Spring_Blog for Efficient Machine Learning Class w/ Sejin Park and Kyumin Cho Introduction to LoRA (Low Rank Adaptation) [2] family of PEFT (Parameter Efficient Finetuning) # Large language Models or LLMs consists of at least billions of parameters. This makes it extremely expensive to run inference on the model or train and finetune it. For example, the weights of GPT-3 175B can take up to 350GB when stored in FP16 precision (2 bytes per FP16 x 175B=350GB) when used in inference. Training requires storing additional parameters such as the optimizer states and gradients in GPU VRAM in higher precision (FP32). Assuming FP32 training with AdamW optimizer, storing the model weights requires 4 bytes for each parameter in FP32, 8 bytes per parameter to for the optimizer AdamW (where two states, first moment and second moment in FP32, are maintained for each parameter), and 4 bytes per parameter to store the gradient in FP32. This adds up to 16 bytes of storage space needed for each model parameter required for training. [3] This shows that each trainable parameter adds a lot of overhead due to different types of data like optimizer state and gradients that needs to be stored.\nMemory Usage Breakdown for Training (FP32 AdamW) [3] # Component Memory Requirement (bytes) Model Weights - fp32 training 4 bytes * number of parameters Optimizer States - AdamW 8 bytes * number of parameters (2 states) Gradients 4 bytes * number of parameters Forward Activations Size depends on sequence length, hidden size, and batch size Temporary Buffers Size varies based on specific operations Functionality-specific memory Size varies based on additional functionalities Total (excluding variable sizes) 16 bytes * number of parameters This means that a full finetune of a small model such as Llama-3 8B can take 128GB (16 bytes x 8B = 128GB) just to store the parameters. This calculation excludes the forward activations as well as the training batch data which mean often higher amount of VRAM capacity is often needed. This makes even training relatively small models impossible on a single GPU as datacenter-class GPUs such as A100 or H100 max out at 80GB and especially difficult for consumer level GPUs such as the RTX 4090 which only has 24GB.\nNot only does the weights needs to be stored on the GPU VRAM during VRAM, each finetune version of the model needs to store the entire copy of the model. This means even if mass-storage devices like HDDs are used, it becomes prohibitively impossible to store multiple custom finetune version of the data itself.\nTherefore, parameter efficient finetuning or PEFT methods have been developed that is able to finetune and specialize LLMs by only using small amount of parameters, this not only reduces the number of GPUs required to train the model itself, it cuts down on the permanent storage capacity required to store multiple versions of it. The most popular of these approaches is low rank adaptation or LoRA. As its name suggests, this technique uses low-rank matrices to represent large matrices in LLMs. The hidden dimension size in LLMs gets very large with size with GPT-3 175B having a hidden dimension (d) of 12,288. By multiplying two matrices with extremely low rank (as low as 1 or 2), it is possible to represent a large matrix. By encoding changes to the original weight in this large matrix new versions of the model can be stored with a very small memory footprint. In the case of GPT-3 175B, the authors of the paper reported reduction as large as 10,000x (from 350GB to 35MB), with rank size of 4 and when being only applied \\(W_Q\\) and \\(W_V\\) projection matrices. This low rank reduction of changes is based on the assumption that the change in weights during finetuning has a low intrinsic rank, based on previous studies [7] [8] that claim that learned over-parameterized models reside in the low intrinsic dimension.\nSince only the differences to the original model are tracked in training, original model parameters can be frozen and only the small low-rank matrices need to be trained. Gradients or optimizer states don\u0026rsquo;t are not required for the original model, only for the small low-rank matrices, so this greatly reduces the GPU VRAM requirement. Also, when servicing large variations of custom finetune models, only a single copy of the large model needs to be stored and each version only needs to store the small low-rank matrices that represents the difference between the original weights. This makes servicing large number of variations feasible and makes switching model versions easy as only the small LoRA weights need to be loaded and merged without loading the entire model itself.\nLet the pre-trained weight matrix be \\(W_o \\in \\mathbb{R}^{d \\times k}\\) .\nThe modified weight matrix is given by:\n\\(W_o \u0026#43; \\Delta W = W_o \u0026#43; BA\\) where \\(B \\in \\mathbb{R}^{d \\times r}\\) , \\(A \\in \\mathbb{R}^{r \\times k}\\) , \\(\\text{rank } r \\ll \\min(d, k)\\\\) , and \\(\\\\Delta W = BA\\) .\nThe original forward pass is:\n\\(h = W_o x\\) The modified forward pass is:\n\\(h = W_o x \u0026#43; \\Delta W x = W_o x \u0026#43; B A x\\) This can be shown in the following diagram.\nIn LoRA, \\(W_o\\) matrix usually corresponds to \\(W_Q\\) , \\(W_K\\) , \\(W_V\\) , or \\(W_O\\) , query, key, value, and output projection matrices of attention as opposed to Feed Forward Networks (FFN) matrices as hidden size of FFNs tend to be much larger then projection matrices of attentions. The most common practice seems to be injecting LoRA into query and value though exact implementation can vary widely.\nDuring training \\(B\\) can be initialized as 0 so that \\(\\Delta W = B A\\) is also 0 when training starts.\nWhen LoRA weights are deployed the original weights and the LoRA weights can be merged, \\(W = W_o \u0026#43; B A \\) , before inference proceeds as usual. The original weights can be obtained by subtracting the LoRA weights ( \\(B A\\) ).\nUnlike other PEFT methods such as adapter layer insertion, LoRA adds no additional latency after the weights are merged as the forward inference operation is exactly the same and no additional operation needs to be performed. This contributed to the popularity of LoRA as no changes to the inference code needs to be made and only weight merging operations before inference are needed which is relatively quick and easy to perform.\nHow VeRA works # Even with parameter efficient nature of LoRA it still requires a non-trivial amount of storage for each version. If a custom version was wanted for each vendor or consumer the storage requirement can easily add up. Even for a PEFT technique like LoRA, a finetune version of GPT-3 175B with a low rank of 4 applied to only query and value projections needed several dozen megabytes in storage. If a custom version was to be stored for each users, a million users would amount to dozens of terabytes of storage. This limits the scalability of LoRA for personalization and demands an even more parameter efficient PEFT technique than LoRA which is where VeRA comes in.\nVeRA tries to take advantageof random matrices and projection to reduce the number of unique parameters needed for each finetune. The idea is to take a pair of randomly initialized matrices and attach a pair of scaling vectors that reparametrize it. The randomly initialized matrices remain frozen while the scaling vectors are trainable. If we use the same seed when generating the random matrices through a PRNG (pseudorandom number generator). We do not need to store the random matrices and only need to store the smaller scaling vectors. This greatly reduces the storage requirement of VeRA and allows larger ranks without drastically increasing the storage requirement.\nThis table taken from [1] shows the relative storage efficiency of VeRA compared to LoRA, when only applied to the query and key projection layers.\nModel Rank LoRA - # Trainable Parameters LoRA - Required Bytes VeRA - # Trainable Parameters VeRA - Required Bytes \\(\\{RoBERTa}_{\\text{base}}\\) 1 36.8K 144KB 18.4K 72KB \\(\\{RoBERTa}_{\\text{base}}\\) 16 589.8K 2MB 18.8K 74KB \\(\\{RoBERTa}_{\\text{base}}\\) 256 9437.1K 36MB 24.5K 96KB \\(\\{RoBERTa}_{\\text{large}}\\) 1 98.3K 384KB 49.2K 192KB \\(\\{RoBERTa}_{\\text{large}}\\) 16 1572.8K 6MB 49.5K 195KB \\(\\{RoBERTa}_{\\text{large}}\\) 256 25165.8K 96MB 61.4K 240KB GPT-3 1 4.7M 18MB 2.4M 9.1MB GPT-3 16 75.5M 288MB 2.8M 10.5MB GPT-3 256 1207.9M 4.6GB 8.7M 33MB Let the pre-trained weight matrix be \\(W_o \\in \\mathbb{R}^{d \\times k}\\) .\n(The parameters updated during training are underlined.)\nThe original LoRA formulation is the following:\n\\(W_o \u0026#43; \\Delta W = W_o \u0026#43; \\underline{B A}\\) where \\(B \\in \\mathbb{R}^{d \\times r}\\) , \\(A \\in \\mathbb{R}^{r \\times k}\\) , \\(\\text{rank } r \\ll \\min(d, k)\\\\) , and \\(\\\\Delta W = BA\\) .\nThe original forward pass is:\n\\(h = W_o x\\) The LoRA forward pass is:\n\\(h = W_o x \u0026#43; \\Delta W x = W_o x \u0026#43; \\underline{B A} x\\) In the case of LoRA low-rank matrices \\(A\\) and \\(B\\) are updated.\nFor VeRA the following formulation can be used:\n\\(W_o \u0026#43; \\Delta W = W_o \u0026#43; BA = W_o \u0026#43; \\underline{\\Lambda_b} B \\underline{\\Lambda_d} A \\) The VeRA forward pass being:\n\\(h = W_o x \u0026#43; \\Delta W x = W_o \u0026#43; \\underline{\\Lambda_b} B \\underline{\\Lambda_d} A x\\) In the case of VeRA \\(B\\) and \\(A\\) matrices are frozen and randomly initialized. Scaling vectors \\(b \\in \\mathbb{R}^{1 \\times d}\\) and \\(d \\in \\mathbb{R}^{1 \\times r}\\) are trainable, and is denoted as diagonal matrices \\(\\Lambda_d \\in \\mathbb{R}^{d \\times d}\\) and \\(\\Lambda_d \\in \\mathbb{R}^{r \\times r}\\) in the equations.\nUnlike in LoRA, the B and A matrices do not need to be low-rank as their values does not need to be stored, they can always be reproduced with a fixed random seed. Only the small \\(b\\) and \\(d\\) vectors need to be updated. During training vector \\(b\\) is set as 0 to keep \\(\\Delta W\\) as 0 while vector \\(d\\) is initialized using Kaiming initialization.\nMore precisely the number of trainable parameters with VeRA scales as \\(L_\\text{tuned} \\times (d_\\text{model} \u0026#43; r)\\) , whereas LoRA scales as \\(2 \\times L_\\text{tuned} \\times d_\\text{model} \\times r\\) .\n( \\(L_\\text{tuned}\\) denote the number of finetuned layers and \\(d_\\text{model}\\) represents the dimension of the layers.)\nThis means that LoRA memory size dramatically with the increase of rank, VeRA can increase the rank without incurring much memory footprint.\nThis can be shown in the following figure comparing LoRA (left) and VeRA (right).\nPerformance # The original VeRA paper [1] found that VeRA tended to perform relatively competitively on various models such as RoBERTa, GPT-2, or on different modalities such as vision while using substantially less trainable parameters.\nRoBERTa Base GLUE benchmarks # Method # Trainable Parameters SST-2 MRPC CoLA QNLI RTE STS-B Avg. Full Finetune 125M 94.8 90.2 63.6 92.8 78.7 91.2 85.2 LoRA 0.3M 95.1±0.2 89.7±0.7 63.4±1.2 93.3±0.3 86.6±0.7 91.5±0.2 86.6 VeRA 0.043M 94.6±0.1 89.5±0.5 65.6±0.8 91.8±0.2 78.7±0.7 90.7±0.2 85.2 GPT-2 Medium E2E benchmarks # Method # Trainable Parameters BLEU NIST METEOR ROUGE-L CIDEr FT 354.92M 68.2 8.62 46.2 71.0 2.47 LoRA 0.35M 68.9 8.69 46.4 71.3 2.51 VeRA 0.098M 70.1 8.81 46.6 71.5 2.50 Image classification benchmarks # Method # Trainable Parameters CIFAR100 Food101 Flowers102 RESISC45 ViT-B Head - 77.7 86.1 98.4 67.2 Full 85.8M 86.5 90.8 98.9 78.9 LoRA 294.9K 85.9 89.9 98.8 77.7 VeRA 24.6K 84.8 89.0 99.0 77.0 ViT-L Head - 79.4 76.5 98.9 67.8 Full 303.3M 86.8 78.7 98.8 79.0 LoRA 786.4K 87.0 79.5 99.1 78.3 VeRA 61.4K 87.5 79.2 99.2 78.6 Extensions # DVoRA (DoRA + VeRA) # DoRA (Weight-Decomposed Low-Rank Adaptation) [4] is a modification on LoRA where the original weight is decomposed to magnitude and direction components to be finetuned with LoRA being used to finetune the direction component. The paper observes that when the weight matrix is decomposed to two separate components, magnitude and direction, LoRA tends to exhibit a proportional relationship between changes of direction and magnitude whereas full finetuning tends to be more varied with a slight negative relationship. [4] suggests that this show\u0026rsquo;s LoRA\u0026rsquo;s inability to decouple the changes in the magnitude and the direction.\nThis relationship is shown in the following diagram.\nDoRA by training the magnitude and direction separately attempts to rectify this deficiency of LoRA. With this modification, [4] claims that learning capacity and training stability is improved.\nAs DoRA uses LoRA as-is in its directional component training, [4] also suggests a new method of PEFT by replacing LoRA with VeRA, named DVoRA. DVoRA merges the advantage of VeRA and DoRA and performs on par or even better than LoRA with fewer trainable parameters.\nAverage MT-Bench scores graded by GPT-4 # Model PEFT Method # Params (%) Score LLaMA-7B LoRA 2.31 5.1 DoRA 2.33 5.5 VeRA 0.02 4.3 DVoRA 0.04 5.0 LLaMA2-7B LoRA 2.31 5.7 DoRA 2.33 6.0 VeRA 0.02 5.5 DVoRA 0.04 6.0 DoRA paper [4] is an example of a paper that takes advantage the parameter efficiency of VeRA while incorporating its own improvements to create a more performant PEFT algorithm.\nFuture Avenue of Research # Behavior of VeRA compared to LoRA # In the paper LoRA Learns Less and Forgets Less [5] the authors claim that LoRA underpeforms full finetuning but, tends to retain the base model performance better on tasks outside of the finetune training data. The authors posits that this is due to the fact that full finetuning tends to find higher rank weight perturbations compared to LoRA. Though VeRA has even fewer tunable parameters than LoRA, the rank of VeRA can be increased more freely compared to LoRA. This contradictory behavior presents an interesting case-study as to how the behavior of VeRA would compare to LoRA in such a scenario presented in [5]. The original VeRA paper either used relatively older encoder-based models such as RoBERTa, relatively coarse evaluations such as GLUE or ROGUE, or relatively simple instruction tuning dataset (Alpaca Cleaned). This paper focuses much more on relevant and challenging LLM tasks such as more complex instruct tuning and continued pretraining in complex domains (math and code).\nFew of things that can be compared is:\nHow performance of VeRA fares compared to LoRA and full finetuning on target domain task performance. Does VeRA exhibit the same regularization characteristic as LoRA by forgetting less of the source domain? Does sample-efficiency suffers compared to LoRA and full finetuning? NAS (Neural Architecture Search) to VeRA # LoRA\u0026rsquo;s hyperparameters are rank size, which projection (key, query, value, output) to apply to, and which layer to apply it. But, LoRA\u0026rsquo;s hyperparameters are more strictly bounded by practical constraints as memory overhead grows quickly with rank size. On the other hand, VeRA is less bounded by rank size which makes it more free to explore higher ranks and various configurations.\nThis relatively wider search space makes VeRA attractive for NAS compared to LoRA. A systematic approach towards determining which layer to target, which rank to use, and which initial value to use for each application and modality could be an interesting research topic.\nThis research could also give us a glimpse into how each type of model responds to PEFT tuning. Whereas the original LoRA dealt in relatively limited domains such as encoder-type LLMs, instruction tuning, or image classification. Today, LoRA family of PEFT has been expanded to continued pretraining of LLMs, finetuning diffusion models, customization of LLMs, et cetera. The relative low-overhead and wider search space of VeRA could be useful as a tool for exploring the optimal configuration of LoRA-type PEFT and how each domains works differently.\nBetter initialization settings # The initialization scheme used in VeRA is relatively simple. The original VeRA paper does present some exploration and ablation studies of initialization schemes. The authors claim that using both \\(d\\) and \\(b\\) scaling vectors improve performance, using Kaiming uniform initialization for the performance is better, and initializing \\(d\\) vector with \\(d_init\\) set to \\(10^{-1}\\) or \\(10^{-7}\\) tends to outperform 1.0.\nBut, the types of initializations and number of parameters explored are limited and focus on relatively old model (RoBERTa) and coarse GLUE-based benchmarks such as RTE, MRPC, CoLA, and STS-B tasks. Additional experiments on more relevant LLM tasks such as instruction finetuning or continued pretraining could be more insightful as well as more diverse modalities(vision, sound, et cetera). For example, LoRAs have become a popular in diffusion models such as Stable Diffusion [9] as a way of generating custom images. It would be meaningful to explore the behavior and the best settings for VeRA in these type of applications and tasks.\nAlso, the fact that the rank can be scaled freely in VeRA with not much overhead was underexplored in the original paper. By varying and expanding the rank size to be much greater than what is feasible with LoRA it seems possible that VeRA could have higher rank compared to LoRA possibly leading to different behaviors. Varying the rank and the initializations of VeRA and comparing the SVD decomposition of VeRA, LoRA, and full finetuning seems like an underexplored topic. How different configurations of VeRA can change the behavior of the weight perturbations or how it relates to performance could be important for exploring how the weight features changes with finetuning. For example, [5] claims that on full finetuning on code and math the model does not learn low-rank perturbations unlike the original assumptions behind LoRA. Considering that VeRA is able to expand to much higher rank, SVD analysis of VeRA when trained on complex tasks like code and math could yield interesting results.\nUniversal basis matrices for VeRA # The Platonic Representation Hypothesis [6] claims that representations in AI models are converging across multiple domains. The hypothesis claims that representation learning algorithms attempts to find vector embeddings that statistically model reality through various measurements and projections. The vector embeddings are all derived from reality and becomes more aligned as models become trained on more data and for more tasks. In the paper, authors claim that model alighment increases with performance and even models trained with different modalities (language and vision) tends to converge as performance increases.\nIf large fundamental models share a common representation, it is possible that there could be an ideal way to represent the randomized matrix basis on which VeRA operates well in. Currently the random \\(A\\) and \\(B\\) matrices are generated relatively arbitrarily. This could suggest a sort of \u0026ldquo;universal\u0026rdquo; matrices where VeRA would perform well for all models and domains as models converge on a similar representation. Even if this hypothesis is true in a limited sense this suggests that there could be a family of matrices that can represent the basis( \\(A\\) and \\(B\\) ) for VeRA better.\nResearch could attempt to isolate such basis matrices through statistical analysis of various SoTA (state of the art) models and compare against random Gaussian generated matrices. Initially the study could focus on models across a single modality such as vision and expand search for multi-modal models. This could not only lead to a more practical method of generating better performing basis matrices but also validate the Platonic representation hypothesis.\nOne stumbling block of applying VeRA to this hypothesis is that the hypothesis largely discusses representation while VeRA is about modifying the model weights. However, LoReFT (low-rank linear subspace ReFT) [10] defines a methodology of finetuning a model by modifying its representations/activations, denoted as h. Using the formula:\n\\(\\text{LoREFT}(h) = h \u0026#43; R^{T} (W h \u0026#43; b - R h)\\) , where \\(R \\in \\mathbb{R}^{r \\times d}\\) and \\(W \\in \\mathbb{R}^{r \\times d}\\) are low rank matrices.\nConsidering that LoReFT also uses a low-rank matrix to represent trainable changes, VeRA\u0026rsquo;s methodology of using a randomly generated matrices with small vectors seems applicable. This expands the possibility that the hypothesis could be applicable for a LoREFT + VeRA hybrid as well.\nReferences # [1]: D. J. Kopiczko, T. Blankevoort, and Y. M. Asano, “VERA: Vector-based Random Matrix Adaptation,” arXiv.org, Oct. 17, 2023. https://arxiv.org/abs/2310.11454\n[2]: E. J. Hu et al., “LORA: Low-Rank adaptation of Large Language Models,” arXiv.org, Jun. 17, 2021. https://arxiv.org/abs/2106.09685\n[3]: “Efficient training on a single GPU.” https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one#anatomy-of-models-memory\n[4]: S.-Y. Liu et al., “DORA: Weight-Decomposed Low-Rank Adaptation,” arXiv.org, Feb. 14, 2024. https://arxiv.org/abs/2402.09353\n[5]: D. Biderman et al., “LORA learns less and forgets less,” arXiv.org, May 15, 2024. https://arxiv.org/abs/2405.09673\n[6]: M. Huh, B. Cheung, T. Wang, and P. Isola, “The platonic representation hypothesis,” arXiv.org, May 13, 2024. https://arxiv.org/abs/2405.07987\n[7]: C. Li, H. Farkhoor, R. Liu, and J. Yosinski, “Measuring the intrinsic dimension of objective landscapes,” arXiv.org, Apr. 24, 2018. https://arxiv.org/abs/1804.08838\n[8]: A. Aghajanyan, L. Zettlemoyer, and S. Gupta, “Intrinsic dimensionality explains the effectiveness of language model Fine-Tuning,” arXiv.org, Dec. 22, 2020. https://arxiv.org/abs/2012.13255\n[9]: R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-Resolution Image Synthesis with Latent Diffusion Models,” arXiv.org, Dec. 20, 2021. https://arxiv.org/abs/2112.10752\n[10]: Z. Wu et al., “REFT: Representation Finetuning for Language Models,” arXiv (Cornell University), Apr. 2024, doi: 10.48550/arxiv.2404.03592.\n"},{"id":8,"href":"/docs/spring24/08_/","title":"08","section":"Spring24","content":" MIXTURE OF LORA EXPERTS # LoRA is a methodology for effective fine-tuning large-scale pretrained models. LoRA is characterized by its ease of applying tuned results to existing models. This property encouragees research into synthesizing multiple trained LoRAs to achieve enhanced performance across various tasks such as linear arithmetic composition and reference tuning-based composition. However, combining these trained LoRAs poses significant two challenges:\nLinear arithmetic composition can diminish the capabilities of the original pre-trained models and the unique characteristics of the individually trained LoRAs, potentially leading to suboptimal results.\nReference tuning-based composition is limited in adaptability and incurs substantial computational costs, as it necessitates retraining a large model.\nSo, we can ask following question:\nHow can multiple trained LoRAs be composed dynamically and efficiently, preserving all their individual characteristics, without the need for retraining? To address this question, Mixture of LoRA Experts (MoLE) presents a new method for achieving the optimal combination of LoRAs for specific tasks. MoLE considers indivisual LoRA as an expert and determines the weights applied to LoRAs at each layer through a gate function.\nWorkflow of Mixture of LoRA Experts (MoLE) Background # What is LoRA? # Low-Rank Adaptation (LoRA) is a parameter-efficient and effective approach for fine-tuning large-scale pretrained models.\nModels such as OPT, LLaMA, and CLIP demonstrate remarkable performance when fine-tuned for various downstream tasks. However, full fine-tuning of these massive models requires substantial computational resources. LoRA enables parameter-efficient fine-tuning by keeping the pretrained model\u0026rsquo;s weights frozen and adding trainable low-rank decomposition matrices.\nLoRA Methodology In the above figure, only the matrices A and B are trained, with dimensions (d x r) and (r x d) respectively. By setting r \u0026laquo; d, the number of parameters to be trained can be reduced. These trained matrices are then simply added to the existing pretrained weights, allowing tuning without affecting the inference speed of the original model.\nLoRAs Composistion # The common solution to further improve the performance of LoRA across various tasks is to compose multiple trained LoRAs. Research on LoRA composition can be broadly categorized into the following two methodologies.\nLinear arithmetic composition. It is a method of directly adding multiple LoRAs. This approach is simple and has been effective in the NLP and Vision-Language domain, but it can result in the loss of pre-trained model\u0026rsquo;s generative capabilities or the individual characteristics of each LoRA. \\[\\hat{\\mathbf{W}} = \\mathbf{W} \u0026#43; \\sum_{i=1}^{N} w_i \\cdot \\Delta \\mathbf{W}_i\\] Reference tuning-based composition tackles the above limitations of linear arithmetic method by introducing gradient fusion and controllable sampling, but is requires retaining when incorporating different LoRAs or creating new masks, which results non-trivial computational costs. (Left) Linear arithmetic composition. (Right) Reference tuning-based composition Mixture-of-Experts # MoE is an effective method that allows scaling up the number of parameters while maintaining the computational cost of the model.\nIllustration of a Swith Transformer block. Experts FFN Layers: MoE layer is composed of N separate feed-forward networks as the experts. This concept involves dividing the FFN layer of traditional transformers into N experts. These experts can be thought of as being responsible for specific tokens.\nGating functions (Router): A function that determines the weights over the experts outputs. For the hidden representation h of input token, and the trainable embedding e of each a expert, the gate value a is obtained as follow:\n\\[\\alpha(E_i) = \\frac{\\exp(h \\cdot e_i)}{\\sum_{j=0}^{N} \\exp(h \\cdot e_j)}\\] The output is a weighted sum of the outputs from the top-k experts, determined by the gated values.\n\\[O = h \u0026#43; \\sum_{i=0}^{N} \\alpha(E_i) \\cdot E_i(h)\\] Mixture of LoRA experts # Observations # Direct linear arithmetic composition reduced the generative power of the model, while normalized linear arithmetic composition retained the generative power of the model but lost its LORA character. (Left) Result of linear arithmetic composision and, (Right) nomalized linear arithmetic composision. Experiment in the NLP domain. NLA denotes normalized linear arithmetic composision In the V\u0026amp;L domain, directly composing multiple trained LoRAs into the original embedding caused significant parameter variations and meaningless output, while normalization compromised their original characteristics. In the NLP domain, composing four or more LoRAs within the FLAN-T5 model resulted in disordered output, and weight normalization across five datasets decreased the performance, suggesting adverse effects on the intrinsic qualities of the trained LoRAs.\nEach layer of the trained LoRA represented a unique characteristic, which cumulatively defined the overall properties of the LoRA. (Right) Observed that different layers of LoRA encode distinct features, such as dog coat color and facial features. (Left) When evaluated on a subset of datasets, there were significant differences in performance across the different layers of LoRA.) So, The conjecture is that adjusting the characteristics by varying the layer-specific weights according to the desired domain objective will result in a more effective composition of trained LORAs.\nMethod # Illustration of proposed MOLE. MOLE employs a learnable gating function that utilizes the outputs of multiple LoRAs at each layer to determine composition weights. See related formulas Symbols input $x \\in \\mathbb{R} ^ {L \\times d}$ L: sequence length d: dim of $x$ Multi attention layer : $$\\mathcal{f}_{Attn} (\\centerdot)$$ Feed forward neural network layer: $$\\mathcal{f}_{FFN} (\\centerdot)$$ LN: layer normalization Trained LORAs $$\\Omega = \\left\\{ \\Delta \\Theta \\right\\}^N_{i=0}$$ learnable gating function $$\\mathcal{G} (\\centerdot)$$ The weight of the $i^{th}$ trained LorA $$\\mathcal{G}_i (\\centerdot)$$ Concatenation operation: $$\\oplus$$ Learnable parameter $e \\in \\mathbb{R} ^ {N^2 \\times L \\times d}$ Learnable temperature scalar $\\tau$ Freezing part $$x^\\prime_{\\theta} = x + \\mathcal{f}_{Attn} (LN(x)|\\theta)$$ $$\\mathbf{F}_\\theta (x) = x^\\prime_{\\theta} + \\mathcal{f}_{Attn} (LN(x^\\prime_{\\theta})|\\theta)$$ LoRA part $$x^\\prime_{\\Delta \\Theta_i} = x + \\mathcal{f}_{Attn} (LN(x)|\\Delta \\Theta_i)$$ The output of each LoRA $$\\mathbf{E} _{\\Delta \\Theta_i} (x) = x^\\prime_{\\Delta \\Theta_i} + \\mathcal{f}_{FFN} (LN(x^\\prime_{\\Delta \\Theta_i})|\\Delta \\Theta_i)$$ The output of all LoRA $$\\mathbf{E}_\\Omega (x) = Normalization(\\mathbf{E}_{\\Delta \\Theta_0} (x) \\oplus \\ldots \\oplus \\mathbf{E}_{\\Delta \\Theta_{N-1}} (x)) \\in \\mathbb{R} ^ {N \\times L \\times d}$$ Flatten and dot product operation $$\\epsilon = Flatten(\\mathbf{E}_\\Omega (x))^T \\centerdot e, \\epsilon \\in \\mathbb{R} ^ N$$ Gate value for each LoRA $$\\mathcal{G} (\\epsilon_i) = \\frac {exp(^{\\epsilon_i} /_ \\tau)} {\\displaystyle\\sum_{j=1}^N {exp(^{\\epsilon_j} /_ \\tau)}} $$ Final output of the gating function $${\\tilde{\\mathbf{E}}_\\Omega (x)} = \\displaystyle\\sum_{i=0}^N {\\mathcal{G} (\\epsilon_i) \\centerdot \\mathbf{E} _{\\Delta \\Theta_i} (x)} , {\\tilde{\\mathbf{E}}_\\Omega (x)} \\in \\mathbb{R} ^ {L \\times d} $$ Final output of Transformer block $$\\mathcal{O}(x) = {\\mathbf{F}_\\theta (x)} + {\\tilde{\\mathbf{E}}_\\Omega(x)} $$ Training # The training loss function used in MoLE is as follows:\nAlpha is a coefficient for weight balancing.\nGating Balacing Loss\nAs shown in Figure 5 (a), the average entropy of the distribution probabilities from the gating functions gradually decreases as training progresses. In Figure 5 (b), we can see a gating probability of 64% for LoRA β among the three LoRAs, indicating that the gating function tends to converge to a state where it assigns large weights to well-performing LoRAs in the early stages. This can result in a significantly larger impact from a few specific LoRAs compared to others, potentially leading to biased outcomes. To avoid this, the author created a gating balancing loss. The gating balancing loss helps prevent bias by ensuring that the loss value decreases as the model becomes less biased. See related Symbols M: The num of blocks where gating functions are placed N: num of LoRAs Domain-specific Loss In V\u0026amp;L, Using a loss in CLIP(Radford et al,20221b) In NLP, Using a loss in FLAN-T5(Chung et al,2022)\nResults # On V\u0026amp;L Domain # Setup\nBase generator: DeamBooth(Ruiz et al., 2023) (built on Stable Diffusion V2.1) LoRA: combination of three separately trained LoRAs Image resolution: 512x512 learning rate: 1e-5 DDPM sampler (Ho et al., 2020) with 50 steps in each case Train 400 iterations for each required composition with batch size 2 and α as 0.5 Metrics\nImage alignment: Evaluate the visual similarity of generated images with individual composed concepts in the CLIP image feature space. Text alignment: Evaluate the text-image similarity of generated images with given text prompts in the CLIP feature space. For each composition, calculated the average scores among 200 generated images per prompt using 5 text prompts. Compared Baselines\nNormalized linear arithmetic composition SVDiff (Han et al., 2023) Results\nIt demonstrates better performance compared to other models and shows outstanding results in other tasks as well.\nWhen viewing the generated images, it is evident that all specified subjects are accurately represented and maintained.\nOn NLP Domain # Setup\nBase Model: Flan-T5 (Chung et al., 2022) LoRA: Several LoRAs based on FLAN datasets learning rate: 1e-5 Train 800 iterations for each required composition with batch size 12 and α as 0.5. Compared Baselines\nLoRAhub PEMs Results\nIt can be observed that MoLE demonstrates better performance in most tasks.\nAnalyisis # 1. Gating balancing loss works! # Gating balancing loss function mitigates the reduction in entropy rates within gating functions, and enhance the performance.\nExperimental results on gating balance of MOLE. NLA denotes normalized linear arithmetic composition 2. MoLE is even better than SOTA multi-concept generation methods. # MoLE outperforms two multi-concept generation algorithms (Custom, Textual Inversion), both of which emphasize full-parameter training for enhanced results.\nText-alignment and image-alignment results for multiple LoRA experts composition in CLIP feature space. SOTA full-parameter training methods are highlighted by pink boxes 3. Scale to a larger number of LoRAs. # MOLE demonstrated optimal performance across varying numbers of LoRA, notably surpassing LoRAHub with larger LoRA counts of 48 and 128. However, all methods, including MOLE, showed performance declines with an extremely large number of LoRA\nNLP domain experimental results on the impact of exploring expand expert numbers on model performance. The result is the average EM on the Big-Bench Hard (BBH) dataset. 4. Coarse gating vs. fine gating # Among matrix-wise, layer-wise, block-wise, and network-wise MoLEs, intermediate granularities, b-MoLE and l-MoLE, achieved the highest performance.\nCoarse-to-fine gating comparison 5. Flexibility of MoLE. # MoLE not only achieves effective LoRA composition but also retains the characteristics of individual LoRA. It can generate images that closely resemble the original features of the LoRA experts\nQualitative result for retaining ability experiment. 6. Hierarchical control analysis # MOLE adaptively assigns weights to different LoRA experts across various layers, resulting in finer-grained weight combinations that yield superior results.\nVisualization of the weights (%) predicted by each gating function (horizontal axis) for LoRA experts (vertical axis) during inference. The top row corresponds to experiments in the NLP domain, while the bottom row pertains to experiments in the V\u0026L domain. Discussion and Limitations # Limitations # LoRA scale When the number of LoRAs increases to a very large value (e.g., 128), the performance of all LoRA composition methods, including MOLE, tends to decrease despite MOLE\u0026rsquo;s superior performance. This indicates that MOLE still faces challenges with large-scale LoRA composition and emphasizes the need for better approaches to handle it effectively.\nParameter The learnable parameter 𝑒 used in MoLE has dimensions of $N^2 \\times L \\times D$. As the number of LoRAs increases, the number of parameters grows quadratically, resulting in a substantial increase. Additionally, since e exists for each transformer block, the number of parameters added by 𝑒 is considerable. This can be seen as a drawback of MoLE. Discussion # How to address MoLE\u0026rsquo;s limitations at LoRA scale?\nCurrently, MoLE\u0026rsquo;s performance decreases when the number of LoRAs exceeds a certain threshold. By reducing the number of LoRAs to below this threshold with minimal loss, performance could be improved. Assuming there is a large number of LoRAs, there will likely be many LoRAs for similar tasks. Given this, we believe that clustering to derive representative LoRAs for similar tasks and using only the representative LoRAs instead of all similar task LoRAs could overcome MoLE\u0026rsquo;s limitations.\n"},{"id":9,"href":"/docs/spring24/09_/","title":"09","section":"Spring24","content":" MobileNetV4 - Universal Models for the Mobile Ecosystem # Posted by JoonSeok Kim and DongGyu Kim\nQin, Danfeng and Leichner, Chas and Delakis, Manolis and Fornoni, Marco and Luo, Shixin and Yang, Fan and Wang, Weijun and Banbury, Colby and Ye, Chengxi and Akin, Berkin and others arXiv preprint arXiv:2404.1051 Main Contributions # MobileNetV4 targets designing neural networks for mobile devices. Since the mobile platform can only offer limited compuation ability and DRAM utilization, software engineers are trying to design small and efficient neural networks. To use AI at the industry level, the inference latency must also be small. Main objectives of designing inference models for mobile devices are\nAcceptable test performance on widely-used datasets such as ImageNet-1k Low inference latency for utilization in mobile devices Minimization of the number of parameters for low memory utilization on mobile platforms Minimization in the number of MACs for high enegy efficiency This paper mainly focuses on lowering inference latency while maintining the test accuracy up to SOTA mobile neural net performance. Since it targets mobile platforms, it analyzes performance of various mobile hardwares, and designs a neural network to fit the harwares maximum performance. The designing process was done by the NAS technique, where the intantiation of UIB blocks were set as the search space. The main contributions of this work can be states as follows.\nUniversal Inverted Bottleneck (UIB) seach block - Unifies the Inverted Bottleneck (IB), ConvNext, Feed Forward Netowork (FFN), and Extra Depthwise variant Mobile MQA - Attention block tailored for mobile accelerators NAS technique to improve performance Achieves Pareto optimal acorss various devices such as CPUs, DSPs, GPUs, and TPUs Novel distillation technique to boost accuracy. Achieves 87% accuracy on ImageNet-1k and 39x smaller model size Preliminaries - Inverted Residual Blocks and Linear Bottlenecks # Fig. 0 (a) Original Residual Block Previously, the residual bottleneck block was propsoed, which consists of the 1x1 pointwise convolution in the first layer, a depthwise convolution in the second layer, and the final pointwise convolution layer, where its output is residually connected with the module\u0026rsquo;s input. The first layer acts as a projection layer to generate the narrow, and parameter-efficient convolution layer for depthwise convolution. This part acts as the bottleneck layer. The output of this layer is expanded again in the pointwise convolution. Here, the module forms a wide-narrow-widw approach considering the number of channels.\nFig. 0 (b) Inverted Residual Blocks However, in MobileNetV2, the authors use the inverted residual blocks with linear bottlenecks. In opposed to the original residual block where the first pointwise convolution acts as the projection layer and the final pointwise convolution acts as the expansion layer, the inverted residual block applied this in the opposite way to create the narrow-widw-narrow block. This assumes that the low-dimensional feature data is stored in the \u0026ldquo;narrow\u0026rdquo; layers, and these need to be expanded and extracted through the depthwise convolution. The required information in the narrow layers are passed onto the deeper layers through residual connections. Also, one has to take note that the final pointwise convolution layer in each inverted residual blocks do not have activations, in order to compensate the performnace degradation due to squeezing the layers where the skip connections are linked.\nPreliminaries - Roofline Model and Hardware Efficiency # Algorithm running on hardware is composed of two parts - memory access and computation. The computation time is determined by the computation requirement and hardware performance.\n\\[ \\text{runtime\\_computation} = \\frac{\\text{Number of Operations}}{\\text{FLOPS}} \\] Algorithm runtime can be limited by the memory access bottleneck or communication overhead\n\\[ \\text{runtime\\_communication} = \\frac{\\text{Number of IO Bytes}}{\\text{Bandwidth}} \\] Hardware performance is determined by the upper bound of the computation time or memory access latency\n\\[ \\text{performance} = \\max(\\text{runtime\\_computation}, \\text{runtime\\_communication}) \\] Below Fig. 1(a) and Fig. 1(b) illustrates the roofline model and its characteristics\nFig. 1 (a) Roofline Model Fig. 1 (b) Roofline Model with Ceiling Hardware-Independent Pareto Efficiency # Fig. 2. Ridge Points and Latency/Accuracy Tradeoffs Fig. 3. Op Cost vs. Ridge Point This research focuses on efficiency on various hardware targets such as DSPs, CPUs, and GPUs. To find whether the hardware is limited by its memory bottlenecked or compute bottlenecked, the Roofline Model of that hardware must be investigated. It is defined by the harware's peak computational throughput and its peak memory bandwidth. The optima of that hardware can be found in its ridge point, which is defined by the hardware's ratio of Peak MACs to Peak memory bandwidth. The algorithm's accuracy and latency are swept by the ridge point on various hardware on Fig. 2, and Fig. 3. The roodline model of MobileNetV4 achieves highest Pareto-optimal performance compared to other MobileNet models. MobileNetV4 is designed to achieve Pareto optimal and hence balances MAC operations and memory bandwidth. The initial layers are designed with high MAC intensity, so as to improve model capacity and downstream accuracy. The end layers use identically-sized FC layers to maximize accuracy. These two initial and end layers are balances so that MobileNetV4 should not see slowdowns at any hardware.\nUniversal Inverted Bottlenecks (UIB) # Fig. 4. Universal Inverted Bottleneck (UIB) blocks The main advantage of UIB is its adaptability and flexibility, that mitigates seach complexity. Optional Depthwise (DW) convolution blocks are inserted before the expansion layer, and between the expansion and projection layer. In the NAS procedure, common components such as the pointwise expansion and projection are shared and DWs are added as search options. UIB has four possible instantiations as follows.\nInverted Bottleneck (IB) : Spatial mixing on the expanded features activations, and provides higher model capacity ConvNext : Cheaper spatial mixing before the expansion with larger kernel size ExtraDW : Inexpensive increase of the network depth and the receptive field. Combined benefits of ConvNext and IB FFN : Stack of two 1x1 pointwise convolutions. Accelerator-friendly operation Mobile MQA # Table 1. Efficiency Gains by MQA This paper considers the Operational Intensity (OI), which is the ratio of arithmetic operations to memory access, to enhance efficiency of vision models on mobile accelerators. Here, Multi-Query Attention (MQA) is proposed instead od Multi-Head Self Attention (MHSA), which is simplified by utilization of shared keys and values across all heads. This sharing of keys and values reduces memory access hence improving OI, especially when the batch size is small. Large language models does not have significant accuracy drop in this MQA case. Table 1 shows that by adding MHSA and MQA, the performace accuracy has increased whereas the inference latency for MQA is approximately x39 lower than that of MHSA. Hence, MQA can accelerate better in the mobile environment, with negligible performance degradation.\nThe Spatial Reduction Attention (SRA) is applied, hence incorporating asymmetric spatial down-sampling, to downscale keys and values, and not queries. In hybrid models, there is a certain correlation between spatially adjacent tokens, hence necessitating spatial mixing convolution filters.\nRefined NAS for Enhanced Architectures # As shown above, the insitantiation of UIB blocks are in the neural architecture search process. TuNAS was adopted for the paper\u0026rsquo;s search strategy. The paper uses a two-stage search operation, the coarse-grained search and fine-grained serach to address the variance in parameter counts between UIB\u0026rsquo;s depthwise layers and other search options. The course-grained search process involves determining optimal filter sizes with fixed parameters. The fine-grained stage searches for the UIB\u0026rsquo;s layer configuration.\nResults - Comparisons with Other Works # Table 5. Classification results on ImageNet-1k Table 6. Object Detection results on the COCO validation set Results on the classification performance on ImageNet-1k dataset show that the MobileNetV4 achieves the highest performance and smallest latency compareed to other models on various mobile platforms such as CPUs and DSPs of mobile phones. While other models have closely competitive latency with the MobileNetV4 model, their latency is much higher.\nThe effectiveness of MobileNetV4 as backbone networks are tested on the COCO object detection experiment. The number of MACs were set to be similiar, and the Retina framework was used as the object detector. As the same as classification, MobileNetV4 achieves highest performance compared to other mobile-target modles, with the lowest CPU latency. Hence, the ability of MobileNetV4 for mobile devices can be shown.\nConclusion # This paper proposes the MobileNet V4 series, a universal high-efficiency model that operates efficiently across a wide range of mobile environments. By introducing a new Universal Inverted Bottleneck and Mobile MQA layer and applying an enhanced NAS recipe, MobileNet V4 achieves near Pareto-optimal performance on various hardware, including mobile CPUs, GPUs, DSPs, and dedicated accelerators. Additionally, using the latest distillation techniques, it demonstrates cutting-edge performance in mobile computer vision by achieving 87% ImageNet-1K accuracy with a latency of 3.8ms on the Pixel 8 EdgeTPU. The paper also presents a theoretical framework and analysis for understanding the model\u0026rsquo;s universality across heterogeneous devices, providing guidance for future design. Perspectives, Discussions and Future Research Directions # MobileNetV4 was designed with a focus on optimizing hardware performance, particularly for mobile devices such as mobile CPUs, GPUs, and DSPs. It leveraged Neural Architecture Search (NAS) to design its Unit Inverted Bottleneck (UIB) blocks and employed distillation techniques to enhance performance. Additionally, MobileNetV4 used MQA to incorporate transformer-like operations, further boosting its performance. The authors aimed to integrate as many state-of-the-art techniques as possible to optimize the neural network\u0026rsquo;s performance in mobile environments. Given its recent development, MobileNetV4 is likely the state-of-the-art convolution-based neural network for mobile platforms.\nThe novelty of this work appears to be the neural architecture search involving the UIB, which includes the expansion and projection blocks with inverted residuals previously introduced in MobileNetV2. Since the performance is validated across various mobile platforms, MobileNetV4 presents an attractive solution for industry applications in mobile systems. However, the paper seems to combine existing methods, techniques, and modules into one comprehensive experiment to produce the optimal MobileNet model. The optimization was conducted logically and practically, but it would have been helpful if the authors had provided an analysis of the finalized MobileNetV4, explaining why NAS designed the UIB in that specific way and how features are extracted at each layer.\nIt is worth noting that many academic labs are currently designing mobile-level transformers like MobileViT or FastViT. This paper shows that these transformer-based models have similar performance to MobileNetV4 but with significantly higher inference latency. Although the parameter counts and the number of MAC operations are comparable, the latency difference raises questions about the practicality of developing mobile-level transformers for vision tasks. Despite incorporating MQA blocks, it appears beneficial to use bottleneck modules as the primary feature extractors.\nBeyond MobileNetV4, many CNNs are integrating multi-head attention layers, while transformers are incorporating convolutions. Convolutions capture local feature relationships, whereas attention modules provide global features in computer vision. Combining these two characteristics enhances neural network performance across the board. In the future, it would be practical to design NPUs and domain-specific accelerators that enable fast and efficient computation for both convolutions and attention mechanisms simultaneously.\nSimilar Works - MobileFormer # Fig. 5. MobileFormer Network A similiar work that utilizes the MobileNet and trasnformer modules was introduced in CVPR 2022, the Mobile-Former. This structure capitalizes on MobileNet\u0026rsquo;s strength in local processing and the transformer\u0026rsquo;s capability for global interaction. The bridge facilitates bidirectional fusion of local and global features. Unlike recent vision transformer approaches, the transformer in Mobile-Former uses very few tokens (typically 6 or fewer), which are randomly initialized to learn global priors, thereby minimizing computational cost. Additionally, the proposed lightweight cross-attention mechanism used to model the bridge enhances both computational efficiency and representation power.\nTable. 7. Performance comparison of MobileFormer with other works Mobile-Former demonstrates superior performance compared to MobileNetV3 in the low FLOP regime, ranging from 25M to 500M FLOPs on ImageNet classification. For example, Mobile-Former achieves 77.9% top-1 accuracy at 294M FLOPs, surpassing MobileNetV3 by 1.3% while reducing computations by 17%. In object detection tasks, Mobile-Former outperforms MobileNetV3 by 8.6 AP within the RetinaNet framework. Furthermore, when applied to the DETR model, replacing its backbone, encoder, and decoder with Mobile-Former results in a detector that not only surpasses DETR by 1.1 AP but also reduces computational cost by 52% and parameter count by 36%.\nReferences # [1] MobileNetV4 Implementation: [Link.](https://github.com/jiaowoguanren0615/MobileNetV4/tree/main)\n[2] Qin, Danfeng, et al. \u0026ldquo;MobileNetV4-Universal Models for the Mobile Ecosystem.\u0026rdquo; arXiv preprint arXiv:2404.10518 (2024).\n[3] Sandler, Mark, et al. \u0026ldquo;Mobilenetv2: Inverted residuals and linear bottlenecks.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n[4] Chen, Yinpeng, et al. \u0026ldquo;Mobile-former: Bridging mobilenet and transformer.\u0026rdquo; Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n"},{"id":10,"href":"/docs/spring24/10_/","title":"10","section":"Spring24","content":" Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length # Authors: Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou\nReviewer: Hyunho Kook\n1. Introduction # Recently, Large Language Models (LLMs) have been gaining popularity. The impressive performance and versatility demonstrated by large models above a certain level have started to be utilized in various fields. However, as the size of the models grows, the size of the data that the models are expected to process is also increasing. Examples of this include processing currently open issues by inputting a GitHub repository or translating a large volume of books without losing context. In addition, the ability to maintain context and carry on a conversation for an extended period within a single chat is also sometimes required. The transformer, which is the foundation model of modern LLMs, exhibits vulnerabilities in this regard. Firstly, since it uses KV cache, memory usage increases rapidly as the sequence length grows, and it has a computational complexity proportional to the square of the sequence length.\nTo address this problem, the authors propose a method that inherits and advances MEGA (exponential moving average with gated attention), the predecessor of this paper. The overall contributions are as follows:\nCEMA (Extending Multi-dimensional Damped EMA to Complex Domain), an extension of Exponential Moving Average (EMA) to the complex domain, is proposed. Timestep Normalization, an extension of Group Norm to the timestep domain, is proposed as an alternative to Layer Norm. Normalized Attention, which performs normalization during attention computation, is proposed. 2-hop Residual Connection, which composes residual connections in 2-hop units, is proposed. By employing these methods, the authors have created a transformer architecture that is linear with respect to context length. They have also addressed the issues encountered in the previous research, MEGA, which were (i) low performance and (ii) the need for different architecture structures for each data type or task.\n2. MEGA (exponential Moving avErage with Gated Attention) # Before diving into the main contributions of our work, it\u0026rsquo;s essential to grasp the concepts behind MEGA (Moving Average with Gating Attention), a technique that serves as a stepping stone towards more efficient transformer models. MEGA leverages the moving average with damping to enhance the attention mechanism. Let\u0026rsquo;s break it down step by step.\n2.a Moving Average with Damping # At the core of MEGA lies the moving average with damping, which can be expressed by the following equations:\n\\(u_t^{(j)} \u0026amp;= \\beta_j x_{t,j} \\\\ h_t^{(j)} \u0026amp;= \\alpha_j \\odot u_t^{(j)} \u0026#43; (1 - \\alpha_j \\odot \\delta_j) \\odot h_{t-1}^{(j)} \\\\ y_{t,j} \u0026amp;= \\eta_j^T h_t^{(j)}\\) This exponential moving average (EMA) serves as the foundation for the subsequent attention mechanism.\n2.b Generating Query and Key # The EMA is then fed into the attention mechanism to generate the Query and Key matrices after applying the SiLU activation function. First, we calculate the shared representation \\(Z\\) using the EMA of the input \\(X\\) :\n\\(X\u0026#39; \u0026amp;= EMA(X) \\in \\mathbb{R}^{n \\times d} \\\\ Z \u0026amp;= \\phi_{silu}(X\u0026#39;W_z \u0026#43; b_z) \\in \\mathbb{R}^{n \\times z}\\) Using the shared representation \\(Z\\) , we can now compute the queries and keys. It\u0026rsquo;s important to note that the original input \\(X\\) is used to calculate the values:\n\\(Q \u0026amp;= \\kappa_q \\odot Z \u0026#43; \\mu_q \\in \\mathbb{R}^{n \\times z}\\\\ K \u0026amp;= \\kappa_k \\odot Z \u0026#43; \\mu_k \\in \\mathbb{R}^{n \\times z}\\\\ V \u0026amp;= \\phi_{silu}(XW_v \u0026#43; b_v) \\in \\mathbb{R}^{n \\times v}\\) 2.c Attention Output Calculation # After obtaining the Query, Key, and Value matrices, we can calculate the output of the attention mechanism using the standard formula:\n\\(O = f\\left(\\frac{QK^T}{\\tau(X)}\\right)V \\in \\mathbb{R}^{n \\times v}\\) Here, \\(f\\) is typically the softmax function, and \\(\\tau\\) is a temperature function based on the input \\(X\\) .\n2.d Gating Mechanisms # MEGA employs gating mechanisms, similar to the Gated Linear Unit (GLU), to generate the final output \\(Y\\) and the internal activation \\(\\hat{H}\\) . This is achieved through the following equations:\n\\(\\gamma \u0026amp;= \\phi_{silu}(X\u0026#39;W_\\gamma \u0026#43; b_\\gamma) \\in \\mathbb{R}^{n \\times v} \\\\ \\varphi \u0026amp;= \\phi_{sigmoid}(X\u0026#39;W_\\varphi \u0026#43; b_\\varphi) \\in \\mathbb{R}^{n \\times d} \\\\ \\hat{H} \u0026amp;= \\phi_{silu}(X\u0026#39;W_h \u0026#43; (\\gamma \\odot O)U_h \u0026#43; b_h) \\in \\mathbb{R}^{n \\times d} \\\\ Y \u0026amp;= \\varphi \\odot \\hat{H} \u0026#43; (1 - \\varphi) \\odot X \\in \\mathbb{R}^{n \\times d}\\) 2.e Chunk-wise Attention for Linear Time Complexity # To reduce the time complexity to a linear function of O(nc), MEGA employs chunk-wise attention, as illustrated in the figure above. By maintaining an internal accumulation of information and processing the input in chunks, MEGA enables transformers to achieve comparable performance on long context datasets.\n2.f Limitations and Future Directions # Despite its benefits, MEGA still faces two primary challenges:\nPerformance falls short compared to typical transformers with full attention. Different architectures are required for different tasks. Moreover, the effectiveness of MEGA in large-scale networks remains to be demonstrated conclusively.\nIn the following sections, we will explore how our proposed approach addresses these limitations and pushes the boundaries of efficient transformer models.\n3. Methods # Now, this paper, Megalodon, proposes four different methods that makes a network have even better performance than the baseline, while it can be applied for general cases.\n3.a CEMA (Extending Multi-dimensional Damped EMA to Complex Domain) # To improve EMA (Exponential Moving Average) capability when working over the complex number system, the authors propose CEMA (Complex Exponential Moving Average), which extends the idea of EMA to the complex plane. The key equations for CEMA are:\n\\(h_t^{(j)} \u0026amp;= \\alpha_j(\\cos \\theta_j \u0026#43; i \\sin \\theta_j) \\odot u_t^{(j)} \u0026#43; (1 - \\alpha_j \\odot \\delta_j)(\\cos \\theta_j \u0026#43; i \\sin \\theta_j) \\odot h_{t-1}^{(j)} \\\\ y_{t,j} \u0026amp;= \\mathcal{Re}(\\eta_j^T h_t^{(j)})\\) Here, \\(\\alpha\\) and \\(\\delta\\) are real-valued parameters, just like in the original EMA. However, \\(\\eta\\) is a complex-valued parameter in CEMA. The \\(\\theta_j\\) values are learnable parameters that are used to uniformly distribute the \\(h\\) arguments over the period of \\(2\\pi\\) . This is achieved by parameterizing \\(\\theta_j\\) as:\n\\(\\theta_{j,k} = \\frac{2\\pi k}{h} \\omega_j, \\quad \\forall k \\in {1, 2, \\dots, h}\\) where \\(\\omega\\) is a learnable parameter that determines the base angles.\nThe introduction of complex numbers in CEMA allows it to capture positional features of internal embeddings without changing the magnitude of \\(h\\) . Instead, it periodically changes the angle in the complex domain. This makes CEMA a powerful tool for modeling long sequences and improving the capability of EMA in complex-valued systems.\n3.b Timestep Normalization # Transformer-based models typically use Layer Normalization (LayerNorm) instead of Batch Normalization (BatchNorm) due to the significant variation in input batch sizes across different timesteps. While this approach has led to great success in large language models, the authors argue that LayerNorm cannot effectively capture and reduce the internal covariance along the temporal domain. To address this issue, they propose using a GroupNorm-like method. However, applying GroupNorm directly is challenging because it cannot access future information during auto-regressive inference.\nTo overcome this obstacle, the authors introduce Timestep Normalization, a superset of GroupNorm. This technique divides the total timesteps into \\(k\\) groups and applies group normalization to each group independently. By doing so, Timestep Normalization can reduce the covariance shift along the temporal domain while still being compatible with the causal nature of auto-regressive models.\n3.c Normalized Attention # During the calculation of attentions, the results can have a hugh instability along the temporal domain because the value of CEMA keeps changing. Therfore, the authors propose to use normalizaion on \\(Z\\) before calculating the queries and keys. In this case, we do not have to scale the \\(QK^{T}\\) since the values are already normalized while improving the stability of the attention mechanism.\n\\(X\u0026#39; \u0026amp;= \\mathcal{CEMA}(X) \u0026amp;\u0026amp; \\in \\mathbb{R}^{n \\times d} \\\\ Z \u0026amp;= X\u0026#39;W_z \u0026#43; b_z, \\quad Z\u0026#39; = \\frac{Z}{|Z|} \u0026amp;\u0026amp; \\in \\mathbb{R}^{n \\times z} \\\\ Q \u0026amp;= \\kappa_q \\odot Z\u0026#39; \u0026#43; \\mu_q \u0026amp;\u0026amp; \\in \\mathbb{R}^{n \\times z} \\\\ K \u0026amp;= \\kappa_k \\odot Z\u0026#39; \u0026#43; \\mu_k \u0026amp;\u0026amp; \\in \\mathbb{R}^{n \\times z} \\\\ O \u0026amp;= f_{\\text{softmax}}\\left(QK^T\\right)V \u0026amp;\u0026amp; \\in \\mathbb{R}^{n \\times v}\\) 3.d 2-hop Residual Connection # Finnally, the last contribution of this paper is the use of the 2-hop residual connection. The authors point out that pre-normalizaion can achieve more stable training in deep learning, this can further lead instability when scaling up the model size. Therefore, to reduce the instability, this paper uses 2-hop residual connections which propagate the input to the end of the layer without modifying it.\n3.e Summary of Methods # The main contribution of this paper lies in the application of CEMA (Complex Exponential Moving Average), which effectively captures the positional information of internal embeddings ( \\(H\\) ) by incorporating a circular system in the complex domain. By using periodic functions like sine and cosine, CEMA can encode the sequential nature of the input data, allowing the model to better understand and utilize the temporal dependencies.\nHowever, the introduction of complex numbers in CEMA doubles the dimensionality of the variables, which can potentially lead to significant instability during the training process. To address this issue, the authors propose three additional methods: Timestep Normalization, Normalized Attention, and 2-hop Residual Connection. These techniques work in conjunction with CEMA to stabilize the training process and ensure the model\u0026rsquo;s robustness.\nTimestep Normalization is employed to reduce the covariance shift along the temporal domain while maintaining compatibility with the causal nature of auto-regressive models. By normalizing the activations within each timestep group, this method helps to mitigate the impact of the increased dimensionality introduced by CEMA.\nNormalized Attention is used to stabilize the attention mechanism by properly scaling the attention weights. This prevents extreme values that could otherwise destabilize the training process, ensuring that the model can learn effectively from the input data.\n2-hop Residual Connection provides a more direct path for gradient flow, allowing the model to propagate information more efficiently during the backward pass. This helps to alleviate the vanishing gradient problem and facilitates faster convergence of the model.\nThese three methods work in harmony with CEMA to counterbalance the potential instability caused by the increased dimensionality. By carefully integrating these techniques, the authors aim to create a robust and effective model that can leverage the benefits of CEMA while maintaining stable training dynamics.\nIn essence, while CEMA is the primary contribution of this paper, the additional methods proposed by the authors play a crucial role in ensuring the model\u0026rsquo;s stability and performance. The combination of CEMA with Timestep Normalization, Normalized Attention, and 2-hop Residual Connection demonstrates a well-rounded approach to tackling the challenges associated with modeling sequential data in the complex domain.\n4. Experiments # In most cases, the authors try to compare the proposed idea with the LLAMA2-7B model. They use MEGALODON-7B model and use the same amount of training tokens to pre-train it. They try to show that MEGALODON can not only achieve the higher performance on long context datasets, but also show superior skills on short context problems than a naive transformer architecture.\n4.a Short-Context Evaluation # The authors first show the loss during training and the performance on short context datasets during inference. The proposed method can achieve training loss between LLAMA-7b and LLAMA-13b with 7b parameters. Furthermore, if you look at the table 1, then during inference the MEGALODON-7B truly show the performance between LLAMA-7b and LLAMA-13b. This proves that MEGALODON can ahieve better performance than the original model unlike the previous work, MEGA.\n4.b Long-Context Evaluation # To verify the performance on long context datasets, the authors provide two results. The figure 5 describes that if the context length increases, then the MEGALODON can truly output lower PPL answers. Then, the table 2 shows the performance on a long context benchmark, Scrolls. Although the LLAMA2 can show slightly better performance when it is finetuned for long context datasets with 500B tokens, the MEGALODON can show comparable accuracies without applying any fine-tuning techniques.\n4.c Instuction Finetuning # The authors also prove that MEGALODON can have a generalization power by testing the pre-trained model on MT-Bench. They do not utilized further techniques such as RLHF during fine-tuning for a specific instruction. They show a comparable or sometimes competitive performance.\n4.d Medium-scale Benchmarks # Finnally, the authors prove that the MEGALODON can show competitive performance compared to the previous long-context-agnostic models on general benchmarks. Inside the table 4 and 5, the MEGALODON achieves highest performance on ImageNet classifcation task and medium-scale benchmakr (PG-19).\n4.e Summary of Experiments # In the experiment part, the authors try to prove that the MEGALODON can have the features that have been proven for the previous foundation models, while showing superior performance on long context benchmarks. This implies that the MEGALODON is not a sophisticately tuned model for long contexts.\n5. Comparison with related works # There have been several similar related studies:\nEfficient Attention: FlashAttention optimized the GPU computation of the attention, showing advantages in speed without changing the existing mechanism. Additionally, there have been attempts to increase the context length by converting the attention mechanism to a linear one or compressing the KV cache.\nStructured State Space Model: A notable study in this area is Mamba, which added mechanisms like Selective Scan to a State Space model with linear time complexity, enabling it to process long context.\nHowever, these studies have limitations. Even if we accept that Flash Attention did not change the attention mechanism itself, Linear Attention, KV cache compression, and State Space Models have shown significantly lower performance on general benchmarks, although they may perform better than standard Transformers in long contexts.\n6. Discussion # In my opinion, there are a few potential limitations that are not extensively discussed in the paper:\nReliance on CEMA for Out-of-Chunk Context: The self-attention mechanism in MEGALODON is applied within each chunk. For data that falls completely outside the chunk boundaries, the model relies solely on CEMA for processing. This limitation could potentially hinder the model\u0026rsquo;s ability to handle long-range dependencies that span across multiple chunks.\nComplexity of the Architecture: Compared to the traditional Transformer layer, the MEGALODON architecture is considerably more complex. It requires the computation of EMA, including the complex domain, for each token. Additionally, several normalization and attention components have been introduced, such as Timestep Normalization, which further increases the complexity of the model compared to the previous works.\nLimited Exploration of Downstream Tasks: While the paper demonstrates the effectiveness of MEGALODON on long-context question answering tasks from the Scrolls dataset, the range of downstream tasks explored is relatively narrow. Evaluating the model\u0026rsquo;s performance on a broader set of tasks, such as summarization, dialogue generation, and composition, would provide a more comprehensive assessment of its capabilities and potential limitations.\nDespite these limitations, MEGALODON presents a promising direction for efficient long-context modeling. In my opinion, this kind of efficent and linear processing of memory can be a breakthrough for long-context LLMs.\n"},{"id":11,"href":"/docs/spring24/11_/","title":"11","section":"Spring24","content":" Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention # Posted by Younghyun Cho, Sangjun Lee\nLarge Language Models with Inifinite Input Sequence # Currently, Large Language Models (LLM) are based on Transformer architecture [1], which utilize interactions over the segments of an input sequence. However, this architecture has a limitation, requiring huge computations and memory in proportion to the length of input sequences. Thereby, current LLM are struggled to infinite input sequence tasks like summarizing books. To overcome this problem, google researchers suggests combining transformer architecture with a compressive memory, which stores previous informations in a constant size. They dubbed this method as Infini-attention.\nDetailed Method # Background: Scaled Dot-product Attention # The multi-head scaled dot-product attention (a.k.a. self-attention or MHA) is the main component in transformer architectures’ block.\nTo calculate the attention state \\( A_{dot} ∈ \\mathbb R^{N×d_{value}} \\) of a single head in the MHA module with an input seqeunce \\( X ∈ \\mathbb R^{N×d_{model}} \\) , three components, key, query and value are computed as\n\\(K = XW_K, V = XW_V \\ \\text{and} \\ Q = XW_Q, \\) where \\(W_K ∈ \\mathbb R^{d_{model} ×d_{key }}\\) , \\(W_V ∈ \\mathbb R^{d_{model} ×d_{value}}\\) and \\(W_Q ∈ \\mathbb R^{d_{model} ×d_{key}}\\) are trainable projection matrices. Then, we can get the attention state as\n\\(A_{dot} = \\text{softmax} \\Bigl( \\dfrac {QK^\\top} {\\sqrt {d_{model}}} \\Bigr) V. \\) We could calculate the MHA By parallely computing \\(H\\) number of attention states over an input sequence and then concatenating them. Infini-attention # Figure 1: Infini-attention has an additional compressive memory with linear attention for processing infinitely long contexts. \\(\\{KV\\}_{s−1}\\) and \\(\\{KV\\}_s\\) are attention key and values for current and previous input segments, respectively and \\(Q_s\\) the attention queries. PE denotes position embeddings. As shown Figure 1, Infini-attention computes both local and global context states and combine them for its output. Similar to multi-head attention (MHA), it maintains \\(H\\) number of parallel compressive memory per attention layer ( \\(H\\) is the number of attention heads) in addition to the dot-product attention.\nCompressive Memory # The researchers suggest three implementation to properly maintain the compressive memory inspired from previous neural network memory [11, 12, 13].\nMemory Retrieval # To fetch information from the memory, Infini-attetnion simply reuse the query ( \\(Q\\) ) in current state and combine with the memory. Specifically, the attention state from the memory \\(M_{s−1} ∈ \\mathbb R^{d_{key} ×d_{value }}\\) , \\(A_{mem} ∈ \\mathbb R^{N×d_{value}}\\) , is computed as follows with query \\(Q ∈ \\mathbb R^{N×d_{key}}\\) :\n\\(A_{mem} = \\dfrac {σ(Q)M_{s−1}}{σ(Q)z_{s−1}}. \\) \\(\\sigma\\) is a nonlinear activation function, and \\(z_{s−1} ∈ \\mathbb R^{d_{key}}\\) is a normalization term. The researchers use element-wise ELU+1 and sum over all keys for each described before.\nMemory Update # After the retrieval of the memory, we should update the memory and normalization part with the current key and value as follows:\n\\(M_s ← M_{s−1} \u0026#43; σ(K)^\\top V \\ \\text{and} \\ z_s ← z_{s−1} \u0026#43; \\sum^N_{t=1}σ(K_t). \\) After the update, the next input segment \\(S\u0026#43;1\\) uses the updated memory \\(M_s\\) and normalization term \\(z_s\\) recursively. Also, \\(σ(K)^\\top V\\) is refered to associative binding operator [3].\nAlso, the authors combines the delta rule [2] into Infini-attention. The delta rule takes the difference between the value of new segment ( \\(V\\) ) and the stored value in memory as the associative binding terms instead of simply using \\(V\\) (which is similar as the advantage function in reinforcement learning).\n\\(M_s ← M_{s−1} \u0026#43; σ(K)^\\top (V − \\dfrac {σ(K)M_{s−1}} {σ(K)z_{s−1}}). \\) The authors call this method as \\(Linear\u0026#43;Delta\\) and the former method as \\(Linear\\) .\nLong-term Context Injection # It is important to have a balance in the local attention \\(A_{dot}\\) and the global context \\(A_{mem}\\) . The researchers add a scalar \\(\\beta\\) which is the gating component of the weighted sum over the above attention states:\n\\(A = sigmoid(β) ⊙ A_{mem} \u0026#43; (1 − sigmoid(β)) ⊙ A_{dot}. \\) Finally, to get the MHA output of an attention layer \\(O ∈ \\mathbb R^{N×d_{model }}\\) , we concatenate the \\(H\\) parallel attention state and then project them to the output dimension:\n\\(O = [A^1; . . . A^H ]W_O \\) where \\(W_O ∈ \\mathbb R^{H×d_{value} ×d_{model}}\\) is the projection weights.\nComparsion with Other Transformers with Context Memory # Table 1: Transformer models with segment-level memory are compared. For each model, the memory size and effective context length are defined in terms of their model parameters ( \\(N\\) : input segment length, \\(S\\) : the number of segments, \\(l\\) : the number of layers, \\(H\\) : the number of attention heads, \\(c\\) : Compressive Transformer memory size, \\(r\\) : compression ratio, \\(p\\) : the number of soft-prompt summary vectors and \\(m\\) : summary vector accumulation steps). Table 1 shows the analysis of transformer models combining with segment-level memory.\nTransformer-XL [4] uses KV components from the privious segment with current components over each layer. Thus the context window of Transformer-XL is enlarged from \\(N\\) to \\(N \\times L\\) , and it requires \\((d_{key} \u0026#43; d_{value}) × H × N × l\\) memory foot prints. Figure from Transformer-XL [4]. Illustration of the vanilla model with a segment length 4. Compressive Transformer [5] append additional cache to Transformer-XL that saves the past activations. It broaden the Transformer-XL’s context window by \\(c × r × l\\) . It keeps a fine-grained memory of past activations, which are then compressed into coarser compressed memories. The below model has three layers, a sequence length \\(n_s = 3\\) , memory size \\(n_m = 6\\) , compressed memory size \\(n_{cm} = 6\\) . The highlighted memories are compacted, with a compression function \\(f_c\\) per layer, to a single compressed memory — instead of being discarded at the next sequence. In this example, the rate of compression \\(c = 3\\) . Figure from Compressive Transformer [5]. Memorizing Transformers [6] trys to gather the every KV components as the global context for the input segment. To reduce the overhead of storing every KV compoents, Memorizing Transformers adapts the context-weaving only on the last layer. The context window could explore entire input sequence \\(N \\times S\\) using KNN retriever. Figure from Memorizing Transformers [6]. Memorizing Transformers extend Transformers with access to (key, value) pairs of previously seen subsequences. RMT [7] and AutoCompressors [8, 9] utilized extra vectors that interact with current segment and then is delivered to next token recursively (which is similar in hidden vector in Recurrent Neural Networks (RNN)). However, the google researchers argue that the size of the additional memory vectors is the main factor of the efficiency of the method, which means that the performance and the memory footprint is aligned each other. Figure from Recurrent Memory Transformer [7]. Memory is added as tokens to the input sequence and memory output is passed to the next segment. During training gradients flow from the current segment through memory to the previous segment. Figure from AutoCompressors [8]. AutoCompressors process long documents by recursively generating summary vectors which are passed as soft prompts to all subsequent segments. Compare to the above context-based transformer models, Infini-Transformer could catch the entire context \\(N\\times S\\) with the fixed memory size \\(d_{key} × d_{value} \u0026#43; d_{key}\\) that only stores \\(M_s\\) and \\(z_s\\) over the every attention heads and layers.\nFigure 2. Infini-Transformer (top) has an entire context history whereas Transformer-XL (bottom) discards old contexts since it caches the KV states for the last segment only. Experiments # Infini attention was tested on three main benchmarks such as long-context language modeling, passkey retrieval and book summarization.\nLong-context Language Modeling # Table 2: Long-context language modeling results are compared in terms of average token-level perplexity. Comp. dentoes compression ratio. Infini-Transformer outperforms memorizing transformers with memory length of 65K and achieves 114x compression ratio. The authors trained and evaluated small Infini-Transformer models on PG19 [5] and Arxiv-math [6] benchmarks. They noted that the model with Infini Attention outperformed the baseline model. Additionally, extending the training sequence length further improved the perplexity score, a metric indicating language model performance, where lower scores signify better performance.\nFigure 3. Gating score visualization. Figure 3 illlustrates the gating value ( \\(sigmoid(\\beta)\\) ) of each heads and layers of the pretrained Infini-Transformer. The speciallized head means that the gating scores are close to 0 or 1 which only pass the local attention outputs or context attention output from the memory. The mixer head, of which the gating scores is near 0.5, combines the both information.\n1M passkey retrieval benchmark # Table 3: Infini-Transformers solved the passkey task with up to 1M context length when fine-tuned on 5K legnth inputs. We report token-level retrieval accuracy for passkeys hidden in a different pat (start/middle/end) of long inputs with lengths 32K to 1M The pass-key task is a task that hides a random number in a long context and asks it back in the model output. Below is the input format of the passkey task.\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. (repeat x times) The pass key is 9054. Remember it. 9054 is the pass key. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. (repeat y times) What is the pass key? The pass key is\nWhile previous work [14] showed that the 8B LLaMA model can solve tasks up to 32K in length when fine-tuned with the same 32K-long inputs using Position Interpolation, Infini-Transformer takes this problem further, fine-tuning with only 5K-long inputs and testing it on a 1M-long region. They reported both zero-shot accuracy and finetuning accuracy. Table 3 shows that Inifni-Transformer solved the passkey test perfectly from 32K to 1M after FT.\n500K length book summarization (BookSum) # Table 4: 500K length book summarization (BookSum) results. The BART, PRIMERA and Unlimiformer results are from Bertsch et al. (2024). Figure 4: Infini-Transformers obtain better Rouge overall scores with more book text probived as input. The researchers scaled up their approach by continuously pre-training an 8B LLM model with an 8K input length over 30K steps. They then fine-tuned this model for the book summarization task, setting the input length to 32K for fine-tuning and 500K for evaluation. They used a generation temperature of 0.5, a top-p of 0.95, and a decoding step of 1024 to generate book summaries. Their model outperforms previous best results and sets a new state-of-the-art on BookSum by processing the full text of books. Figure 4 shows the overall Rouge score for the validation split of the BookSum data, indicating a clear trend: the more text provided from the book, the better the summary performance for Infini-Transformers.\nConclusion # This work presents a novel attention, Infini-Attention, which is a close integration of compressive memory module into the vanilla dot-product attention layer. It builds both masked local attention and long-term linear attention into a single transformer block. It helps handle infinitely long processes with limited memory and computation resources. As long-context LLMs are increasingly important today, having such an effective memory system shows the potential for powerful reasoning, planning, continuous adaptation and capabilities not previously seen in LLMs.\nDiscussion # Since Infini-Attention compresses and stores information, it is questionable whether it can produce inconsistent or confusing output if it conflicts with the knowledge of the base model. It use the name Infini-Attention due to its incremental updates, but the authors only test it on 1M tokens. As mentioned earlier, it is doubtful that it can perform on truly infinite data with minimal information loss. We can use memory-based not only for language tasks but for the other tasks. For example, In transformer models for videos [15], they compute over spatio-temporaly combined 3D input (multiple frames) with transformer model, but this requires huge computation overhead. Instead, we could only use a transformer models with 2D input that only takes one frame with the compressive memory that stores global context extracted from the previous frames. Reference # [1] “Attention Is All You Need.”, Vaswani et al.\n[2] “Metalearned neural memory.”, Munkhdalai et al.\n[3] “Tensor product variable binding and the representation of symbolic structures in connectionist systems.”, Smolensky.\n[4] “Transformer-xl: Attentive language models beyond a fixed-length context.”, Dai et al.\n[5] “Compressive transformers for long-range sequence modelling.”, Rae et al.\n[6] “Memorizing transformers.”, Wu et al.\n[7] “Recurrent Memory Transformer.” Bulatov et al.\n[8] “Adapting Language Models to Compress Contexts.”, Chvalier et al.\n[9] “In-context Autoencoder for Context Compression in a Large Language Model.”, Ge et al.\n[10] “Leave No Context Behind: Efficient Infinite Context Transformer with Infini-attention.”, Munkhdalai et al.\n[11] “Metalearned neural memory.”, Munkhdalai et al.\n[12] “Learning associative inference using fast weight memory.”, Schlag.\n[13] “Transformers are rnns: Fast autoregressive transformers with linear attention.”, Katharopoulos et al.\n[14] “Extending context window of large language models via positional interpolation.” Chen et al.\n[15] “ViViT: A Video Vision Transformer.”, Arnab et al.\n"},{"id":12,"href":"/docs/spring24/12_/","title":"12","section":"Spring24","content":" Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies # Posted by: Harit Keawmuang, Junkyeong Park\nAuthors: Zichao Li (University of California, Santa Cruz), Cihang Xie (University of California, Santa Cruz), Ekin Dogus Cubuk (Google Deepmind)\nIn recent years, there has been a growing interest in image-and-language representation learning, which aims to capture the complex interactions between visual and textual information. The Contrastive Language-Image Pre-Training (CLIP) framework has emerged as a leading approach in this field, utilizing large-scale text and image data to create a unified representation space. CLIP has achieved remarkable performance across various tasks and has demonstrated robust generalization to out-of-distribution data. While prior studies on scaling CLIP have focused on scenarios with substantial computational resources, this paper investigates the performance of CLIP under resource constraints, specifically examining the effects of data size, architecture, and training strategies.\nThe study explores the impact of different training data sizes, showing that smaller, high-quality datasets can outperform larger, lower-quality ones. This is critical for practical applications where data quality and computational limits are significant considerations. The research also compares various architectures, highlighting that larger vision transformers (ViTs) do not always guarantee better performance and that CNNs may be more effective when data is limited. Additionally, the paper evaluates different training strategies, including SLIP, FLIP, CLIP, and CLIP+Data Augmentation, revealing that data augmentation can enhance performance without significant computational costs. These findings provide valuable insights for efficiently training and deploying CLIP models, making advanced image-and-language learning more accessible and affordable.\nWhat is CLIP? # CLIP effectively merges the capabilities of natural language processing (NLP) and computer vision. By learning from images and their textual descriptions, CLIP unifies text and image understanding, allowing it to perform various tasks without task-specific training.\nRelated Works # In the recent years, NLP has advanced significantly in pre-training language models on large text datasets. Simultaneously, computer vision has improved by pre-training convolutional neural networks (CNNs) on extensive image datasets. The CLIP model combines these approaches by jointly pre-training on images and text using a contrastive loss function, creating a shared embedding space for both.\nRecent efforts have focused on improving scalability and efficiency of CLIP model. For example, FLIP was introduced to minimize the computation by masking image patches, enabling larger batch sizes without sacrificing performance. Most research has focused on large-scale training with significant computational resources, utilizing ViT large models on extensive datasets. However, less attention has been given to optimizing CLIP for smaller training budgets.\nMethod # Training Pipeline, Dataset, and Hyperparameters # In this paper, they adopted the identical training approach as CLIP, which employs a contrastive loss to simultaneously train the vision and text encoders from scratch. This loss function encourages the encoders to map related image-text pairs to similar feature representations in a shared embedding space by minimizing the distance between positive pairs and maximizing the distance between negative pairs. Key aspects of the training include:\nMinimal data augmentation: Images resized to 224x224 and pixel values normalized to the range of -1 to 1. Optimizer: AdafactorShazeer \u0026amp; Stern with β1 = 0.9 and β2 = 0.999. Batch size: 16k Learning rate: Initial rate of 0.001 with a cosine learning scheduler and weight decay of 0.0001. Evaluation Matrices # Zero-shot transfer evaluation: Assesses the model\u0026rsquo;s ability to generalize to new tasks without fine-tuning. Linear probe evaluations: Freezes the vision encoder and optimizes the fully connected layer\u0026rsquo;s learning rate. Retrieval performance on MSCOCO captions: Ranks text captions based on cosine similarity with image embeddings, reporting Recall@1 for image-to-text retrieval and average results for text-to-image retrieval. Data # Data Quantity # To evaluate the effect of data quantity on CLIP\u0026rsquo;s performance, they conducted experiments with datasets of different sizes: 10M, 25M, 100M, 200M, and 400M. Using ViT-B/32 as the vision encoder, models were trained for 2 to 32 epochs.\nFigure 1. Data Quantity: Zero-Shot performances with the same dataset size across varied training epochs Results showed that for smaller datasets (e.g., 25M), increasing epochs did not significantly improve ImageNet performance. In contrast, larger datasets (e.g., 400M) benefited from more epochs. Additionally, zero-shot performance on ImageNet variants followed a similar pattern: larger datasets and longer training improved performance. However, the correlation between performance on ImageNet and its variants was inconsistent, with some datasets showing improved results in specific variants but not others.\nFigure 2. Data Quantity: Few-Shot Performances on ImageNet They also observed that the few-shot performance also showed a similar trend to the zero-shot performance.\nFigure 3. Data Quantity: Retrieval Performances on MSCOCO In Retrieval Performances, a slightly different trend emerged. Specifically, they found that there was little to no improvement in both image retrieval and text retrieval performance when the number of epochs exceeded eight.\nData Quality # They also examined the impact of data quality by creating subsets of the 3.4B dataset based on image-text similarity, selecting the top 20%, 40%, 60%, and 80% highest-quality data.\nFigure 4. Data Quality: Zero-Shot Performances on ImageNet. (a) trained for one epoch. (b) trained for the same number of sampled data. Models trained on these subsets for a single epoch demonstrated that higher quality data subsets yielded superior zero-shot performance on ImageNet. Specifically, the Top40% subset outperformed the entire dataset despite fewer iterations. When comparing datasets with an equal number of samples, the Top40% dataset achieved the best performance, highlighting the importance of data quality in training CLIP models.\nFigure 5. Data Quality: Few-Shot Performances on ImageNet. (a) and (b) one epoch. (c) and (d) the same number of sampled data. Additionally, when the number of sample data points is the same, higher quality datasets have superior 5-shot and 10-shot performance.\nFigure 6. Data Quality: Retrieval Performances on MSCOCO. (a) and (b) one epoch. (c) and (d) the same number of sampled data. When it comes to search performance, the top 80% datasets in particular show the most impressive retrieval performance.\nVariants of Vision Transformers # This study examines how the performance of various CLIP models, differentiated by the size of their vision encoders, is influenced by dataset size and the number of sampled data points. They used different vision encoders (ViT-Ti/16, S/16, B/32, B/16, L/16) while keeping text transformers fixed at vit-base. They sampled ten subsets from the full dataset, ranging from 10M to 3.4B samples, maintaining consistent data distribution and quality. Models were trained for one epoch to assess the effect of data quantity, ensuring fair comparison by training all subsets for the same number of iterations.\nFigure 7. Various ViTs: Zero-Shot performances with various numbers of sample data Zero-shot performance on ImageNet revealed that larger vision encoders (e.g., ViT-L/16) did not consistently outperform smaller ones when the sample size was under 100M. As data size increased, larger encoders showed better performance.\nFigure 8. Various ViTs: Zero-Shot performances with the same number of sampled data: 3.4B As the dataset size grows, the performance difference between larger ViTs and their smaller counterparts becomes more pronounced. Additiallay, accuracy trends across various datasets (ImageNet-R, ImageNet-Sketch, ImageNet-V2, ObjectNet) were nearly linear, except for ImageNet-A, which had a non-linear improvement, highlighting its challenging nature. (appendix)\nFigure 9. Various ViTs: Linear probing performances with various sizes of vision encoders with the same number of sampled data: 3.4B Linear probing results indicated that for smaller datasets, ViT-L/16 underperformed compared to smaller models, but excelled with more data. Larger ViTs demonstrated better robustness on out-of-distribution datasets.\nFigure 10. Various ViTs: Retrieval Performances on MSCOCO Retrieval tasks showed ViT-L/16 performed poorly with less than 100M samples but improved with more data, aligning with zero-shot trends and benefiting more from larger datasets compared to smaller models.\nComparison of Network Architectures # To effectively choose the best network architectures, they performed a comparison among the various architectures. Previous studies have explored various vision encoders for CLIP, such as ResNet, MLP-Mixer, and ViT, but some architectures like Swin-Transformer and ConvNext haven\u0026rsquo;t been investigated. Here, they compared CNN and vision transformer architectures with similar computational costs, including ViT-B/32, ResNet-50, ConvNext-T, Swin-T, and Mixer-B/32. In Zero-shot, when considering limited data samples, ResNet-50 performs better initially, but ViT-B/32 achieves superior performance with more samples due to its stronger ability to capture global information (see Figure 11(a)). In linear probing, MLP-Mixer outperforms others with fewer samples, but ViT-B/32 excels with larger datasets. ViT and MLP-Mixer show better robustness, likely due to their lower inductive bias, leading to improved generalization (Figure 11(b)). For retrieval tasks, ResNet-50 is better with smaller sample sizes, but ViT-B/32 surpasses it as sample sizes increase. Mixer-B/32 performs poorly in retrieval tasks, making ViT the preferred choice for CLIP\u0026rsquo;s vision encoder across various tasks.\nFigure 11. Performances of the various network architectures Training Strategies # In this section, the various training strategies for CLIP are explored, including SLIP, FLIP, and a proposed method from this paper called CLIP+Data Augmentation. SLIP enhances the vision encoder through self-supervised learning but is computationally expensive compared to the original CLIP. FLIP masks patches in training images to reduce computation. However, CLIP+Data Augmentation aimed to enhance CLIP\u0026rsquo;s vision encoder while mitigating the computational demands associated with previous self-supervised learning approaches. By applying data augmentation directly to input images, they offered a cost-effective alternative, validated across four subsets with 30 epochs of training using techniques like crop\u0026amp;flip, RandAugment, and Stacked RandAugment. The results in Figure 12 demonstrated consistent performance improvements of all three methods over raw CLIP, with no additional computational burden incurred, even enabling comparable performance to larger datasets, exemplified by the Stacked RA model trained on a dataset half the size achieving similar results.\nFigure 12. Comparison between various data augmentation for CLIP Their experiments on the ImageNet dataset show that SLIP outperforms CLIP and FLIP when training samples are under one billion, indicating the benefit of self-supervised learning for limited data. However, as sample size increases, CLIP and FLIP surpass SLIP, suggesting that enhancing vision encoders isn\u0026rsquo;t necessary for large datasets. Additionally, SLIP is twice as computationally expensive as CLIP and performs worst in zero-shot tasks when costs are equal. Data augmentation, particularly CLIP + Data Aug, improves performance and generalization on ImageNet and its variants without extra computational costs, especially for larger datasets and multiple epochs of training as presented in Figure 13.\nFigure 13. Zero-shot performance with the various training strategies In the linear probing evaluation, vision encoders trained with CLIP + Data Aug consistently outperformed the other strategies, particularly on OOD datasets. CLIP and CLIP + Data Aug also showed better robustness than SLIP with similar ImageNet accuracy. Combining CLIP with data augmentation offers a more effective feature extractor, balancing performance, and computation cost. The training results on linear probing performance are shown in Figure 14.\nFigure 14. Linear probing performance with the various training strategies In retrieval tasks, SLIP consistently outperformed CLIP, CLIP + Data Aug, and FLIP on both image and text retrieval across all dataset sizes. Unlike its zero-shot performance, SLIP showed the best results for retrieval tasks as presented in Figure 15, suggesting it is a superior strategy for these tasks despite being less effective for classification.\nFigure 15. Retrieval performances with the various training strategies Conclusion # This study examines how data size, network architecture, and training methods affect CLIP\u0026rsquo;s performance. Their experiments highlight the critical roles of data quantity and quality. They also demonstrate that data augmentation can improve CLIP\u0026rsquo;s performance with minimal additional computational cost. Furthermore, they investigate various network architectures and training strategies, finding that some outperform others depending on the computational budget, emphasizing the need for careful selection. From our perspective, the balance between computational efficiency and model accuracy is crucial, and exploring adaptive methods could yield significant benefits. Future research could focus on integrating transfer learning with CLIP to enhance domain-specific performance and investigating AutoML techniques for optimal architecture and strategy selection.\n"},{"id":13,"href":"/docs/spring24/13_/","title":"13","section":"Spring24","content":" Exploring µ-Parameterization in Large-Scale Neural Networks # Paper : A Large-Scale Exploration of µ-Transfer Author : Lucas Dax Lingle produced by Jeonghyun Choi, Minhye Choo Introduction # In the field of artificial intelligence, especially in natural language processing and computer vision, large neural network models have become a cornerstone. However, the initialization and learning rates of these models are often determined through heuristic methods, varying significantly between different studies and model sizes. This inconsistency can lead to suboptimal performance, particularly when scaling up models, resulting in inefficient training processes and less effective models. As models grow larger, the tuning process becomes increasingly costly and time-consuming.\nThe concept of µ-Parameterization (µP) provides a potential solution to this problem. µP offers scaling rules for model initialization and learning rates, enabling zero-shot hyperparameter transfer from small models to larger ones. This technique promises stable training and optimal hyperparameters at scale with minimal cost. Despite its potential, µP has not been widely adopted due to its complexity and the need for further empirical validation.\nFigure 1 : In the default parameterization in PyTorch, the graph on the left, the activation scales diverge in width after one step of training. But in µP, the graph on the right, the activation scales change by a consistent amount regardless of width for any training step. The y-axis shows the change of network activation scales on a fixed input after t=0, 1, 2, 3, and 4 steps of training as the width of the model varies, which is shown along the x-axis. .\nIn this blog post, we delve into the details of µ-Parameterization, its underlying principles, and its practical applications as explored in the paper \u0026ldquo;A Large-Scale Exploration of µ-Transfer\u0026rdquo; by Lucas Dax Lingle.\nWhat is µ-Parameterization? # Concept and Principles # µ-Parameterization (µP) is a set of rules for initializing neural networks and setting learning rates that allows for the seamless transfer of hyperparameters from small proxy models to larger target models. This approach is grounded in a Gaussian Process interpretation of deep neural networks, where the width of the network (number of neurons per layer) is a critical factor.\nThe core idea is to scale the initialization and learning rates based on the width of the network. The general formulation of µP when training with the Adam optimizer and using an i.i.d. Gaussian initialization is as follows:\nInitialization Variance: Parameters are initialized with a variance that scales inversely with the width of the layer. For example, if the width of a layer is M, then the initialization variance for weight matrices WAQ, WAK, WAV is \\(\\frac{1}{M}\\) .\nLearning Rate: The learning rate for each parameter is scaled based on the width of the network. For instance, the learning rate for the same weight matrices WAQ, WAK, WAV is \\(\\frac{αP}{M}\\) , where α is the base learning rate and P is a fixed proxy model width.\nThese scaling rules ensure that the behavior of small and large models remains consistent, facilitating the transfer of optimal hyperparameters across different model sizes.\nPractical Implementation # In practical terms, µP involves specific scaling rules for various components of a transformer model, which is a popular architecture in NLP and computer vision. For example, the initialization variance for the weight matrices in the attention mechanism and MLP blocks is set according to the width of the model. Similarly, the learning rate is adjusted to maintain consistent training dynamics across different scales.\nBelow is an example of µP scaling rules for transformers:\nParameter Initialization Variance Adam Learning Rate WE 1 α WAQ 1/M αP/M WAK 1/M αP/M WAV 1/M αP/M WAO 1/(HD) αP/M WFI 1/M αP/M WFO 1/F αP/M WU 1/M² αP/M Table 1: µP scaling rules for transformers.\nIn this table, M represents the model width, H the number of heads, D the head width, F the hidden width of the MLP, and α the base learning rate.\nµ-Transfer # Figure 2 : Illustration of µ-Transfer\nOne of the significant advantages of µ-Parameterization is the concept of µ-Transfer. This method allows hyperparameters, such as learning rates, found optimal in small models to be transferred directly to larger models without the need for extensive re-tuning. This process is particularly beneficial for scaling models efficiently and maintaining consistent performance across different model sizes.\nSteps in µ-Transfer # Training a Small Proxy Model: Begin by training a small proxy model, which is easier and less expensive to experiment with. Perform hyperparameter tuning on this model to find the optimal learning rate and other hyperparameters. For instance, let\u0026rsquo;s denote the optimal learning rate found for this small model as α.\nScaling the Hyperparameters: Use the scaling rules provided by µP to adapt the hyperparameters for a larger model. The key scaling rule here is that the learning rate should be adjusted based on the ratio of the widths of the large model to the small model. For example, if the small model has a width 128 and the large model has a width 2048, the scaled learning rate for the large model would be \\(\\frac{α2048}{128}\\) .\nApplying the Scaled Hyperparameters: Implement these scaled hyperparameters in the larger model. This involves adjusting the initialization variance and learning rates according to the µP rules to ensure that the training dynamics remain stable and consistent.\nBenefits of µ-Transfer # Efficiency: By using µ-Transfer, researchers can avoid the costly and time-consuming process of hyperparameter tuning on large models. Instead, they can perform this tuning on smaller models and scale the results.\nConsistency: µ-Transfer helps maintain consistent training dynamics across different model sizes. This consistency is crucial for achieving optimal performance as models scale up.\nSimplicity: The process of scaling hyperparameters using µP is straightforward once the initial tuning on the small model is complete. This simplicity can significantly reduce the complexity of managing large-scale model training.\nAblation Experiment # Setup \u0026amp; Objective # The objective of these experiments is to examine how various methods that can enhance performance affect the actual transfer of learning rates from smaller width models to larger models with µP and their impact on performance in reality. While the existing µP aimed at transferring initialization and learning rates, this experiment focused on the learning rate, an important hyperparameter in large transformer models. The experiments were implemented using Jax/Flax on TPU V3, and the optimal learning rate was determined by measuring the validation loss. Models with widths of \\(M\\) = {128, 512, 2048} have parameters ranging from 4.7M to 1.2B, with the depth fixed at L = 24. The reason for focusing solely on width and not depth is that in the case of depthwise µP, only one linear layer is used per residual block, whereas transformers use at least two layers. So, in this experiment, width is the main change to control the # of parameters.\nBaseline \u0026amp; Summary # The baseline represents the experimental results used as a reference for performance improvement or degradation across various experimental settings. In the baseline using µP, it is confirmed that the optimal learning rate for the smallest model is also the optimal learning rate for larger models that are 4x wider (16x larger). The experimental results can be categorized into three main groups:\nTransfer O, Performance Improvement O Cases where both learning rate transfer and performance improvement is observed. Examples: Zero query initialization, Multiplicative Nonlinearities, SP unembedding initialization, Multi-query attention, batch size(4x) Transfer O, Performance Improvement X Cases where learning rate transfer, but there was no improvement in performance. Examples: Projection biases, Embedding normalization, Cosine schedule Transfer X Cases where learning rate does not transfer. Examples: RMS Norm gain, Decoupled weight decay, SP attention scale Cases where learning rate does not transfer are not separately classified with performance improvement, as performance improvement in these cases is not significant.\nProjection Biases # Adding a bias vector to the linear layer does not guarantee an improvement in model performance. In fact, experimental results showed that the performance was similar to that of the baseline and learning rate transfer across the model size and width under µP.\nRMS Norm gain(vector \u0026amp; scalar) \u0026amp; Embedding normalization # RMSNorm is a normalization method that uses the root mean square instead of the mean and standard deviation. \\[\\bar{a_i}=\\frac{a_i}{\\mathrm{RMS(a)}}g_i, \\text{where } \\mathrm{RMS(a)}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^na_i^2}\\] To re-scale the standardized summed inputs, a trainable gain \\(g_i\\) and bias \\(b_i\\) were used. The gain can be implemented in two forms: a vector gain and a scalar multiplier. Similar to projection bias, the use of a trainable gain does not guarantee performance improvement. The results showed that transfer does not occur in any case, vector gain and scalar multiplier, and performance degradation was observed in the models with the largest width. On the other hand, using normalized embedding with RMSNorm without a trainable gain did not improve performance, but it was observed that the learning rate transfer was successful.\nQuery Initialization # For query projection, µP initialization typically uses a Gaussian distribution with variance \\(\\Theta(1/M)\\) , but zero-initialization has been proposed to facilitate transfer. Transfer occurs with zero-initialized query, and there was a slight improvement in performance compared to the baseline, traditional Gaussian initialization.\nCosine schedule # Adjusting the learning rate over iterations is an open problem with no definitive solution, with methods like power and exponential scheduling. This experiment use cosine scheduling, which is the method that periodically decreases and increases the learning rate to prevent convergence to local minima. This approach can help the model escape suboptimal points and potentially find better solutions.The baseline used a linear schedule, but switching to cosine scheduling did not negatively impact transfer. However, a slight performance degradation was observed with cosine scheduling compared to the baseline.\nDecoupled weight decay # When optimizing hyperparameters with Adam, the decoupled weight decay method separates the weight decay from the optimization step, allowing independent exploration of the learning rate and weight decay factor. Experimental results using decoupled weight decay showed that optimal learning rate transfer was not achieved. A large \\(\\lambda\\) value was suggested as a potential cause for this issue. In this experiment, \\(\\lambda = 0.1\\) was used. The smaller difference in optimal learning rates between small and large models, compared to other transfer failures, suggested that reducing \\(\\lambda\\) may help resolve the transfer problem.\nMultiplicative Nonlinearities # To enhance the performance of transformers, multiplicative nonlinearity activation functions such as SwiGLU and Squared ReLU can be utilized. SwiGLU is an activation function created by combining the Swish activation function with the Gated Linear Unit (GLU) and is considered to get better performance compared to traditional activation methods. . The formulas for each are as follows: \\(Swish(x) = x\\sigma (\\beta x)\\) ( \\(\\sigma\\) : sigmoid function, \\(\\beta\\) : trainable parameter ) \\(GLU(x,W,V,b,c) = \\sigma(xW\u0026#43;b)\\otimes (xV\u0026#43;c)\\) ( \\(W,V\\) : trainable tensor, \\(b,c\\) : trainable tensor bias, \\(\\otimes\\) : element-wise multiplication) \\(SwiGLU(x,W,V,b,c, \\beta) = Swish_\\beta(xW\u0026#43;b)\\otimes(xV\u0026#43;c)\\) Similarly, Squared ReLU, which is obtained by squaring the ReLU activation function, is known to help improve performance. Experimental results showed that they allow µ-transfer of the learning rate across model sizes and unlike the RMSNorm gain, there was a performance improvement from the perspective of multiplicative interaction.\nStandard Parameterization # Yang et al. (2021) stateed that µP models perform better than SP models, and our various experiments with SP settings confirmed that some of the µP settings offer advantages in terms of transfer and model performance.\nattention scale Usual attention scaling is \\(\\tau^{-1} = 1/\\sqrt{D}\\) , while µP proposes \\(\\tau^{-1} = \\Theta(1/D)\\) , and baseline experiment was implemented using a simple \\((1/D\\) ) scaling. In this experiment, attention scaling was \\(1/\\sqrt{D}\\) to check the SP setting. For the \\(M = 128\\) model, the optimal learning rate was \\(2^{-8}\\) , but for larger models, the optimal learning rate was changed to \\(2^{-6}\\) . This means that transfer did not occur, and performance slightly deteriorated compared to the original baseline. unembedding initialization The initialization of µP\u0026rsquo;s unembedding matrix follows a Gaussian distribution with a variance of \\(\\Theta(1/M^2)\\) , while standard parametrization (SP) uses \\(1/M\\) . Experiments using the original SP method with \\(1/M\\) showed that transfer was maintained and there was a slight improvement in performance for larger models. To compare the result of the SP and µP, this experiment was implemented using SP and compared the result with baseline's. The differences between the baseline and SP included using trainable biases in linear layers, trainable gains in RMSNorm layers, attention scale \\(1/\\sqrt{D}\\) , and unembedding initialization variance \\(1/M\\) . All other hyperparameters remained the same. The combined results for SP transformers showed that transfer does not occur, and the optimal loss is lower in performance compared to the baseline. Lion Optimizer # The Lion optimizer is known for being more than twice as memory-efficient as Adam while delivering similar performance in transformers. This optimizer restricts updates to \\(\\{-1, 1\\}\\) for each coordinate, yielding a coordinate size of \\(\\Theta(1)\\) per step. Consequently, it seems suitable to use the existing \\(\\Theta(1/M)\\) transfer rule as it is. However, experimental results showed that the learning rate transfer was not successful, indicating the need for further research on the transfer rule.\nMulti-query attention # Transformer LLMs can use multi-query attention and group generalization to increase inference speed by sharing keys/values across multiple heads. Experimental results showed that these methods lead to significant performance improvements compared to other methods, and transfer also occurred effectively.\nBatch Size(4x larger, 4x smaller) # By adjusting the batch size while keeping the number of training tokens constant, it is possible to reduce training time or determine the minimum batch size required for operation. In this case, the learning rate formula is adapted by using twice the specified value for 4x larger batch sizes and half the value for 4x smaller batch sizes. The results showed that learning rate transfer effectively in both cases, though further research is needed to determine the optimal batch size.\nLarge-scale Transfer Experiement # To verify if transfer is possible over a larger scale difference, experiments was implemented by reducing \\(L\\) to 12 and setting the width to \\(\\{128, 512, 2048, 8192\\}\\) , resulting in models with 2M, 40M, 600M, and 10B parameters(5000x). Zero query and Squared ReLU were used, which showed good performance and did not negatively impact transfer. The results confirmed that, despite a 5000x scale difference, the learning rate transfer well. Related Works # To summarize the contributions of the paper and consider the future works, this section summarizes the contents of a relevant research paper and explains its works and limitations.\nTensor Programs V: Tuning large neural networks via zero-shot hyperparameter transfer The performance of µ-Transfer, which reduces computation by tuning a small model and then transferring the obtained hyperparameters to a large model instead of tuning the large parameter model directly, was demonstrated experimentally. Figure : (1) MLP with SP (2) MLP with µP (3) transformer with SP (4) transformer with µP\nThe paper experimentally confirmed that when Standard Parameterization was applied to MLP and Transformer models, the hyperparameter stability was low. However, using µ Parameterization enabled stable transfer by using width-128 network and width-8192 network. The experiement sweeped the width and depth by changing only one of the hyperparameters—learning rate, output weight multiplier, initialization standard deviation, and learning rate schedule to check if they transfer across scale dimensions. Optimal hyperparameters transfered well across scale dimensions when the minimum width, depth, batch size, sequence length, and training steps were met. However, unlike the changes in width where hyperparameter transfer occured to wider models, the transfer to deeper models was less effective. The reason is same as the main paper's fixed depth and the experiments in this paper also adjusted the scale by fixing the depth and only varying the width. In this paper, experiments were conducted to evaluate the efficiency and performance of µ-Transfer using various models: from a 10M to 40M Transformer on IWSLT14 De-En, from a 15M to 211M Transformer on WMT14 En-De, from a 13M to 110M BERT and to 350M BERT-large. The largest model tested was the GPT-3, where the width is shrunk from 4096 to 256, resulting in a scale difference from 40M to 6.7B, representing a 168x scale difference. The total tuning cost was only 7% of total pretraining cost and the hyperparameter was stable across the scale. Contribution of the main paper : The performance of µ-Transfer, which has been proven effective with a scale difference of up to 168x, was experimentally confirmed to be reliable even with a scale difference of up to 5000x. The paper also conducted experiments not only comparing SP (Standard Parameterization) and µP (µ Parameterization) overall but also by changing specific elements such as attention scale.\nConclusion # The paper demonstrates that the transfer properties observed with µ-Parameterization (µP) can be maintained across most scenarios. µP outperforms standard parameterization (SP) and confirms the efficacy of part-by-part transfer for elements like attention scale and unembedding initialization, validating µP\u0026rsquo;s superiority. Additionally, it shows that transfer is feasible for models ranging from 2M to 10B parameters (5000x), suggesting applicability to larger models. However, some issues are identified where optimal learning rate transfer does not occur, or there is a performance decline in large models. For instance, trainable RMSNorm gain and decoupled weight decay do not function properly for learning rate transfer. Although transfer is observed with projection biases and cosine scheduling, there is no performance improvement or even a decline. The impressive performance of µ-Transfer demonstrated in this paper is expected to be highly beneficial in the current AI landscape, where model sizes are continually increasing. However, tuning hyperparameters for large models is not always feasible, indicating a need for further research. The suggested further research areas are as follows:\nDetailed explanation and analysis of each experiment, investigating the causes of transfer failures and potential solutions through the expansion of each experiment (e.g., experiments on specific SP elements, experiments with various batch sizes). Scaling adjustment through depth rather than width and exploring its transferability. Transferability of other hyperparameters. Personal Insights and Future Directions # The significance of this paper lies in summarizing the impact of various methods on performance and transfer under µ through ablation experiments. However, it is limited by focusing on many ablations without additional experiments for detailed results, leading to a lack of comprehensive analysis. For instance, where transfer fails, minor differences in optimal learning rates between small and large models suggest potential for improvement, but no further experiments were conducted. Structural issues in transformers make scaling adjustments through depth challenging, and research on architectures that allow adjustments through both depth and width could increase flexibility. Additionally, expanding the scope of transfer to include other hyperparameters like the output weight multiplier, initialization standard deviation, and learning rate schedule is necessary.\nWe also believe µ-Parameterization offers a promising solution to one of the significant bottlenecks in training large neural networks. Its potential to standardize and simplify the hyperparameter tuning process is a substantial step forward. However, the complexity of its implementation and the need for more empirical validation remain hurdles to its widespread adoption.\nTo fully realize the benefits of µP, the following future research directions are proposed:\nEmpirical Validation: Conduct large-scale studies across various types of neural networks and applications to validate µP’s effectiveness and identify any limitations. Tool Development: Create user-friendly tools and libraries that automate the µP scaling process, making it more accessible to practitioners. Hybrid Approaches: Explore combining µP with other optimization methods like LAMB to further enhance training efficiency and performance. By addressing these future research directions, the full potential of µ-Parameterization can be unlocked, paving the way for more efficient and effective neural network training methodologies.\nReferences # Lingle, L. D. (2024). A Large-Scale Exploration of µ-Transfer. arXiv preprint. Retrieved from arXiv:2404.05728. Yang, G., \u0026amp; Hu, E. J. (2021). Tensor Programs V: Tuning large neural networks via zero-shot hyperparameter transfer. Advances in Neural Information Processing Systems. Biao Zhang and Rico Sennrich. Root mean square layer normalization. CoRR, abs/1910.07467, 2019. URL http://arxiv.org/abs/1910.07467. Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. CoRR, abs/1711.05101, 2017. URL http://arxiv.org/abs/1711.05101. Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/abs/2002.05202. Noam Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019. URL https://arxiv.org/abs/1911.02150. "},{"id":14,"href":"/docs/spring24/14_/","title":"14","section":"Spring24","content":" BinaryDM: Towards Accurate Binarization of Diffusion Model # Authors: Xingyu Zheng, Haotaong Qin, Xudong Ma, Mingyuan Zhang, Haojie Hao, Jiakai Wang, Zixiang Zhao, Jinyang Guo, Xianglong Liu\nPosted by Junhyuk So, Juncheol Shin\nPreliminary # Diffusion # Diffusion model Diffusion models learn how to remove gaussian noise added to original image. Equation below shows how forward process proceeds. During the forward process, Gaussian noise is gradually added to original image for \\(T\\) times. Strength of the noise is controlled by the term \\(\\beta\\) . \\(x_t\\) denotes corrupted image at time step \\(t\\) . In the reverse process, diffusion model tries to restore original image by estimating conditional distribution \\(q(x_{t} | x_{t-1})\\) with \\(p_\\theta(x_{t-1} | x_{t})\\) . \\( q(x_{t} | x_{t-1}) = \\mathcal{N}(x_t;\\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I) \\\\p_\\theta(x_{t-1} | x_{t}) = \\mathcal{N}(x_{t-1}; \\tilde{\\mu}_\\theta(x_t, t) , \\tilde{\\beta}_{t}I ) \\) Quantization # Quantization example Quantization is an optimization technique which restricts data(weights, activations) in low precision. Not only does it reduce the memory footprint, but it also enables accelerated computations given hardware support to low-precision arithmetic. The quantized values are represented as follows when linear symmetric quantization is applied:\n\\( q(x) = clamp(\\lfloor \\frac{x}{\\Delta} \\rceil, -2^{b-1}, 2^{b-1}-1) ,\\ \\ \\Delta = \\frac{\\alpha}{2^{b-1}-1} \\) x is value to be quantized and \\(\\alpha\\) denotes trunctaion range. \\(b\\) denotes bit-width. \\(\\Delta\\) is step-size, interval between quantization levels. Motivation # While diffusion models achieved great success in generation tasks, its iterative nature act and huge compuation costs act as a bottleneck to real-world application.\nQuantization is one reasonable choice for the optimization of diffusion models. Especially when binarization is applied to weight, floating point operations can be substituted with cheap addition and memory footprint of the model can be reduced greatly.\nHowever, models are hard to binarize in two aspects. One arises from perspective of representation, as binarization is extreme case of quantization which only uses 1bit to represent data. Naive binarization introduces severe degredation in quality of output. Another aspect arises from perspective of optimization. Training becomes unstable with binarized representation and hinders convergence of the model.\nThis work tackles binarization of diffusion models by handling aformentioned two aspects. By introducing Learnable Multi-basis Binarizer(LMB) and Low-rank representation mimicking(LRM), BinaryDM is able to achieve 16.0× and 27.1× reductions on FLOPs and size.\nMethodology # Learnable Multi-basis binarizer # Typical binarization of the weight can be described as follows:\n\\( w^{bi} = \\sigma sign(w) = \\left\\{\\begin{matrix} \\sigma, \u0026amp; if w\\geq 0 \\\\ -\\sigma, \u0026amp; otherwise \\end{matrix}\\right. \\) In this work, authors propose a learnable multi-basis binarizer(LMB) to maintain quality of representations. Instead of using single base, multiple bases are utilized for the binarization. \\(\\sigma_{1}\\) and \\(\\sigma_{2}\\) denote learnable scalar values and initialized as \\(\\sigma_{1}^0 = \\frac{\\left \\| w \\right \\|}{n}\\) and \\(\\sigma_{2}^0 = \\frac{\\left \\| w - \\sigma_{1}sign(w) \\right \\|}{n}\\) \\( w_{LMB}^{bi} = \\sigma_{1} sign(w) \u0026#43; \\sigma_{2} sign(w - \\sigma_{1} sign(w) \\) Gradient of learnable scalar values can be computed as follows: \\( \\frac{\\partial w_{LMB}^{bi}}{\\partial \\sigma_{1}} = \\left\\{\\begin{matrix} sign(w)(1-\\sigma_{2} sign(w))), \u0026amp; if \\ \\sigma_{1}sign(w) \\in (w-1, w\u0026#43;1) \\\\ sign(w), \u0026amp; otherwise \\end{matrix}\\right. \\) \\( \\frac{\\partial w_{LMB}^{bi}}{\\partial \\sigma_{2}} = sign(w - \\sigma_{1} sign(w)) \\) During the inference, computation for each bases are indepedent to each other and can be parallely computed. Thus, diffusion model can be fully accelerated with LMB.\nIt is important to note that LMB is applied at only crucial parts of difusion model. Only modules where features cale is greater than or equal to \\(\\frac{1} {2}\\) input scale. In other words, some of the first consecutive layers and last consecutive layers are binarized with LMB. The binarized modules close to input or output play important role, as they extract patterns from original data or directly influence the final result. Figure below shows result of naive binarization and LMB applied to weights.\nWeight quantization result of naive binarization and LMB Low-rank representation mimicking # Binarization of weights makes the training hard and hiders convergence. Since full precision model is available, it is natural to align intermediate representations of binarized diffusion model and original model as additional supervision. However, fine-grained alignment of high-dimensional representation leads to blurry optimization direction and binarization makes the model hard to mimic full precision model.\nAuthors propose Low-rank Representation Mimicking(LRM) to handle these problems. LRM utilize principal component analysis(PCA) to project representations to low-rank space. Then representaiton aligning is done in low-rank space by minimizing mean squared error (MSE)\nFirst, covariance matrix of \\(i\\) -th module \\(C_i\\) is computed with representation of full-precision diffusion model, \\(\\hat{\\epsilon}_{\\theta_{i}}^{FP}(x_t, t) \\in \\mathbb{R}^{h\\times w\\times c}\\) . Then eigenvector matrix \\(E_i\\) can be obtained and first \\(\\lfloor\\frac{c}{k}\\rceil\\) column eighenvectors are used to compute projected representations, \\(\\mathcal{R}_i^{FP}\\) and \\(\\mathcal{R}_i^{bi}\\) .\n\\( C_{i} = \\frac{1}{{(h \\times w)}^2}\\hat{\\epsilon}_{\\theta_{i}}^{FP}(x_t, t) \\hat{\\epsilon}_{\\theta_{i}}^{FP^T}(x_t, t), \\\\ E_{i}^{T}C_{i}E_{i} = \\Lambda_{i}, \\\\ \\mathcal{R}_i^{FP}(x_t, t) = E_{i}^{\\lfloor \\frac{c}{K}\\rceil}\\hat{\\epsilon}_{\\theta_{i}}^{FP}(x_t, t), \\\\ \\mathcal{R}_i^{bi}(x_t, t) = E_{i}^{\\lfloor \\frac{c}{K}\\rceil}\\hat{\\epsilon}_{\\theta_{i}^{bi}}^{bi}(x_t, t) \\) LRM loss and total loss can be expressed as follows: \\( {\\mathcal{L}_{LRM}}_{i} = \\left\\| \\mathcal{R}_i^{FP} - \\mathcal{R}_{i}^{bi} \\right\\| \\\\ \\mathcal{L}_{total} = \\mathcal{L}_{simple} \u0026#43; \\lambda\\frac{1}{M}\\sum_{i=1}^{M}{\\mathcal{L}_{LRM}}_{i}, \\) where \\(M\\) denotes the number of timestep embedding modules and \\(\\lambda\\) is a hyperparmater coefficient to balance loss terms Since computation of transformation matrix \\(E_i\\) is expensive, it is computed with the first batch of input and fixed during entire traning. As shown in the figure below, LRM stabilizes training process, accelerating convergence.\nProgressive binarization # Despite the enhanced methodology, training process remains slow and unstable. To further stabilize convergence, authors additionally apply progressive binarization strategy. \\(\\lfloor \\frac{M}{2} \\rfloor\\) -th time stepping module is quantized in first iteration and \\(\\lfloor \\frac{M}{2} \\rfloor - i\\) -th and \\(\\lfloor \\frac{M}{2} \\rfloor \u0026#43; i\\) -th modules are quantized in next i-th iteration. As show in the figure, benefit coming from progressive binarization is significant compared to baseline traning process.\nExperiments # Generation Performance # To demonstrate the performance of the binarized diffusion model, the author conducted experiments on two major categories of diffusion models: unconditional and conditional models. The metrics used to measure performance included FID and sFID for assessing the perceptual quality of images, IS for measuring diversity, and Recall and Precision for evaluating accuracy. Additionally, for experiments with baselines, the methods used were LSQ for multi-bit cases and a QAT method similar to STE for single-bit cases. Unconditional Generation\nIn this experiments, authors utilized two types of models: the Pixel Space Diffusion Model, which performs generation directly in the pixel space, and the Latent Diffusion Model, which conducts generation in the latent space of a VAE.\nTable 1 shows the results of experiments conducted with the Pixel Space Model on the CIFAR-10 dataset, using a DDIM sampler with 100 steps. As can be seen in the table, the proposed method outperforms the baseline binarization method across all metrics. Additionally, it also yields better results than LSQ, which uses one more bit, across all metrics.\nTable 2 presents the results of experiments conducted with the same sampling strategy (DDIM 100) in the Latent Diffusion Model (LDM). As shown in the table, BinaryDM also outperforms the baseline across all metrics. However, unlike in Table 1, there are a few cases where LSQ with 2 bits shows better performance.\nFinally, Table 3 shows the experimental results for Conditional Generation. The dataset used was ImageNet 256. In this experiment, two unusual trends were observed: 1. The performance gap between the baseline and BinaryDM is very small, and 2. Lower FID scores than FP were observed in all quantization experiments. However, I personally believe that achieving better performance than FP in all situations, despite using 1-2 bit quantization, is not a reasonable result, suggesting that the experiment may have been conducted incorrectly.\nAblation Study # Three ablation studies were conducted. The first ablation study focused on the various methods of Binary DM, measuring performance changes with the addition of LMB and LRM. The second study evaluated inference efficiency by measuring the FLOPs required during the inference of the binarized DM. The third study examined training efficiency, comparing the training time of the proposed method to that of another diffusion quantization method, Q-Diffusion.\nAs seen in Table 4, when using the BinaryDM methods LMB and LRM independently, the performance improvement is not significant, but when used together, their effect is maximized.\nSecondly, when measuring FLOPs, it was found that despite having a lower FID compared to multi-bit quantization schemes like Q-Diffusion or LSQ, the FLOPs are 2 to 4 times lower.\nLastly, when comparing the training time cost with Q-Diffusion, it was found that the training cost is approximately 1.2 times lower.\nConclusion # BinaryDM is a paper on binary quantization of diffusion models, which achieves dramatic performance improvements by introducing two methods, LMB and LRM, instead of traditional quantization methods. LMB increases the representation quality of binary quantization by using two scaling factors instead of one baseline factor. LRM minimizes the loss of low-frequency components information in the quantized model by performing feature matching in a low-rank space. Furthermore, the progressive quantization proposed in the paper prevents significant information loss in the early iterations, minimizing performance degradation. Extensive experiments demonstrate the superiority of BinaryDM across various tasks.\n"},{"id":15,"href":"/docs/spring24/15_/","title":"15","section":"Spring24","content":" Training LLMs over Neurally Compressed Text # TL;DR # This paper addresses the problem of training large language models (LLMs) using highly compressed text, which offers several advantages: (i) faster training, (ii) improved serving efficiency, and (iii) easier handling of long text spans. However, strong compression can sometimes produce opaque outputs and difficult to use for learning.\nTo overcome this, Equal-Info Windows is proposed, a novel compression technique where text is segmented into blocks which are each compressed in to the same bit length. Experiments demonstrate that this method significantly outperforms byte-level baselines in both (i) perplexity and (ii) inference speed. Although the perplexity is worse than that of subword tokenizers, the method benefits from shorter sequence lengths, leading to reduced latency. Additionally, the paper provides an analysis of properties that contribute to learnability.\nIntroduction # Large Language Models (LLMs) and compression # LLMs are mostly trained over subword tokens, whereby the tokens are produced by tokenizers are compressors that typically achive ~4 compression. The advantages of using compressed text can be analyzed as the follwoing.\nEfficiency LLMs can process more text for the same computational cost, effectively increasing the amount of data seen during training and improving performance. This efficiency also reduces serving costs and inference latency by requiring fewer sequential autoregressive steps. Longer context LLMs can model longer contextual dependencies by reducing the sequence length, enabling transformers to handle longer contexts efficiently. This extended context is crucial for applications like document retrieval and answering coding questions with provided documentation. Distribution of Compute Information is spread more uniformly across the sequence, meaning each token represents an equal amount of information. This ensures the model allocates more compute to complex text spans, similar to \u0026ldquo;Adaptive Computation Time\u0026rdquo; but with dense, identical operations applied at each position. Motivation # Among many compression methods, Arithmetic coding (AC) has been known to reach near-optimal compression rate for a particular model. Motivated by this, the authors suggest the following compresion method.\nA Small language model “M1” is trained over raw byte sequences A frozen \u0026ldquo;M1\u0026rdquo; is used to compress pretraining corpus text by applying a standard compression algorithm, like AC. The compressed bitstream is chunked into tokens, which are used to train a second language model “M2”, that directly reads and writes neural-compressed text. Challenges of LLMs over compressed text # Learnability Strong compression can make the bitstream appear random and difficult to interpret. If M1 compresses too strongly, the bitstream may become too noisy for M2 to detect any meaningful signal. Therefore, M2 must accurately simulate M1\u0026rsquo;s behavior and understand the compression process, which is complex and requires high-precision numerical state management. Also, M2 needs to learn the compression procedure itself. Numerical stability Compression methods can be sensitive to the precise model probabilities used. To achieve lossless compression, it is critical that the probabilities from M1 match exactly during both compression and decompression. However, ensuring this match is difficult in practice due to numerical noise in LLM inference, especially when running on parallel hardware. Multi-model inference For inference, multiple models simultaneously need to run and stored. Assuming M1 is relatively small, this additional overhead may not be a significant drawback compared to a standard tokenizer, which is also a separate model required for tokenizing text input and detokenizing LLM outputs. Method # Briefly, the training process can be summarized as the following steps.\nTraining data # All training data used is English web text from C4 dataset. 128 documents are concatenated to generate a long sequence of text. This results an average length 277,760 bytes (128 documents), whic are split into individual examples and shuffled using the deterministic dataset functionality from SeqIO.\nTraining M1 # Authors use a decoder-only Transformer model, where the final validation performance of the M1 model is 1.457 bits/byte. Similar to how tokenizers are trained, M1 and M2 are both trained on the C4 training data, but the final validation data used to evaluate M2 is unseen during M1 training.\nCompression # Now, modeling compressed text can be difficult because of language modeling model not able to track the state variables used in Arithmetic Coding.\nTo weaken the coding component of AC compression, the authors rest the AC encoder once it has output a set number of bits, creating windows of fixed size where each window is an independently AC-compressed sequence.\nFor example in the figure above, text is encoded into a series of N-bit windows. To determine each successive window, the remaining text is encoded byte-by-byte via Arithmetic Coding until no more bytes can be added without exceeding the target bit threshold, here 16 bits. Both M1 and the AC algorithm are reset at each step, so no information persists across windows.\nTokenization of compressed text # Training M2 directly over the bits from the compression method would be not ideal. Therefore, the bitstreams are converted into tokens, using a vocbulary size of $2^N$, i.e., grouping every $N$ bits to a token.\nAnother critical point to consider it the token compression ratio $L_{iT}/L_{oT}$, the ratio between the input and output token sequence lengths. This metric measures the weakening of Arithmetic coding. Note that the meaning of “token” can differ between the input and output sequences.\nTraining M2 # Finally, the M2 model is trained for 200,000 steps with a batch size of 256 and a sequence length of 512, cumulatively training on 26.2 billion tokens. Models are trained at four sizes with 25M, 113M, 403M, and 2B parameters, excluding embedding parameters.\nExperiments # Baselines # The M2 model is compared with two standard tokenization methods.\nBytes Train directly over UTF-8 bytes Byte tokenizer from ByT5 Models sees 26.2 billion bytes total SetencePiece Train on tokenized text SentencePiece vocabulary of 32,000 tokens from T5. Models see 112 billion bytes total Evaluation metrics # One major point is that models cannot be directly compared on “per-token” metrics such as negative log likelihood loss. Rather, following previous works, perplexity in terms of “bits-per-byte”, $[bits/byte] = (L_{oT} /L_{iT} ) \\ell / ln(2)$ is used. Models are also compared on how much computation (FLOPs) are required to perform inference over a given length of raw text (bytes). When validating the models, C4 validation set is used. Models are run over 20 batches or ~2.6 million tokens.\nResults \u0026amp; analysis # Obviously, simply training over nerual-compressed fails in terms of both bits/byte and inference FLOPs. Moreover, SentencePiece shows quite a impressive performance among the baselines.\nEqual-Info Windows make Arithmetic Coding(AC) learnable # EqualInfoAC[b=16, v=256] outperforms byte-level baselines, improving bits/byte performance and reducing FLOPs/byte due to shorter sequence lengths. EqualInfoAC[b=16, v=65k] models achieve competitive bits/byte performance and require fewer autoregressive steps than SentencePiece models, which can reduce generation latency. Despite SentencePiece\u0026rsquo;s slight edge in bits/byte when FLOPs/byte are constant, EqualInfoAC\u0026rsquo;s shorter sequences offer a significant advantage in latency-sensitive applications. Window size # Analyzing bits-per-token reveals a discernible pattern: longer window lengths pose greater challenges to learning, resembling the complexity of running Arithmetic Coding over the entire sequence. Shorter window sizes(16-bit windows) yield the best performance in terms of bits/byte. However, longer window sizes(128-bit windows) exhibit better compression rates despite limited learning by the models beyond a uniform distribution.\nSize of M2 vocabulary # Using a larger 16-bit vocabulary (v=65k) for tokenizing compressed text leads to a doubling in token compression rate, evident from the leftward shift of each curve depicted in the figure below.\nConclusion # To sum up, the paper can be summarized as the following.\nThe naive approach does not work, requiring a realtively simple modiciation. Compression Equal Info Windows results beter performance than byte-level models in terms of perplexity and inference cost, and close to the performance of SentencePiece tokenization Worse perplexity than subword tokenizers for models trained with the same parameter count, but offers benefit of shorter sequence lengths. Discussion # Though the Equal Info Windows shows quite impressive performance compared to byte-level models, worse preplexity compared to subword tokenizers is critical. Since the proposed methods contains the compression of bytes to tokens, this limitations be more highlighted.\nFuture reserach direction # Variational length compression Currently, nearly the same length of bits are compressed into tokens. If compresingvariable length of bits to tokens, i.e., dictionary of variable lengths is possible would be a interseting research. Hierarhical compression Bits are compress into tokens, and tokens are generated by the M2 model. The concept of using compression multiple times could be extended more, compressing list of tokens into another set of tokens. In other words, a hierarhical compression would also be a interesting research. Reference # I. H. Witten, R. M. Neal, and J. G. Cleary. Arithmetic Coding for Data Compression. Communications of The Acm, 30(6):520–540, June 1987 C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research (JMLR 2020), 21(140):1–67, 2020. A. Roberts, H. W. Chung, G. Mishra, A. Levskaya, J. Bradbury, D. Andor, S. Narang, B. Lester, C. Gaffney, A. Mohiuddin, C. Hawthorne, A. Lewkowycz, A. Salcianu, M. van Zee, J. Austin, S. Goodman, L. B. Soares, H. Hu, S. Tsvyashchenko, A. Chowdhery, J. Bastings, J. Bulian, X. Garcia, J. Ni, A. Chen, K. Kenealy, K. Han, M. Casbon, J. H. Clark, S. Lee, D. Garrette, J. Lee-Thorp, C. Raffel, N. Shazeer, M. Ritter, M. Bosma, A. Passos, J. Maitin-Shepard, N. Fiedel, M. Omernick, B. Saeta, R. Sepassi, A. Spiridonov, J. Newlan, and A. Gesmundo. Scaling Up Models and Data with t5x and seqio. Journal of Machine Learning Research, 24(377):1–8, 2023. L. Xue, A. Barua, N. Constant, R. Al-Rfou, S. Narang, M. Kale, A. Roberts, and C. Raffel. ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models. Transactions of the Association for Computational Linguistics, 10:291–306, 2022. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research (JMLR 2020), 21(140):1–67, 2020. "},{"id":16,"href":"/docs/spring24/16_/","title":"16","section":"Spring24","content":" Accelerating Transformers via Conditional Computation: As Aspect of Mixture-of-Depths # Posted by: Inkwan Hwang, Minjae Park\nThis image was generated by DALL·E 3. Introduction # “Choice and concentration” is an effective strategy for achieving success in problems. Sometimes, it is not necessary to put the same amount of effort and time into all problems. Expending energy on trivial issues may fail to concentrate on what truly matters. Similarly, in language models, there is a technique that does not focus equally on all tokens but allocates less budget to non-essential tokens. This technique is called conditional computation.\nIn this post, We will explain conditional computation strategies for Transformers, focusing on a technology announced this year called Mixture-of-Depths.\npaper: Mixture-of-Depths: Dynamically allocating compute in transformer-based language models Let\u0026rsquo;s dive in!\nUnderstanding the problem: Uniform computation in Transformers # These days, most language models are based on Transformers, and we stack these blocks to make big models. When given an input sequence, tokens pass through these blocks to predict the next token. The problem is that the models spread computations uniformly across input sequences. Transformers use the same amount of computation for essential tokens as for non-essential ones. For instance, predicting a token within a sentence is cheaper than predicting the first token of the next sentence. Researchers want to address this issue by making Transformers focus on important tokens by allocating unimportant tokens with fewer computing resources.\nConditional computation for Transformers # Early exiting\nInstead of passing through all layers, the model can stop early if it is confident enough about its prediction. This saves computation time and resources. Large pre-trained models like BERT can use early exiting to maintain performance while reducing computing resources. CoLT5\nCoLT5 is an architecture allowing unnecessary tokens pass through light attention and light MLP. Light attention refers to a local attention layer that just calculates attention value between a few nearby tokens. Conversely, heavy Attention refers to a global attention layer that calculates some chosen token (chosen by router) and calculates attention values with all input tokens. It uses top-k routing mechanism that performs well (will be discussed in a later section). The figure above is the attention map in CoLT5. Light-colored ones indicate light attention(local attention) and bold ones indicate heavy attention. The model chooses 1/16 of query tokens and 1/8 of key tokens for heavy attention calculation. Mixture of Experts (MoE)\nMoE is a model consisting of parallel expert models which is fitted to certain domains. Token-level routing decisions are made across the network depths. Routing decision of the model determines which expert it will be sent to. Overview to Mixture-of-Depths (MoD) # Our goal is to reduce the overall FLOPs by focusing on essential tokens and relatively fewer non-essential tokens. The router is responsible for determining the path each token should take. A trained router evaluates whether the token is necessary. If the token is deemed essential, it passes through self-attention and the subsequent MLP (requiring FLOPs). Otherwise, it bypasses these stages via a residual connection (saving FLOPs).\nThe above image depicts the path of the model with an input sequence length of 64. The purple color shows the computation performed by that layer and the orange color shows the path taken by the residual connection. Routing schemes # Routing implementation is the most crucial part of MoD. Authors compare three routing schemes, demonstrating that MoD is the most efficient approach.\nToken-choice routing # Token-choice routing is a method where each token selects the path it will follow. The router produces probability distributions for each token across the computational paths. Based on this distribution, each token chooses its preferred path at each layer.\nIn this scheme, tokens have the flexibility to select their path, allowing for dynamic processing. However, this can lead to path-balancing issues as all tokens might prefer on the same path. It causes potential overloads on specific paths. To mitigate it, auxiliary loss is used to ensure that most tokens do not prefer a single path.\nExpert-choice routing # Expert-choice routing is the reverse version of token-choice routing. Similar to token-choice routing, the router produces a probability distribution for each token. In this scheme, instead of tokens selecting their paths, each path selects the top- \\(k\\) tokens based on the experts\u0026rsquo; preferences.\nUsing this method ensures that each path receives k tokens, maintaining balance among the paths. However, some tokens may not be selected because there might be common tokens that multiple paths prefer.\nExpert-choice MoD # This method is advantageous as it reduces the overall FLOPs in the model\u0026rsquo;s forward pass. When k is smaller than the input sequence length, some tokens do not need to undergo self-attention and MLP computations. For the left and middle approaches in the figure, selecting the top-k tokens may result in increased FLOPs since multiple experts need to perform computations.\nFor the following reasons, the authors decided to use expert-choice routing and utilize only single path:\nEfficiency of computation There is no need for an auxiliary balancing loss. Simplicity of implementation Tokens can be chosen with the highest output value of router in order. Clear criteria Top-k strategy can guarantee that the most important token is calculated since the top- \\(k\\) tokens are independent of the magnitude of router weights. Since tokens are divided into two sets, one passing through self-attention and MLP, and the other passing through residual connections, a strategy is needed to partition tokens into these two sets. Routing # \\(l\\) is a given layer. \\(S\\) is a sequence length. \\(\\beta=1-C/S\\) is an user-defined capacity per batch element. \\(f\\) comprises self-attention and subsequent MLP. \\[x^{l\u0026#43;1}_i=\\begin{cases}r^{l}_i f_i(\\tilde{X}^l)\u0026#43;x^{l}_i, \u0026amp; \\text{if } r^{l}_i \u0026gt; P_\\beta(R^l)\\\\x^{l}_i, \u0026amp; \\text{if }r^{l}_i \u0026lt; P_\\beta(R^l)\\end{cases}\\] Find the \\(\\beta\\) -th percentile ( \\(P_\\beta(R^l)\\) ) of the set of router weights \\(R^l\\) . If the router weight \\(r^l\\) is greater than \\(P_\\beta(R^l)\\) , perform self-attention and subsequent MLP computations. If it is less than \\(P_\\beta(R^l)\\) , pass through the token residual connection.\nImplementation # Capacity # In this paper, capacity-based routing is employed. Token capacity is the total proportion of tokens composing the input for a given operation. For instance, if the input sequence length is 100 and the capacity is 20%, each layer operates on the top-20 tokens determined by router weights.\nBy lowering the capacity of the computations, a smaller compute budget can be utilized per forward pass compared to the vanilla Transformers. In MoD, capacity is utilized as a hyperparameter to determine the proportion of tokens processed per layer operation.\nAutoregressively sampling # We\u0026rsquo;re looking to implement expert-choice routing, but there is one distinct problem: top-k operations rely on future tokens! Our goal is for each token to determine if it belongs to the top-k using routers. To do this, every token needs access to the router weights of future tokens. Unfortunately, we cannot predict the future router weights and cannot employ autoregressive sampling. To solve this problem, the authors propose two methods.\nSimple auxiliary loss\nDesigning an additional binary cross-entropy loss function at the router's output can resolve this issue. By incorporating this, the value of tokens in the top-k is guided to be greater than 0.5, while the value of tokens not in the top-k is guided to be less than 0.5. As token passes through the router, they are categorized into top-k set if their value exceeds 0.5. Then it passes through self-attention and subsequent MLP. Conversely, tokens with values below 0.5 pass through the residual connection. Integrating such a function impacts the primary language modeling objective by approximately 0.2-0.3%. We believe this likely refers to the extent to which performance and inference time are affected. Small auxiliary MLP predictor\nThe second method does not affect the primary language modeling objective at all. The authors design a new MLP layer that functions as a binary classifier to determine whether a token is in top-k during the training process. This classifier is trained to make these determinations, and it is used in real time during the autoregressive sampling process.\nThese methods, in autoregressive situation, could predict whether given token is important or not in real-time. They provide empirical results that auxiliary tasks achieved 99% accuracy.\nOpen source MoD (not official) # The following is an implementation of MoD that supports various LMs such as Mixtral, LLama3, and BLOOM. It implements MoD using PyTorch and Hugging Face Transformers library.\nLINK: https://github.com/astramind-ai/Mixture-of-depths\nResults # Hyperparameter tuning # The authors first trained the model with a limited FLOPs budget (6e18) to determine the optimal hyperparameters. Through training the MoD Transformer with routing blocks and self-attention blocks arranged alternately, they found the optimal parameters. The two top-middle graphs show the actual training loss graphs for the points plotted in the left graph. Among them, MoD with 12.5% capacity generally results in lower loss values than the baseline.\nComputation efficiency: In the right graph, the points #1, #3 and #2, #4 pairs are MoD models of the same parameter size. Not only does it have a lower loss value, but it also runs approximately 66% faster than the baseline. isoFLOP analysis # In this figure, the training FLOPs budget is limited to 6e18, 2e19, and 1e20 comparing isoFLOP baseline and 12.5% capacity MoD.\nTotal loss: The graph in the top-left corner shows that the isoFLOP baseline has a slightly better loss when the number of parameters is small (Note that there is a crossing point!). Normalized loss: When the x-axis is converted from parameters to FLOPs per FFW (Forward Pass) as shown in the top-right graph, MoD is better than the baseline in all cases. Auto-regressive evaluation # MoD variants were evaluated during auto-regressive sampling. Each model was tested on data comprising 256,000 sequences.\nPredictor accuracy: Using predictor-based methods is cheaper than top-k but not more accurate. In the left graph, the performance of the predictor strategy is almost indistinguishable from the top-k strategy. Authors attribute this to the ease of learning this prediction problem. Mixture-of-Depths-and-Experts (MoDE) # This figure shows the performance of MoDE and its two proposed structures. The top-left graph demonstrates that the performance of MoDE is better than both the Baseline and MoE. The right side explains the structures of Staged MoDE and Integrated MoDE. Staged MoDE: Two routers are deployed to first for determine the depth (MoD) and second for the expert (MoE). Integrated MoDE: The MoD router and MoE router are integrated into one single Router that can simultaneously decide whether to select an expert or the residual path (depth). The paper mentions that the former is computationally efficient as it can skip self-attention operations through the MoD router, and the latter has better performance as the router mechanism is unified and self-attention operations are always performed.\nConclusion and discussion # This paper insists that using MoD with a capacity 12.5% is better than the baseline transformer model.\nHowever, there are some unresolved limitations not discussed in the paper.\nOnly loss values: We believe this approach only indicates if parameters converge to the training dataset, not the model\u0026rsquo;s performance. To ensure MoD\u0026rsquo;s superiority over the baseline model, additional evaluation methods such as perplexity (WikiText-2, Lambada) and specific tasks (BoolQ, Hellaswag, etc.) should be included.\nMore experiments are needed: The paper only compares loss values for 12.5% and 50% capacity. They also applied MoD in one of two layers, but there are no comments on why applying this method. Further studies about using one of three or four should be done.\nMore baselines are needed: Further studies should provide validation of MoD method by comparing other methods like COLT5 or MoE and proof of optimal hyperparameters.\nReferences # Arian et.al., Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead , arXiv, 2021.\nJoshua et.al., COLT5: Faster Long-Range Transformers with Conditional Computation , EMNLP, 2023.\nNoam et.al., OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER , ICLR, 2017. AstraMind AI (2024). Unofficial implementation for the paper \u0026ldquo;Mixture-of-Depths\u0026rdquo;. https://github.com/astramind-ai/Mixture-of-depths.\n"},{"id":17,"href":"/docs/spring24/17_/","title":"17","section":"Spring24","content":" QuaRot : Outlier-Free 4-Bit Inference in Rotated LLMs # Author : Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li\nPosted by MyeongJi Yun, JungGyu Min, POSTECH\nThis post assumes that the reader has a structural understanding of Transformer and Llama models. If you need a detailed understanding of these models, please refer to the Transformer, LLaMa.\nLarge Language models ( LLMs ) like GPT-2, LLaMa have become increasingly important due to their countless applications. However, their inference requires a significant amount of computation, memory, and energy. Quantization is among the most important techniques to solve both memory and compute issues in LLM inference.\nOutlier makes quantization difficult # Recent research has shown that LLMs have large outliers and make quantization more difficult, especially in 4-bit case. Also, they mentioned that the activations have more outliers, which makes quantization harder. There are three main streams to solve this problem.\nWeight only quantization LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale, 2022 NeurIPs Weight quantization can ease the memory budget for saving the model. However, since activations are not quantized, the computation still involves integer and float operations, making it difficult to address compute issues. Remain outlier in higher bitwidth QUIK: Towards End-to-End 4-Bit Inference on Generative Large Language Models, 2023 Weight quantization can ease the memory budget for saving the model, and since most operations are integer X integer, compute issues are largely resolved. However, some operations still involve integer X float, and the occurrence of float values is irregular, leaving some compute issues unresolved. Use calibration set and normalize activation SmoothQuant: Accurate and Efficient Post-Training Quantization for LLM, 2023 ICML Accuracy is guaranteed up to 8-bit quantization, but it is not assured with 4-bit quantization. In “ QuaRot : Outlier-Free 4-Bit Inference in Rotated LLMs”, the author introduces a new method for quantizing LLM models end-to-end, by utilizing “computational invariance” to all weights and activation and optimizing the computing process.\nRandom Hadamard transform doesn’t change the result # In the concept of computational invariance theorem, small changes in input parameters do not cause the output difference if the algorithm is stable. When applying this to a transformer-based large language model (LLM), it implies that rotating the coordinate system of activations between weight and computation blocks using an orthogonal matrix does not alter the model\u0026rsquo;s output. According to this theory, instead of using any matrix X that constitutes the transformer, you can use X′=UXV where U and V are orthogonal matrices, and the computational results will remain unchanged.\nIf the number or proportion of outliers in 𝑋′ is less than that in 𝑋, the information loss during quantization can be reduced. QuIP demonstrates that multiplying a matrix by orthogonal matrices on both sides reduces the value of max⁡(𝑋)/mean(𝑋). This means that the presence of extreme values relative to the average is diminished, leading to a more uniform distribution of values within the matrix. However, performing 𝑈𝑋𝑉 also incurs overhead, so selecting orthogonal matrices 𝑈 and 𝑉 that minimize this overhead is essential.\nQuaRot uses Random Hadamard transformation because the result PPL is lower, so random Hadamard transformation is better than random matrix.\nLLama2-7B LLama2-7B LLama2-7B QuaRot ( Random ) 7.45 5.84 4.07 QuaRot (Hadamard) 6.10 5.40 3.79 Random Hadamard transformation matrix H is described below : \\[ H_{2} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{bmatrix}, \\quad H_{2^n} = H_2 \\otimes H_{2^{n-1}} \\] \\[H\u0026#39; = H \\cdot \\mathrm{diag}(s), \\quad s \\sim \\mathrm{Uniform}(\\{-1, \u0026#43;1\\})\\] This transformation pairs elements to perform simultaneous computations, allowing the matrix-vector multiplication between matrix 𝐻 and vector 𝑥 to be executed using only 𝑂(𝑑log⁡𝑑) addition operations without any multiplications, as illustrated below:\n--- QuaRot demonstrates that using this technique reduces the number of outliers. By applying the random Hadamard transformation, the distribution of activations is more uniform, which decreases the number of extreme values or outliers, thereby minimizing information loss during quantization.\nStep by Step modification and quantization # Step 1 involves applying the new schemes proposed by QuaRot to significantly reduce outliers in weights and activations, thereby minimizing accuracy degradation due to quantization held in Step 2. The key technique is to apply the Hadamard transform to each activation and weight in both attention blocks and FFN. This is done by merging operations through the use of two different Hadamard transform matrices across consecutive layers, creating an optimal computational flow.\nStep 1-a. Weight Modification # Note that the multiplication of two orthogonal matrices generates identical matrix, so inserting Q and Q^T between linear layers doesn’t change any output.\n\\[I = Q Q^T, XQQ^TW = XW\\] Considering LayerNorm or RMSNorm at the start of the transformer multiplying some orthogonal matrices does not change output. Also, we can fuse the scaling operation of RMSNorm’s : diag(a) into an adjacent weight matrix.\n\\[RMSNorm(X) = x_i \\leftarrow \\frac{x_i}{||x_i||} = ( \\frac{x_i * Q}{||x_i||} ) Q^T = RMSNorm (XQ^T)Q\\] So for all weights after the RMSNorm layer, the weight becomes :\n\\[W \\leftarrow Q^T diag(a) W, Q = Hdiag(s)\\] Step 1-b. Rotate FFN # Inserting online Hadamard operation can ease the activation value’s quantization difficulty within each block. This operation is implicitly reserved by fusing a Hadamard matrix into the next matrix of the network.\nStep 1-c. Attention Value Projection # This step applies Hadamard transformations to the value and output projection matrices in the attention block throughout both offline weight modification and online activation transformation. Since value and output projection weight are multiplied in each head, two matrices can be transformed using the Hadamard matrix without changing the result of attention.\n\\[W_v^{(h)} \\leftarrow W_v^{(h)}H_{d_h} \\\\ W_{out}^{(h)} \\leftarrow H_{d_h} W_{out}^{(h)} \\] This transformation can be represented with Kronecker multiplication in the point of full attention computation view.\n\\[W_v \\leftarrow W_v(I\\otimes H_{d_h})\\\\W_{out}\\leftarrow (I\\otimes H_{d_h}) W_{out} \\] The following simple lemma defines the remaining Hadamard operation after modification.\n\\[H_{a\\times b}= (I\\otimes H_{b}) (H_{a}\\otimes I )\\] This defines the remaining Hadamard operation as the later term of the upper lemma, which results in a modification of the online forward path.\n\\[Z \\leftarrow Z(H_{n_h} \\otimes I)\\] Step 1-d. Key Rotation # This step applies Hadamard transformation to the key vectors in the attention module. Utilizing the RoPE method (Su et al., 2021), the positional encoding is directly attended to query and key vectors. This reshapes the attention score computation equation into a modification-convenient form.\n\\[\\text{Score}=\\text{Softmax}(\\alpha \\text{Pos}(Q_h) \\text{Pos}(K_h^T)\\odot M)\\] The Hadamard transformation is applied to both position encoded query and key vectors similar to step 1-c.\n\\[\\text{Pos}(Q) = \\text{Pos}(XW_q) \\leftarrow \\text{Pos}(XW_q)(I\\otimes H_{d_h})\\\\\\text{Pos}(K) = \\text{Pos}(XW_k) \\leftarrow \\text{Pos}(XW_k)(I\\otimes H_{d_h}) \\] Note that this transformation can be applied without changing final attention scores since both queries and keys are rotated, therefore no remaining Hadamard transformation exists.\nStep 2 involves applying various state-of-the-art techniques to quantize weights and activations.\nStep 2-a. Weight Quantization # You can quantize the adjusted weights using GPTQ, or you can use a very simple round-to-nearest (RTN) technique. The paper have shown simpler method(RTN) have shown a slight sacrifice in accuracy.\nStep 2-b. Online Quantization # To quantize the activations, find the scale factor for each row (max(row) / 7), then divide all values by the scale factor and convert them to the nearest 4-bit integer. For dequantization, multiply the 32-bit integer output of GEMM by the scale factors of both the activation and the weight, and convert the result to FP16.\nStep 2-c. Quantized Attention # The significance of storing in 4-bit is greater than performing calculations in 4-bit because attention operations are memory-bound. Thus, to compute attention, keep the query, key, and value in FP16 and use Flash Attention for the softmax computation.\nQuaRot saves runtime \u0026amp; memory # As highlighted in the contributions of the paper, this model demonstrates that it maintains accuracy even with 4-bit quantization, achieving the same level of accuracy as other models with significant computation overhead. Additionally, this paper presents results across various model sizes(7B to 70B) and different tasks(PIQA(PQ), ARC-e(A-e), ARc-c(A-c), HellaSwag(HS), Winogrande(WG), LAMBADA(LA) ), demonstrating that as the model size increases, the quantization error compared to FP16 decreases for all tasks. Regarding the Llama-1-7B model\u0026rsquo;s 4-bit quantization situation, which exhibits the largest difference from the FP16 model, we compared it with other recent papers not mentioned in the original study. It is evident that QuaRot, which has lower computational cost, outperforms the generally best-performing QAT and OmniQuant, which involves some additional training on top of SmoothQuant, in 4-bit quantization. Despite this low cost, QuaRot has the smallest inference accuracy difference from the FP16 model, making it a highly effective quantization technique. Moreover, while the original SmoothQuant may have lower computational cost at the same bandwidth due to its simplicity, as shown in the table below, its inference accuracy in 4-bit quantization is so poor that it necessitates the use of 8-bit, making comparisons with QuaRot unnecessary. The key point of QuaRot is that the process of performing the Hadamard transform for quantization to INT4 should not introduce a large overhead compared to the computational benefits gained from converting to INT4. From the perspective of the runtime of the FFN block, it has been confirmed that the overhead remains minimal regardless of layer size, model size, or batch size. Additionally, the memory saving factor ranges from x3.48 to x3.71, which is very close to the ideal value (4 = FP16 / INT4), demonstrating significant efficiency. This paper is particularly noteworthy for addressing the issue of memory overhead in long sequence scenarios by quantizing the KV cache as well. Discussion and future work direction # Why we limited to symmetric INT4 qunatization?\nNumerous papers discuss the limitations of using symmetric quantization in INT4 format for quantization. For example, ANT demonstrate that, even with the same bitwidth, numeric formats like flint and PoT(power of Two), which divide the representation into exponent and mantissa, can achieve better accuracy due to their ability to represent a wider range of values. In the figure below, the INT-4bit example uses only integers, while the others utilize new data formats. It is evident that the Mean Squared Error (MSE) significantly decreases with these new formats.\nQuaRot considers INT4 format for both weight quantization and activation quantization, likely because modern GPUs support efficient operations with INT4 and INT8 formats. If we could use other formats, it might be possible to maintain accuracy even with formats as small as 3-bit, leading to greater memory savings. However, maintaining computational simplicity is challenging because GPUs are not optimized for operations with custom data types, unlike INT4. Therefore, achieving optimal computation with custom data types would require the development of custom hardware.\nToward Quantization + Pruning\nThere are two paper about low-cost LLMs GPTQ and OBS. GPTQ focuses on reconstructing matrices after quantization, while OBS deals with reconstructing models after pruning. Both papers share a common foundation in using the Hessian matrix and employ various optimization techniques such as Wood-Fisher. Combining these two approaches, the OBC study explores methods to preserve the accuracy of networks that undergo both pruning and quantization. SliceGPT similarly achieves effective pruning by employing the concept of computational invariance when multiplying orthogonal matrices. By analyzing the properties of orthogonal matrices in both QuaRot and SliceGPT, I believe it is possible to achieve quantization and pruning simultaneously. Nonlinear layer Quantization\nThis paper discusses performing quantization in an end-to-end manner. However, it lacks detailed explanations regarding operations in layers known to require higher bitwidth, such as the input to the softmax function, gelu, and residual operations in layer normalization. Therefore, future research could potentially extend this approach to include all these operations using only low-bitwidth integer calculations. How to reduce the overhead of online Hadamard transformation\nThe forward path in QuaRot mostly follows the activation-quantized LLM tasks like GPTQ, yet requires the additional task of online Hadamard transformation on attention activation. The online Hadamard transformation can be performed by utilizing existing computational resources by converting the task into a matrix-multiplication form, or tossing the task to a dedicated hardware accelerator. Either way have an optimization point of acceleration, where data scheduling of the Hadamard transformation matrix into GEMM task accelerator, or utilizing various previous works about hardware accelerator Hadamard transformation with dedicated dataflow. "},{"id":18,"href":"/docs/spring24/18_/","title":"18","section":"Spring24","content":" ViTAR: Vision Transformer with Any Resolution # Authors: Qihang Fan, Quanzeng You, Xiaotian Han, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang\nPosted by Jungwon Lee, Minsang Seok\nWhat is Vision Transformer? # Vision Transformer (ViT) is an innovative approach to computer vision that leverages the principles of the Transformer architecture, which was originally designed for natural language processing tasks. ViT has recently emerged as a competitive alternative to Convolutional Neural Networks (CNNs) that are currently state-of-the-art in different image recognition computer vision tasks.\nVision Transformer architecture consists of a series of Transformer blocks, each containing a multi-head self-attention layer and a feed-forward layer. This structure allows ViT to capture complex relationships within an image more effectively than traditional convolutional layers.\nKey Components of ViT # The key coomponents of ViT are described below:\nA. Patch Embedding # Instead of processing the entire image as a whole, ViT divides the input image into fixed-size patches (e.g., 16x16 pixels). Each patch is then flattened into a single vector, essentially treating each patch as a \u0026ldquo;token\u0026rdquo; similar to how words are treated in text processing. These flattened patch vectors are linearly projected to a desired embedding dimension. This projection helps in transforming the patches into a suitable format for the Transformer model. B. Positional Encoding # Since Transformers are permutation-invariant and do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings. These encodings provide information about the position of each patch in the original image. C. Self Attention # The self-attention layer calculates attention weights for each pixel in the image based on its relationship with all other pixels.\nFor each input vector X, three new vectors are created through learned linear transformations: Query (Q), Key (K), and Value (V), where \\(W_{Q}, W_{K}, W_{V}\\) are learnd weight matrices.\n\\[Q = XW_Q, K=XW_K, V=XW_V\\] The attention score for each pair of input vectors is calculated using the dot product of their Query and Key vectors: \\[Attention Score = Q K^T\\] These scores indicate how much focus the model should place on one part of the input when considering another part. \\[Attention Output = softmax(\\frac{Q \\dot K^T}{\\sqrt{d_k}}V)\\] The attention scores are scaled by the square root of the dimensionality of the Key vectors to prevent excessively large values that could destabilize training. The scaled attention scores are passed through a softmax function to obtain the attention weights. This ensures that the weights are normalized (summing to one) and highlight the relative importance of each input vector. Each input vector is then updated by computing a weighted sum of the Value vectors, using the attention weights. C. Multi-Head Self Attention (MHSA) # The multi-head attention extends self-attention mechanism by allowing the model to attend to different parts of the input sequence simultaneously. Each \u0026ldquo;head\u0026rdquo; in the multi-head attention mechanism can capture different features, leading to a richer and more nuanced representation of the image. D. Feedforward Neural Networks: # Each self-attention layer is followed by a feedforward neural network that further processes the information. These networks consist of fully connected layers and typically include activation functions and normalization. If interested in more details about ViT, please refer to the following paper. \u0026ldquo;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\u0026rdquo;\nChallenge: Multi-Resolution ViT Modeling # Shortcoming of ViT is revealed when receiving multi-resolution images as input. There are limits to its application in actual use environments because ViT cannot process images of various resolutions well.\nThe most common method used to address this problem is to apply interpolation to positional encoding before feeding it into the ViT. This approach allows for some compensation of positional information even when the input resolution changes. However, this method has shown significant performance degradation in image classification tasks.\nRecently, ResFormer proposed adding depth-wise convolution to the existing positional encoding method when performing global-local positional embedding, enabling it to work well even with unseen resolutions. (Chu et al., 2023; Tian et al., 2023).\nHowever, ResFormer has three drawbacks.\nShows high performance only in a relatively small range of resolutions (Degradation significantly when resolution is greater than 892) It cannot be used with self-supervised learning methods like masked auto-encoding (MAE). Computation cost increases as input resolution increases, which has a negative impact on the training and inference process. ViTAR: Vision Transformer with Any Resolution # To address this issue, ViTAR introduces two key innovations.\nAdaptive Token Merger : A novel module for dynamic resolution adjustment, designed with a single Transformer block to achieve highly efficient incremental token integration. Fuzzy positional encoding : A novel positional encoding to ensure consistent positional awareness across multiple resolutions, thereby preventing overfitting to any specific training resolution. 1. Adaptive Token Merger (ATM Module) # Adaptive Token Merger (ATM) module is designed to efficiently process and merge tokens of different resolutions in a neural network using a simple structure that includes GridAttention and FeedForward network (FFN). ATM Module takes tokens processed through patch embedding as input. ATM Module specially processes the inputs of different resolutions M times to reduce them to the same preset size \\(G_{h} \\times G_{w}\\) before fed into the MHSA.\nThe detailed process for ATM is as follows:\nFirst, ATM divides the tokens of shape \\((H \\times W)\\) into a grid of size \\(G_{th} \\times G_{tw}\\) .\nFor simplicity, we\u0026rsquo;ll use above Figure as an example. In the figure, we can see \\(H=4\\) , \\(W=4\\) , \\(G_{th}=2\\) , and \\(G_{tw}=2\\) .(We assume that H is divisible by \\(G_{th}=2\\) and W is divisible by \\(G_{tw}=2\\) . The number of tokens in each grid would then be \\(H/G_{th} × W/G_{tw}\\) , which is 2x2.\nWithin each grid, the module performs a special operation called Grid Attention.\nGridAttention # For a specific grid, we suppose its tokens are denoted as \\({x_{ij}}\\) , where \\(0 \\leq i \u0026lt; H/G_{th}\\) and \\(0 \\leq j \u0026lt; W/G_{tw}\\) .\nAverage Pooling: First, it averages the tokens within a grid to create a mean token. Cross-Attention: Using this mean token as the Query, and all the grid tokens as Key and Value, it applies cross-attention to merge all tokens in the grid into a single token. \\[x_{avg} = AvgPool(\\{x_{ij}\\})\\] \\[GridAttn(\\{x_{ij}\\}) = x_{avg} \u0026#43; Attn(x_{avg}, \\{x_{ij}\\}, \\{x_{ij}\\})\\] After passing through GridAttention, the fused token is fed into a standard Feed-Forward Network to complete channel fusion, thereby completing one iteration of merging token. GridAttention and FFN undergo multiple iterations and all iterations share the same weights.\nDuring these iterations, we gradually decrease the value of \\((G_{th} , G_{tw})\\) , until \\(G_{th} = G_{h}\\) and \\(G_{tw} = G_{w}\\) . (typically set \\(G_h = G_w = 14\\) , in standard ViT)\nThis iteration process effectively reduces the number of tokens even when the resolution of the image is large, and with enough iterations, this size can be reduced effectively. This has the advantage of being computationally efficient because when performing subsequent MHSA calculations, we always use the same size tokens as input, regardless of resolution.\nFor Ablation study, ViTAR-S Model is used to compare with AvgPool which is another token fusion method. The results of the comparison demonstrate that ATM significantly improves the model\u0026rsquo;s performance and resolution adaptability. Specifically, at a resolution of 4032, our proposed ATM achieves a 7.6% increase in accuracy compared with the baseline.\n2. Fuzzy Positional Encoding (FPE) # Existing ViT Models generally use learnable positional encoding or sin-cos positional encoding. However, they do not have the ability to handle various input resolutions because these methods are sensitive to input resolution. In response to this, ResFormer attempted to solve this problem through convolution-based positional embedding.\nHowever, convolution-based positional embedding is not suitable for use in self-supervised learning such as masked auto-encoding (MAE). This is because the method can extract and utilize the complete spatial feature only if it has all adjacent patches, but in the case of MAE, some of the image patches are masked. This makes it difficult for the model to conduct large-scale learning.\nFuzzy Positional Encoding(FPE) differs from the previously mentioned methods. It enhances the model\u0026rsquo;s resolution robustness without introducing specific spatial structures like convolutions. Therefore, it can be applied to self-supervised learning frameworks. This property enables ViTAR to be applied to large-scale, unlabeled training sets for training, aiming to obtain a more powerful vision foundation model.\nInitially, the learnable positional embedding is randomly initialized and used as the model\u0026rsquo;s positional embedding. At this time, FPE provides only fuzzy positional information and experiences changes within a certain range. Specifically, assuming that the exact coordinates of the target token are (i, j), the fuzzy positional information is (i + s1, j + s2). s1 and s2 satisfy -0.5 ≤ s1, s2 ≤ 0.5 and follows uniform distribution.\nDuring training, randomly generated coordinate offsets are added to the reference coordinates during the training process, and grid samples for learnable location embeddings are performed based on the newly generated coordinates to generate fuzzy location encoding.\nIn case of inference, precise positional encoding is used instead of FPE. When there is a change in input resolution, interpolation is performed on learnable positional embedding. This has strong positional resilience because it was somehow seen and used in the FPE used in the training phase.\nTo compare the impact of different positional encodings on the model’s resolution generalization ability, several positional encoding methods were used. This includes commonly used sin-cos absolute position encoding (APE), conditional position encoding (CPE), global-local positional encoding (GLPE) in ResFormer, Relative Positional Bias (RPB) in Swin, and FPE. Note that only APE and FPE are compatible with the MAE framework.ViTAR-S is used for experiments without MAE, and ViTAR-M is used for experiments with MAE. As a result, FPE exhibits a significantly pronounced advantage in resolution generalization capability. Additionally, under the MAE self-supervised learning framework, FPE also demonstrates superior performance relative to APE.\nViTAR shows superior performance with any resolution # Image Classification # ViTAR is trained on ImageNet-1K form scratch and it demonstrates excellent classification accuracy across a considerable range of resolutions. Especially, when the resolution of the input image exceeds 2240, ViTAR is capable of inference at lower computational cost. In contrast, traditional ViT architectures (DeiT and ResFormer) cannot perform high resolution inference due to computational resource limitations.\nAs can be seen in the pareto frontier figure, ViTAR has high performance for various resolution images and can also be used for high resolution images of 2240 or higher.\nObject Detection # For object detection, COCO dataset is used ATM iterates only once because it does not utilize the multi-resolution training strategy in this experiment. If \\(\\frac{H}{G_{th}}\\) and \\(\\frac{w}{G_{tw}}\\) in ATM are fixed to 1, the results indicate that ViTAR achieves performance in both object detection and instance segmentation. And if setting \\(\\frac{H}{G_{th}}\\) and \\(\\frac{w}{G_{tw}}\\) to 2 in ATM, ATM module reduces approximately 50% of the computational cost while maintaining high precision in dense predictions, demonstrating its effectiveness.\nDiscussion # Applicability to Diffusion Models # It is currently challenging to generate images of various resolutions with generative models like Diffusion Models. Additionally, many diffusion models with ViT structures have been proposed recently (e.g. DiT, PixArt-α, Sora). Can the proposed method be applied to Diffusion Models as well? However, one consideration for applying it to diffusion models is how to effectively upscale the reduced size obtained through Grid Attention to ensure that the input and output sizes are the same. Applicability to Large Language Models (LLMs) # In LLMs, when receiving long context as input, positional embeddings are sometimes added using interpolation like this case. Would applying Fuzzy Positional Embedding (FPE) help handle long context inputs better? Or, just like training a network on low-resolution images to perform well on high-resolution images, can a network trained on short context in LLM maintain good performance on long context input? Can Grid Attention Replace Convolution? # The operation of GridAttention is quite similar to the process performed by kernels in Convolution when calculating each grid. However, ATM maintains parameter efficiency by sharing weights. We expect that applying GridAttention to existing CNN structures (e.g., VGG, ResNet) will allow us to design more efficient architectures. "},{"id":19,"href":"/docs/spring24/19_/","title":"19","section":"Spring24","content":" # "},{"id":20,"href":"/docs/spring24/20_/","title":"20","section":"Spring24","content":" Evolutionary Optimization of Model Merging Recipes # Authors: Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, David Ha., arXiv 2024\nPosted by Jaehyun Pahk, Youngkil Song\nBackground # Evolutionary Algorithm # The evolutionary algorithm is a type of black box optimization that can achieve the desired optimization without directly computing the gradient ▽f(x) or the Hessian ▽²f(x) of the function. Evolutionary algorithms are typically composed of the following elements:\nPopulation: A set of possible solutions to the problem, where each individual represents a candidate solution. Fitness Function: A function that evaluates the quality of each individual. The fitness function reflects the objective of the problem, with higher values indicating better solutions. Selection: The process of selecting individuals to be passed on to the next generation. Individuals with higher fitness are more likely to be selected. Termination Condition: The criteria for ending the algorithm. This is usually based on a maximum number of generations, achieving a target fitness level, time limits, or other benchmarks. Simple Evolutionary Strategy (Simple ES) # Sample a set of solutions from a Normal distribution with mean 𝜇 and a fixed standard deviation 𝜎. Evaluate the fitness of each solution using the fitness function. Set 𝜇 to the best solution in the population, and sample the next generation of solutions around this new mean. Repeat the above processes. Covariance-Matrix Adaptation Evolution Strategy (CMA-ES) # CMA-ES is an evolutionary strategy that can dynamically adjust the search range for solutions. CMA-ES finds the global optimum effectively even in high-dimensional problems by adaptively updating the covariance matrix of the multivariate normal distribution used for sampling a set of solutions, thereby adjusting the search direction and range. Fig. 1 from article shows a simple CMA-ES simulation in a 2D space.\nFigure 1. A 2D Simulation of CMA-ES Algorithm. How to leverage the strengths of multiple pre-trained models? # Fine-Tuning # Fine-tuning involves taking a pre-trained model and further training it on a specific dataset to optimize its performance for a particular task.\nBase Model → Task-Specific Model Example: Fine-tuning a general language model on a medical dataset to create a medical chatbot. Advantages\nSpecialized Performance: Highly effective for optimizing models for specific tasks. Efficiency: Requires less computational power and time compared to training from scratch. Flexibility: Can be applied to any pre-trained model to fine-tune it for different tasks. Disadvantages\nOverfitting: Risk of overfitting to the small dataset used for fine-tuning. Limited Generalization: May not perform well on tasks outside the fine-tuned domain. Dependent on Pre-trained Model: The performance heavily relies on the quality of the pre-trained model. Model Merging # Model merging involves combining multiple pre-trained models into a single model by integrating their weights and architectures to leverage their collective strengths.\nModel A (e.g., Japanese LLM) + Model B (e.g., Math LLM) → Merged Model Example: Combining a Japanese language model with a mathematical reasoning model to create a Japanese math reasoning model. Advantages\nCost-Effective: Does not require additional training, making it computationally efficient. Enhanced Capabilities: Can combine strengths from different models, potentially handling a broader range of tasks. Cross-Domain Application: Effective in creating models that perform well across different domains (e.g., language and vision). Disadvantages\nComplexity: The merging process can be complex and requires careful selection and comparison of models. Black-Box Nature: May be seen as less interpretable since it relies on heuristic methods for weight integration. Potential for Suboptimal Performance: If not done correctly, merged models may not achieve the desired performance improvements. Figure 2. Example of Model Merging. Fig. 2 from (Xu et al., CVPR 2024) illustrates an example of model merging. This approach involves pair-wise comparison of weights from two pre-trained models and merging the most similar weights together.\nMerging Language Models # Research on applying model merging to language models is actively progressing, with a large number of capable merged models being developed by the community. As a result, most of the top models on the Open LLM Leaderboard are increasingly dominated by merged models produced by language model enthusiasts.\nMergekit github\nMergekit is a toolkit that provides various popular recipes for merging language models. It includes both simple methods like linear and spherical interpolation as well as more advanced techniques.\nAdvanced Merging Methods\nTask Arithmetic: Involves creating task vectors by subtracting the weights of a pre-trained model from a fine-tuned model and then manipulating these vectors to steer the merged model’s behavior. TIES-Merging: Addresses parameter interference by resetting minimal parameter changes, resolving sign conflicts, and merging only aligned parameters. This approach aims to mitigate information loss during the merging process. DARE (Differentiable Adaptation and Regularization of Ensembles): Amplifies significant differences between models while zeroing out small differences. Often used in conjunction with Task Arithmetic or TIES-Merging to improve merging performance. Contributions # Automated Model Composition: Developed an evolutionary method to automatically discover optimal combinations of diverse open-source models, creating powerful models without extensive training data or compute resources. Cross-Domain Merging: Demonstrated the ability to merge models from different domains (e.g., language and Math, language and Vision) to achieve enhanced capabilities beyond conventional design. State-of-the-Art Performance: Achieved state-of-the-art results with automatically generated models, including a Japanese LLM with Math reasoning capability and a culturally-aware Japanese VLM. High Efficiency and Surprising Generalizability: Showed that a 7B parameter model outperformed some 70B parameter models, highlighting efficiency and generalization capabilities. Culturally-Aware VLM: Produced a Japanese VLM that excels in handling Japanese culture-specific content, achieving top results on relevant benchmarks. Method # Explanation of overall method # The goal of this paper is to develop a unified framework that can automatically generate a merged model from a set of foundation models, ensuring that the merged model outperforms an any single model in the collection. In this paper, an evolutionary algorithm was applied to reduce the complexity of the model merge process. The model merge was applied independently and also sequentially in both parameter space and the data flow space.\nMerging in the Parameter Space (PS) # The model merge in the parameter space can be summarized as a weighted average of the model parameters. In this paper, the fitness of each foundation model for a specific task is determined using the task vector of each foundation model, and then merging configuration parameters for combining the parameters of the candidate models are estimated based on those fitness values. Specifically, this paper enhances TIES-Merging with DARE, allowing for more granular, layer-wise (input/output embedding layers or transformer blocks) merging. Fig. 3 of Sakana.ai shows an overview of the PS merging.\nFigure 3. Model Merging in the Parameter Space. Merging in the Data Flow Space (DFS) # In DFS, the proposed framework discovers the best combinations of the layers of different models to form a new model, without changing the model parameters. In other words, the goal of merging in the DFS is to find the optimal inference path across the multiple models. For example, after the i-th layer in model A, a token may be directed to the j-th layer in model B. Fig. 4 of Sakana.ai shows an overview of the DFS merging.\nFigure 4. Model Merging in the Data Flow Space. Please note that the search space in the data flow space (DFS) is very large. Assuming the total number of layers across all models is $M$ and the lengh of the inference path is $T$, then the size of the search space is $M^T$. This astronomically large search space leads to a challenge for a evolutionary search algorithm, even with a modest configuration of $M=64$ and $T=60$. To address this issue, this paper exploits the result of preliminary studies that certain layer arrangements, particularly repetitive or permuted sequences from earlier in the model, can adversely affect performance. Specifically, this paper layout all the layers only in sequential order (i.e., all layers in the $i$-th model followed by those in the $i+1$-th model) and repeat them $r$ times, therefore the size of the search space can be reduced to $2^{M \\times r}$. The authors use indicator array $\\mathcal{I} \\in \\mathbb{R}^{M \\times r}$ to represent which layers are included and excluded. However, in the above setting, a layer may face an input whose distribution is different from what it is used to (from its original model), leading to unexpected outputs. They just apply scaling the input based on the scaling matrix $W \\in \\mathbb{R}^{M \\times M}$, which is also optimized by the evolutionary search together with the indicator array $\\mathcal{I}$.\nMerging in Both Spaces # Model merging in the parameter space (PS) and data flow space (DFS) can be applied orthogonally to boost the performance of the merged model. Specifically, in this paper, model merging is first applied in the PS to generate several merged models, which are then put back to the collection of models. The expanded collection is subsequently used for merging in the DFS. Fig. 5 of Sakana.ai shows an overview of the overall method.\nFigure 5. Overall Method. Experiments # The experiments in the paper focus on applying the proposed evolutionary model merging approach to create advanced models in two primary areas: Japanese LLMs with Math reasoning capabilities and culturally-aware Japanese Vision-Language Models (VLMs).\nEvolving Japanese Math LLM # Setup # Source Models: Japanese LLM: shisa-gamma-7b-v1 Math LLMs: WizardMath-7B-V1.1, Abel-7B-002 Dataset: Training: 1069 translated samples from GSM8k test set Testing: 250 samples from the Japanese test set of MGSM Evaluation: Accuracy measured by the correctness of the numerical value and reasoning text in Japanese. Used fasttext for language detection and greedy sampling for generation. Optimization: Parameter Space (PS): Used CMA-ES algorithm implemented in Optuna for optimization. Data Flow Space (DFS): Limited to two models with a budget of T = 192 steps, using CMA-ES in EvoJAX for optimization. Table 1. Performance Comparison of the LLMs on both MGSM-JA and JP-LMEH benchmarks. Table 2. Breakdown of JP-LMEH Scores for Japanese Language Proficiency. Results # Performance: Merged models in PS and DFS showed substantial performance improvements, with the hybrid model (PS+DFS) achieving the highest accuracy. The PS merged model (Model 4) scored 52.0 on MGSM-JA, while the hybrid model (Model 6) scored 55.2 in Table 1. The PS merged model (Model 4) scored 70.5 on JP-LMEH, while the hybrid model (Model 6) scored 66.2 in Table 1. Analysis: While they validate the effectiveness of the evolutionary model merging approach, it is challenging to determine the superiority of the PS, DFS, and merged model methods. Notably, as shown in Table 2, the PS method exhibits the highest accuracy on the JP-LMEH benchmark. Evolving Japanese VLM # Setup # Source Models: Japanese LLM: shisa-gamma-7b-v1 VLM: LLaVA-1.6-Mistral-7B Dataset: JA-VG-VQA-500: 500 samples from the Japanese Visual Genome VQA dataset. JA-VLM-Bench-In-the-Wild: 42 images with 50 questions, focusing on Japanese cultural elements. Evaluation: Baselines: LLaVA-1.6-Mistral-7B and Japanese Stable VLM. ROUGE-L score used for evaluation, with non-Japanese responses replaced by empty texts. Table 3. Performance Comparison of the VLMs. Results # Performance: As shown in Table 3, the merged VLM outperformed baselines on both benchmarks, scoring 19.7 on JA-VG-VQA-500 and 51.2 on JA-VLM-Bench-In-the-Wild. Qualitative Analysis: The merged VLM demonstrated superior handling of Japanese cultural content, providing more detailed and accurate responses compared to baseline models. Conclusion # The evolutionary model merging method, which operates in both parameter space (PS) and data flow space (DFS), challenges the conventional paradigm of expensive model development, offering a more efficient alternative that can produce competitive models without relying on gradient-based training. This paper is significant in that it attempts to automate the merging of LLM foudation models based on an evolutionary algorithm, which previously relied on human intuition.\nHowever, There are limitations to discuss.\nLimitation # Contradiction in Automation: Although the method aims to automate the merging process by removing prior information and heuristic components, it still relies on experimental experience for design and learning methods, which is contradictory to the initial motivation. Complexity and Efficiency of the Model: By applying the PS merging and the DFS merging orthogonally and sequentially, the temporal and spatial complexity for model merging seems to have increased significantly. Effectiveness of applying both merging methods: We also observe in the experiments that the performance of the merged model is better when PS merging is applied alone than when both PS merging and DFS merging are applied, which raises the question of whether applying both merging methods together is worth the increased complexity of merging. Limited Language Scope: The paper presents experimental results only for Japanese (JP), which is not convincing for demonstrating the method\u0026rsquo;s superiority in non-English languages. Lack of Comparative Experiments: There is a lack of experimental comparison with various other methods that use pre-trained models for multiple tasks such as fine-tuning. Future Work # Figure 6. Evolved Configurations for DFS Merging of models. Fig. 6 shows that the initial inference steps are not used in the model merging process. This indicates that DFS relies heavily on prior knowledge to enhance the performance of the merged model. Furthermore, the experimental results demonstrate that DFS has less impact on performance improvement compared to PS and is not fully optimized. Therefore, research focusing on automating and optimizing DFS could make it a powerful tool for model merging.\nQualitative Results # Case Study of EvoLLM-JP-v1-7B # Case Study of EvoVLM-JP # References # The Open LLM Leaderboard https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\nMergekit https://github.com/arcee-ai/mergekit\nHa, David., A Visual Guide to Evolution Strategies. 2017.\nZhengqi Xu, Ke Yuan, Huiqiong Wang, Yong Wang, Mingli Song, Jie Song., Training-Free Pretrained Model Merging. CVPR., 2024.\nSakana.ai., Evolving New Foundation Models: Unleashing the Power of Automating Model Development., 2024.\n"},{"id":21,"href":"/docs/spring24/21_/","title":"21","section":"Spring24","content":" A Unified Framework for Model Editing # Authors: Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli\nPosted by Donggeun An, Jonghyun Chae\nIntroduction # In the rapidly evolving field of artificial intelligence and machine learning, keeping large language models (LLMs) up-to-date with the latest information is crucial. This paper presents a comprehensive framework, Equality-constrained Mass Model Editing for Transformers (EMMET), that integrates two major model editing techniques: Rank-One Model Editing (ROME) and Mass Editing Memory in Transformer (MEMIT). The proposed framework focuses on the retention-memory objective, which aims to inject new knowledge into the model while maintaining the fidelity of existing information.\nFigure 1. A diagrammatic representation of the preservation-memorization objective. Step 1: Find Keys to Preserve Identify key vectors \\(k_0\\) representing existing knowledge, ensuring they remain intact by processing with the weight matrix \\(W_0\\) to produce output vectors \\(v_0\\) . Step 2: Find a Fact to Memorize\nLocate new information to be added, represented by key vector \\(k_e\\) and output vector \\(v_e\\) , ensuring the model generates the correct new fact. Step 3: Update Weight Matrix\nModify \\(W_0\\) to \\(\\hat{W}\\) , preserving existing key vectors \\(k_0\\) while ensuring \\(k_e\\) produces \\(v_e\\) , thus integrating the new information accurately. Model Editing Evaluation Metrics # The success of model editing is measured using standard metrics.\nEfficacy Score (ES): Indicates if an edit has been successfully made to a model. It is measured as the percentage of edits where the probability of the new fact is greater than the probability of the old fact for a given query prompt. Paraphrase Score (PS): Represents the generalization ability of the model under an edit. It measures the percentage of edits where the probability of the new fact is greater than the probability of the old fact for paraphrases of the query prompt. Neighborhood Score (NS): Represents the locality of model editing. It measures whether editing a fact affects other facts stored inside a model. NS indicates the percentage of facts in the neighborhood of the edited fact that remain unaltered post-edit. Generation Entropy (GE): Represents the fluency of a model post-edit. It is calculated by measuring the weighted average of bi-gram and tri-gram entropies of text generated by an edited model. This value drops if the generated text is repetitive, a common failure case of model editing. Score (S): A composite metric defined to represent a combination of edit success, generalization, and locality. It is the harmonic mean of ES, PS, and NS. ROME and MEMIT: Overview # Figure 2: Figure shows a diagrammatic representation of a transformer layer. The layer being edited by ROME, MEMIT and EMMET is the projection weight matrix inside the MLP layer ( \\(W_{proj}\\) ). To further understand how model editing techniques like ROME, MEMIT, and EMMET work, it's essential to look at how they interact with the layers of a transformer model. Input Representation ( \\(h_{l-1}\\) ): The input to the transformer layer, \\(h_{l-1}\\) , is either the output from the previous layer or the initial input embedding. Attention Mechanism (Attn): The input \\(h_{l-1}\\) passes through the attention mechanism, which calculates attention scores and generates a context vector by attending to different parts of the input sequence. Feed-Forward Layer: The transformed input then goes through the feed-forward layer, consisting of a fully connected layer ( \\(W_{fc}\\) ) producing an intermediate representation, followed by a non-linear activation ( \\(\\sigma\\) ) like ReLU or GELU. Key Vector Generation ( \\(k\\) ): After the non-linearity, the intermediate representation is used to generate key vectors \\(k\\) , crucial for storing and retrieving the model\u0026rsquo;s knowledge. Projection Weight Matrix ( \\(W_{proj}\\) ): The projection weight matrix \\(W_{proj}\\) projects the key vectors into the final output space and is the focus of edits in ROME, MEMIT, and EMMET. Output Vector Generation ( \\(v\\) ): The projection weight matrix \\(W_{proj}\\) transforms the key vectors \\(k\\) into output vectors \\(v\\) , integrating the edits made to the model. Layer Output ( \\(h_{l}\\) ): The final output of the transformer layer, \\(h_{l}\\) , serves as the input for the next layer or as the model\u0026rsquo;s final output if it is the last layer. ROME (Rank-One Model Editing) # ROME is a method that facilitates direct modification of model parameters to incorporate new factual knowledge or modify existing information. ROME works by enforcing equality constraints that ensure precise alignment between the output of the updated model and the intended new knowledge. This method uses first-order updates to model parameters. This is expressed mathematically as adding a single outer product of the two vectors to the existing weight matrix. This approach is highly targeted, modifying the weights in a way that exactly matches the new facts with minimal changes elsewhere, making it ideal for precision-critical applications. ROME is effective for single edits or small batches, but because the method strictly adheres to equality constraints, it does not scale well for large edits, potentially leading to inefficiencies or long computation times in batch scenarios.\nMEMIT (Mass Editing Memory in Transformer) # MEMIT is designed for batch updates and is known for its flexibility and scalability in model editing tasks. Unlike ROME, MEMIT uses a least-squares constraint that provides more flexibility in how edits are implemented. This method optimizes a relaxed objective where the goal is to minimize the overall error across a batch of edits rather than achieving exact matches for each individual update. MEMIT’s strength lies in its ability to handle large batches of edits simultaneously, making it particularly useful for applications that require frequent and extensive updates to the stored knowledge. The algorithm adjusts the model\u0026rsquo;s parameters by calculating a closed-form solution that distributes the edits across the parameters in a way that balances the introduction of new facts with the preservation of existing knowledge.\nTable 1. Comparison between ROME and MEMIT when editing only a single layer for CounterFact dataset. The comparison between ROME and MEMIT reveals that both techniques are highly effective at model editing, with each having its strengths. ROME generally excels in generalization and efficacy, while MEMIT performs slightly better in maintaining locality and fluency, especially for larger models like Llama-2. EMMET (Unifying ROME and MEMIT) # Introducing EMMET # EMMET unifies ROME and MEMIT under the preservation-memorization objective. EMMET uses an equality constraint for batched edits, providing a balanced approach that leverages the strengths of both ROME and MEMIT.\nEMMET\u0026rsquo;s Closed-Form Solution # EMMET uses a closed-form solution to implement the equality constraints across batch edits. This solution involves modifying the weight matrix of a transformer model in such a way that the edits are distributed across the parameters efficiently, ensuring that each targeted update is reflected accurately in the model\u0026rsquo;s output. The key formula for EMMET’s update is given by: \\[\\Delta = (V_E-W_0K_E)(K_E^TC_0^{-1}K_E)^{-1}K_E^TC_0^{-1}\\] Here \\(V_E\\) represents the vector of desired outputs, \\(W_0\\) is the original weight matrix, \\(K_E\\) is the key vector representing the input associated with each fact, and \\(C_0\\) is the covariance matrix derived from the existing model parameters. EMMET operates under the preservation-memorization objective, which aims to preserve the integrity of the model’s existing knowledge while accurately incorporating new information. The algorithm is carefully designed to balance these objectives, ensuring that the updates enhance the model\u0026rsquo;s utility without introducing errors or biases. EMMET is designed to incorporate batch edits under equality constraints. This approach is similar to ROME\u0026rsquo;s method of applying precise updates but is scaled to handle multiple edits simultaneously. EMMET ensures that each edit precisely matches the desired update without adversely affecting the existing knowledge encoded in the model. One of the features of EMMET is its ability to perform large-scale batch edits, which can include up to 10,000 edits in a single batch. This is a significant enhancement over traditional methods that typically handle edits one at a time or in smaller batches. EMMET’s batch processing capability makes it particularly valuable for applications requiring frequent and extensive updates to model data.\nExperiments and Results # Figure 3. Single layer editing performance of EMMET as a function of batch size when compared to MEMIT on the CounterFact dataset. Figure 4. Performance comparison of EMMET and MEMIT when distributing the edit over multiple layers using the MEMIT edit-distribution algorithm on the CounterFact dataset. The effectiveness of EMMET has been validated through extensive testing on standard model compilation datasets, including evaluations on various models such as GPT2-XL, GPT-J, and Llama-2-7b. These experiments demonstrated that EMMET matches and sometimes exceeds the performance of MEMIT in terms of editing success rate, maintaining data integrity, and generalization ability across multiple datasets and model architectures. Conclusion # This unified approach allows for a comprehensive comparison of the two methods, showing they can optimize similar objectives through different constraints. The introduction of EMMET, a new algorithm for batch editing under equality constraints, demonstrates the ability to handle large updates efficiently, maintaining performance on par with existing methods. The paper confirms the robustness of these techniques through extensive empirical testing and establishes a solid theoretical foundation for understanding model editing dynamics.\n"},{"id":22,"href":"/docs/spring24/22_/","title":"22","section":"Spring24","content":" Larimar: Large Language Models with Episodic Memory Control # Posted by: Sunggyu Jang, Hyeonwoo Park\nAuthors: Payel Das (IBM AI Research), Subhajit Chaudhury (IBM AI Research) et.al\n1. Background # Large Language Model (LLM) is one of the most popular topics in these days, due to their outstanding performance on various Natural Language Processing (NLP) tasks. However, LLM has faced a lot of challenges at the same time. In this report, we especially focus on the \u0026ldquo;knowledge edit\u0026rdquo; problem.\nKnowledge edit in LLM research # Knowledge edit problem can be summarized as \u0026ldquo;constantly updating the knowledge of pre-trained LLMs to keep models fact-relevant, safe, and ethical after deployment.\u0026rdquo; [1] The point is that, we have to update the knowledge on the pre-trained model accurately and quickly. Figures below illustrate why do we need knowledge update.\nTo update new knowledge To mitigate context length generalization problem To erase sensitive data Fig1. Knowledge update: New knowledge should be injected constantly [2] Fig2. Context length generalization: The ability to quickly update the LLM can help with \"input context length generalization problem\" [3] Fig3. Selective fact forgetting: LLMs should forget personal \u0026 sensitive data [4] Memory network # However, knowledge edit is not so simple as it sounds. Pre-training LLMs requires substantial computational cost due to thier unprecedented amounts of parameters. Considering the fact that we have to introduce new knowledge into the pre-trained model frequently, re-training the whole model is not a feasible solution [2].\nTo tackle the problem, \u0026ldquo;memory network\u0026rdquo; was proposed. The main point of memory network is \u0026ldquo;to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to.\u0026rdquo; [5]\nFor example, let\u0026rsquo;s assume that you\u0026rsquo;re providing new information to a pre-trained LLM. What you expect to the model is to answer the following questions based on the facts you mentioned. In this case, the model can do the job by writing the knowledge from you into a memory and reading the relevant one from the memory to answer the question. This problem is called as \u0026ldquo;Question Answering (QA).\u0026rdquo;\nFig4. Example of QA [5] Variational auto encoder (VAE) # To implement the idea of memory network, concepts from variational auto encoder are usually used. VAE is a kind of generative model to generate an output similar to real data. To be specific, it aims to approximate the true distribution of input data with three components - encoder, decoder, and latent space.\nIn this post, we assume that readers have knowledge about VAE. For details, please refer to [6] and [7].\nFig5. VAE Structure [7] Neocortex-Hippocampus interactions # This paper imitates the role of brain. Humans can rapidly update their knowledge after encountering the first relevant instance. In the brain, this process is facilitated through interactions between the neocortex and the hippocampus. The hippocampus is the site for storing long-term memories, while the neocortex integrates long-term and short-term memories to relay the results to the body.\nFig6. Neocortex and the Hippocampus The Complementary Learning Systems (CLS) theory proposes a model that combines these complementary learning systems of the hippocampus and neocortex. The interaction between the neocortex and hippocampus in the brain is known to promote adaptive behavior through memorization and generalization. Furthermore, it is suggested that memory consolidation from the hippocampus to the neocortex is facilitated by the activation synchronized with multiple exact or false replays of the encoded experience in the hippocampus. This implies that the hippocampus functions as a generative associative network. 2. Contributions # Larimar introduces a class of memory-conditioned language models inspired by complementary learning mechanisms in the brain. This architecture facilitates real-time test-time adaptation without requiring time-intensive gradient-based learning or internal fact tracing, offering a faster method for updating LLMs. Utility Demonstration in Knowledge Editing and Context Generalization:\nThe proposed method is demonstrated on two significant and challenging use cases: knowledge editing and generalizing to longer input contexts. Larimar exhibits fast and accurate training-free adaptation to new inputs in both scenarios, outperforming baseline editing methods and existing language models. Selective Fact Forgetting and Information Leakage Prevention:\nLarimar effectively supports selective fact forgetting and prevents information leakage using its one-shot memory updating mechanism. Recursive Search-Based Solution for Long Context Generalization: A simple recursive search-based approach is provided to enable Larimar\u0026rsquo;s memory to generalize to longer input contexts.\n3. Model architecture [1] # Inspired by human brain (neocortex-hippocampus interactions), authors suggest \u0026ldquo;a class of LLMs augmented with an external episodic memory controller.\u0026rdquo; They utilize an episodic memory to mimic hippocampal fast-learning system, and use LLM as a neocortical slow learning system.\nFig7 below shows the overall architecture of Larimar. Basic idea is to implement VAE with external memory. It consists of three main components: encoder, decoder, and adaptive memory. Comparing the architecture with Fig5 would be helpful. In Larimar, memory corresponds to a latent vector.\nEncoder: Transforms the input into a latent vector Decoder: Generates an answer to the question conditioned on the memory Memory: Stores episodes in encoded form Fig7. Larimar architecture Let\u0026rsquo;s see how it works in two stages.\n3-1. Training # (1) Writing # The memory M in Fig7 has to be trained so as to approximate the distribution of X (X is an exchangeable-order invariant episode: X = { $x_{1}$, \u0026hellip;, $x_{N}$ }, a subset of the input data consisting of N samples). To do so, the model is trained to maximize the conditional log-likelihood of lnp (X|M). In this way, the model learns to compress X in a memory M, which then becomes a distributed associative memory. This process is similar to that of encoder in VAE. (Look at the green arrows in Fig7)\n(2) Reading # The reading weight matrix, W, is a random variable for generative ability of the model. In this paper, authors set a standard Gaussian prior p(W) ~ N(0, $I_{N \\times K}$ ) and posterior q(W) ~ N($\\bar{W}$, $\\sigma^2_{W} \\cdot I_{N \\times K}$), where the mean $\\bar{W}$ is estimated from each episode and $\\sigma_{W}$ is learnable. Memory readouts are obtained as Z$_{readout}$ = WM.\n(3) Summary # Three main components - encoder(e), associative memory(M), and decoder(d) - are jointly trained and optimized for an episode X, using the following loss:\n3-2. Inference # Once M$_{0}$ is trained via backpropagation, the posterior memory M is updated in one-shot by solving a minimization problem below. This problem can be efficiently done with the pseudo-inverse of matrix. For more details, please refer to [8].\n4. Memory Operations [1] # In this paper, authors followed the ideas from [8] to combine pre-trained LLMs with memory component for knowledge edit. Fig8 illustrates the single training step of the memory.\nFig8. Basic memory operations [8] On top of that, sequential writing and forgetting is conducted as follows.\nFirst, given an initial set of encodings $Z_{0}$ and writing weights $W_{0}$, memory matrix and key covariance matrix are initialized as below.\nNext, memory $M_{i-1}$ is sequentially updated by adding a new set of encodings $Z_{i}$ or forgetting a previously written set of encodings $Z_{i}$. This process is conducted as below.\nFor instance, $\\alpha_{i}$ is 1 for writing, -1 for forgetting.\n5. Results # Wall Clock time # Fig9. Comparison between different editing methods and the wall clock time for a single edit The experiment was conducted on a single A100 GPU. Comparing the wall clock time for each editing method across 10 single edits, as shown in Fig8, Larimar was found to be approximately 4-10 times faster than the existing ROME and GRACE methods. Additionally, Larimar demonstrates the ability to address sequential edits, batch edits, and forgetting/deletion, which were not previously addressed in existing work. Single Fact Editing # This paper utilizes the CounterFact dataset for comparing Single Fact editing. The CounterFact dataset is designed to test the language model\u0026rsquo;s ability to handle counterfactual edits. It evaluates whether the model accurately learns new facts. It contains a total of 21,919 data points, and the evaluation is conducted using the first 2000 samples. In contrast to training the LLM on edits or causally tracing the original fact within the LLM and updating the relevant parameters, Larimar leverages one-shot memory update for editing. In this approach, the memory posterior is updated as the edit(s) of interest is written, and then the updated memory is queried. The read-out from the memory conditions the decoder to output the edit.\nFig10.Single fact edit performanceon CounterFact dataset comparing with baseline. Top two best systems are highlighted. The results are shown in Fig 9. Edit Success measures the percentage of cases where the edited result has a higher probability than the original result, while Paraphrase evaluates whether the model achieves the same performance using paraphrase prompts. Neighborhood assesses the model's ability to retain knowledge about the original object. Larimar demonstrates comparable performance in editing new facts and handling prompts. Sequential Fact Editing # To check sequential fact editing, Test retention rate(TRR) and edit retention rate(ERR) are used. TRR check how well an edited model retains its performence on tis original testing data. Larminar decoder\u0026rsquo;s perplexity was tested on 1000 random test samples from wikitext using a separate language model. In comparison, baseline models compute TRR from mean F1 scores from 1000 random samples of NQ data. ERR check how well an edited model retains previous edits. This paper, ERR was evaluated by F1 score after 1000 sequential edits when querying the memory with the encoded query Zquery for each written fact.\nFig11. Sequential editing on ZsRE dataset According to the figure 9, Larimar’s comparable ERR performance to GRACE, while preserving its original test set performance.Larimar-1.3B achieves editing speeds approximately 10 or more times faster than GRACE on GPT-2 XL. Selective Forgetting # This results shows that specific fact can be selectively erased from N facts that are have been written in Larimar\u0026rsquo;s memory.\nFig12. Batch editing accuracy on counterfact dataset. Green: MEMIT, Orange: ROME, Magenta: MEND, Black: Larimar. Fig 10 shows many edits can be written at once to memory and accurately retrieve from it. Rewrite accuracy is near 100% for up to 512 edits (the memory size K) and then drops to 82% for 1024 edits. This result shows Larimar's ability to compress more than K facts into its size-K memory. This performance level is higher when compared to baselines like MEND and ROME, but subpar compared to MEMIT, which can accurately handle a very large batch of edits at a cost of reduced editing speed and is also not meant to handle sequential editing. To test the ability of Larimar for selectively forgetting specified facts during inference, write N facts to memory and then forget one fact, and also write to memoty in its place the same fact with the answer replaced with the string \"unknown.\" Then, compare recall for the forgotten fact before and after the forgetting operation. Paper also report the recall on the remaining N −1 facts in memory to demonstrate that forgetting does not compromise other memories. The samples used are from the ZsRE validation set and from the Counterfact test set. Fig13. Fraction of facts with accurate recall, for the Counterfact and ZsRE datasets, after writing N factrs to memory and removing one. As a result, Larimar achived higher performance in forgotten and retained information is all testbench than Basemodel. This shows that Larimar works better on selective forgetting. Recall Performance # Larimar performs fact recall with long context using data that is not present in the base decoders pretraining corpus. Facts are curated from CNN Fast Facts. Recursive search in the latent memory space and using readouts to construct new higher-level memory is performed to process the long context with Larimar’s memory trained on a relative small episode length. It should be noted that memory hierarchy is also found in hippocampus and thought to be implicated in learning.\nThe recall rate, in the context of information retrieval, is a measure of how well a system retrieves all relevant items of a specific class. It represents the proportion of relevant items that the system correctly identifies out of all the relevant items available. For example, in a search engine scenario, the recall rate indicates how many of the relevant documents related to a user's query are successfully retrieved by the system. A high recall rate implies that the system effectively captures most, if not all, of the relevant information. Fig14. Novel fact addition recall rate on FastFacts. Fig 12 shows Larimar’s recall performance does not degrade much with increasing input context length, even compared to some of most competitive baseline LLMs trained with longer training context. We also compare with Supersizing Transformer, which is a memory-augmented model, however it did not show competitive recall performance because it was not trained to perform memory-conditioned generation. Due to memory processing in the latent space, Larimar is also efficient is terms of number of KV cache token computation compared to baseline methods. 6. Conclusion and further improvements # This paper propose enhancing Large Language Models (LLMs) with a dynamically updatable and distributed episodic memory. By leveraging a one-shot memory update mechanism and combining it with memory-conditioned decoding, this framework demonstrates precise, robust, and significantly faster editing performance compared to baselines in both single-fact and challenging sequential editing experiments. Using the same memory update mechanism enable fast and selective fact deletion operations, as well as effective information deletion mechanisms. Additionally, provide a simple approach for handling long input contexts, demonstrating better fact recall from longer input contexts in Larimar\u0026rsquo;s memory space compared to state-of-the-art LLMs trained with much larger training context windows. When compared to cutting-edge LLMs trained with much larger training context windows, Larimar showcases its advantages.\nJust as the interaction between the Neocortex and Hippocampus inspired the design of the memory module in this paper, drawing inspiration from the Corpus Callosum to conceptualize hardware could also be a viable approach. The Corpus Callosum, as a part of the brain, serves as a major connecting structure of the cerebral nervous system responsible for communication between the hemispheres. It spans across the entire brain, situated between the left and right hemispheres, facilitating the exchange and coordination of information between them to harmonize various brain functions. Adjusting all parameters of the model during the process of learning new knowledge in LLMs incurs significant costs. I propose a method to divide the model\u0026rsquo;s parameters into parts and update only the relevant parameters corresponding to the data being trained, thereby reducing costs. Introducing a module performing the role of the Corpus Callosum separately allows for the exchange and adjustment of data between parts, enabling more efficient learning with reduced costs and facilitating the processing of various types of data individually and complex information collectively within the model.\nAnother improvement could be to add a memory module specifically for image processing. The memory module used in this paper accepts natural language as input and produces natural language as output. I propose introducing a separate memory module for processing images, so that when both natural language and images are input simultaneously, the information can be processed and reflected in the output. This would enable more effective processing by LLMs when both images and natural language are provided as input. For example, it could be used to provide a photo of a crime scene and information as input, and obtain clues about the suspect as output.\n7. References # [1] https://arxiv.org/abs/2403.11901 -\u0026gt; Larimar: Large Language Models with Episodic Memory Control\n[2] https://arxiv.org/abs/2310.16218 -\u0026gt; Knowledge Editing for Large Language Models: A Survey\n[3] https://arxiv.org/abs/2207.04901 -\u0026gt; Exploring Length Generalization in Large Language Models\n[4] https://arxiv.org/abs/2402.05813 -\u0026gt; Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models\n[5] https://arxiv.org/abs/1410.3916 -\u0026gt; Memory Networks\n[6] https://arxiv.org/abs/1312.6114 -\u0026gt; Auto-Encoding Variational Bayes\n[7] https://process-mining.tistory.com/161 -\u0026gt; VAE, blog post\n[8] https://openreview.net/forum?id=Harn4_EZBw -\u0026gt; Generative Pseudo-Inverse Memory\nbrain figure\n"},{"id":23,"href":"/docs/spring24/23_/","title":"23","section":"Spring24","content":" Beyond Language Models: Byte Models are Digital World Simulators # Authors: Shangda Wu, Xu Tan, Zili Wang, Rui Wang, Xiaobing Li, Maosong Sun Links: Paper, GitHub, Hugging Face, Official Project Page\nPosted by Dohun Kim and Yeongwoo Kim\nIntroduction # Byte models take traditional language models to the byte level, treating all digital data and operations as fundamentally byte-based. These models process data from different modalities (such as text, images, and audio) uniformly as bytes, making them versatile in a wide digital environment. However, recent research on byte models has been limited, primarily focusing on narrow tasks and overlooking their broader potential in simulating the digital world.\nFigure 1: The bGPT framework simulates digital systems using native binary data. It integrates diverse data types into a single model by treating everything as a byte sequence.\nTo address this issues, the authors propose bGPT, which operates at the byte level and efficiently processes byte sequences. Through comprehensive evaluations across various modalities, bGPT demonstrates performance comparable to specialized models. Moreover, bGPT opens up new possibilities for simulating algorithms and hardware operations. By learning to predict the next byte, bGPT provides a deeper understanding of the intricate patterns in the digital world.\nThe main contributions of this paper are as follows:\nbGPT, a model with next-byte prediction is presented to simulate digital systems Hierarchical Transformer architecture is adapted to handle byte sequences efficiently. In-depth analysis of bGPT\u0026rsquo;s performance on text, audio, and image data is provided. Novel benchmarks are introduced to show bGPT\u0026rsquo;s capabilities for digital world simulation. bGPT Framework # Model Architecture # Learning patterns in digital systems at the byte level offers a unified approach for integrating various data types. However, the high granularity of bytes leads to long sequences, which significantly increase computational costs due to the quadratic scaling of self-attention. This limits the efficiency and scalability of processing binary data.\nFigure 2: The hierachical Transformer architecture of bGPT. It segments byte sequences into patches, to balance the need for long sequences and computational efficiency.\nTo address this issue, the authors adapted a hierarchical structure for bGPT, enabling efficient handling of long byte sequences. This structure segments a sequence of byte \\(B\\) of length \\(T\\) into a sequence of patches \\(\\mathcal{P}\\) , where each patch contains exactly \\(S\\) bytes, i.e., \\(\\mathcal{P}\\) includes \\(N = \\left\\lceil \\frac{T}{S} \\right\\rceil\\) patches. If \\(T \\mod S \\neq 0\\) , the last patch is padded with \u0026lt;eop\u0026gt; (end-of-patch) token.\nComponents of bGPT # Byte encoding: Each byte is one-hot encoded into a 257-dimensional vector, including all possible byte values and \u0026lt;eop\u0026gt; token. Each patch is viewed as a matrix of size \\(S \\times 257\\) . Linear Projection Layer maps the flattened patch into a dense vector of hidden size \\(H\\) , enabling more efficient processing of byte sequence. Patch-Level Decoder autoregressively predicts the features of the next patch, thereby learning the structural patterns of the entire dataset. Byte-Level Decoder takes the predicted patch features and autoregressively reconstruct the bytes within each patch. This process is repeated for all patches to generate the output byte sequence. Training Objectives # Pre-training: Generative Modeling # bGPT is pre-trained using a generative modeling approach, i.e., next-byte prediction. For a byte sequence \\(B=\\{b_1, b_2, \\ldots, b_T\\}\\) , the model predicts the next byte \\(b_{i\u0026#43;1}\\) at each position. The loss function is the negative log likelihood of the next byte at each step, encouraging the model to maximize the likelihood of the actual occurrence of the next byte.\n\\[ \\mathcal{L}_{\\text{GEN}}(\\theta) = - \\sum_{i=1}^{T-1} \\log p(b_{i\u0026#43;1} \\mid b_1, b_2, \\ldots, b_i; \\theta)\\] Fine-tuning: Classification # bGPT is further fine-tuned for classification tasks by adding a classification head on top of the byte-level decoder. The model takes a byte sequence \\(B\\) as input and predicts the category \\(y\\) to which that sequence belongs. The loss function used is the cross-entropy loss, ensuring that the model accurately outputs the prediction probabilities for each category.\n\\[ \\mathcal{L}_{\\text{CLF}}(\\theta) = - \\sum_{k=1}^{K} y_k \\log p(y_k \\mid B; \\theta)\\] Applications # Table 1: Overview of datasets for bGPT evaluation, with computational costs benchmarked in NVIDIA V100 GPU hours.\nDigital Media Processing # Having proficiency in processing diverse forms of digital media, including text, audio, and images, is a essential for human communication and interaction. bGPT is trained for generative modeling and classification tasks on text, images and speech dataset, demonstrating its ability to handle different modalities. The standardization for audio and image data was done as follows:\nModality Format Specifications Audio WAV 8000 Hz sampling rate, mono channel, 8-bit depth, 1 second length Image BMP 32x32 resolution, RGB color, 24-bit depth Algorithm and Hardware Simulation # Furthermore, to evaluate the model\u0026rsquo;s ability to simulate algorithms and hardware operations, bGPT is trained on two underexplored tasks: data conversion and CPU state modeling. These tasks are crucial for understanding and predicting the behavior of digital systems.\nData Conversion: bGPT learns to convert music data from ABC notation to MIDI format and vice versa. This task involves understanding the structure of music data and the conversion process between different formats.\nCPU State Modeling: bGPT predicts the state of a CPU after executing a sequence of machine instructions. This task requires understanding the internal operations of CPUs and predicting their states based on the given instructions.\nExperiments # Performance Metrics # The performance of bGPT is evaluated using the following metrics:\nBits Per Byte (BPB): The average number of bits required to encode each byte in the output sequence. Lower BPB values indicate better performance. Accuracy: The percentage of correctly classified samples in the classification task. Higher accuracy values indicate better performance. Digital Media Processing # Experiment Overview To assess the flexibility and versatility of the bGPT model, experiments with various types of digital media data were conducted. This involved handling a wide range of file types including text, audio, and image data. Also, to evaluate the model\u0026rsquo;s generalization capabilities, various settings of mixed modalities and transfer learning were explored.\nExperimental Setup # Model Modality Pre-training dataset bGPTrandom (None) (None, randomly initialized) bGPTwiki Text Wikipedia bGPTimage Image ImageNet bGPTlibri Speech LibriSpeech bGPTsignal Image+Speech ImageNet+LibriSpeech bGPTmix Text+Image+Speech Wikipedia+ImageNet+LibriSpeech Results and Analysis # Table 2: Performance comparison of bGPT models pre-trained on different datasets and baseline models in their respective modalities: GPT2-small on text, ViT-B/16 on images, and AST on speech.\nPerformance on downstream tasks (yellow) # Text/speech: competitive performance compared to baseline models Image: Large discrepancy compared to baseline models. The sequential nature of byte-level processing made it difficult to capture spatial information in images, which is crucial for image tasks.\nMixed modality pre-training # Trade-off between versatility and performance It dilutes the depth of domain-specific understanding in each modality Cross-modal fine-tuning # Negative transfer observed in transitioning between text and other modalities. Since text is human-created, byte-level patterns differ significantly from others. Cross-model knowledge transfer efficacy # Downstream task: Classification on spectrogram images with speech content Result: Accuracy - bGPTlibri \u0026gt; bGPTimage Interpretation: Content alignment is more important than modality alignment Algorithm and Hardware Simulation # Experiment Overview One of the unique capabilities of the bGPT model is its ability to simulate the operations of algorithms and hardware. This experimental section assesses how bGPT handles complex data conversion processes and CPU state modeling tasks. These capabilities are particularly significant in the fields of cybersecurity, system diagnostics, and hardware optimization.\nExperimental Setup # Data Conversion: Convering between ABC notation and MIDI format CPU State Modeling: Predicting the state of CPUs after executing a sequence of machine instructions Both tasks utilize data scales ranging from \\(10^3\\) (bGPT3), \\(10^4\\) (bGPT4), \\(10^5\\) (bGPT5), to \\(10^6\\) (bGPT6).\nResults and Analysis # Data Conversion Performance: bGPT performed the conversion between MIDI and ABC notation with high accuracy. Notably, it also showed high accuracy in converting MIDI back to ABC notation, indicating that bGPT successfully learned the inherent structures and patterns of the data. CPU State Modeling Performance: bGPT accurately predicted the resulting state of CPUs from an initial state across a variety of CPU instructions. It achieved over 99% accuracy even with complex instruction sequences, demonstrating bGPT\u0026rsquo;s detailed understanding of the internal workings of hardware. For both tasks, bGPT\u0026rsquo;s performance is significantly influenced by data volume. As the data scale increases, bGPT\u0026rsquo;s performance improves, indicating its scalability and ability to model complex algorithms and hardware operations.\nConclusion and Future Work # bGPT has proven to be a powerful model capable of effectively processing various types of digital media data. Particularly, this model can be flexibly applied to different types of data and has demonstrated performance that can compete with models pretrained on specific datasets. These results show that bGPT can be extremely useful in solving a wide range of real-world problems.\nMoreover, bGPT has proven its ability to go beyond simply processing data, successfully modeling and simulating the operations of complex algorithms and hardware. This capability will be particularly valuable in fields related to technical problem solving and new hardware design.\nbGPT extends deep learning to binary data processing through byte prediction. Experiments have demonstrated bGPT\u0026rsquo;s strong scalability in native binary data modeling.\nFuture research directions for byte models include:\nReducing training costs to make byte model training more feasible. Expanding the model and dataset sizes to accommodate a wider range of native binary data and handle larger digital media files such as high-resolution images and videos. Improving performance in underexplored tasks involving native binary data across various application domains. References # Beyond Language Models: Byte Models are Digital World Simulators (arXiv) Beyond Language Models: Byte Models are Digital World Simulators (Project Page) "},{"id":24,"href":"/docs/spring24/24_/","title":"24","section":"Spring24","content":" # "},{"id":25,"href":"/docs/spring24/25_/","title":"25","section":"Spring24","content":" Merging Text Transformer Models from Different Initializations # Posted by: Kyungtae Kim, Minwoo Kim\nAuthors: Neha Verma (Johns Hopkins University), Maha Elbayad (Meta)\n[paper link].\nAlthough recent works on model merging have exhibited low- or zero-barrier mode connectivity between models with different initialization, model merging on transformer architecture has not yet been studied extensively. The application of previous merging techniques on the transformer structure is limited due to its unique structural characteristics, such as residual connection, multi-head attention (MHA), and sequential input. The paper merges separate transformer minima, proposing a new model merging technique to investigate the relationship between the pre-trained models\u0026rsquo; minima in the loss landscape. Using permutation-based model merging, authors found lower loss barriers between minima compared to other model merging techniques such as model averaging. The results showed that the model has less sharp and isolated minima than previously expected.\nThe contributions of the researchers are listed as follows:\nThey introduced a new transformer merging algorithm based on model permutation. They showed that the technique leads to decreased loss barriers between masked language models trained from different initializations compared to other merging methods. They extended their approach to fine-tuned models and showed consistently smaller loss barriers between models compared to vanilla merging. Background # Transformer # Figure 1. Basic structure of a transformer. Transformer is a type of sequence-to-sequence (seq2seq) models that takes a sequence of tokens as an input, and computes according to the input token. Unlike previous seq2seq models where a certain input token had a hard time affecting every output tokens, transformer uses self-attention, which allows all tokens to affect every output tokens. This allows for better performance in data where the distance between tokens has low relationship to the importance of the tokens\u0026rsquo; importance. For more details on transformers and attention, see the paper \u0026lsquo;Attention is All You Need\u0026rsquo;.\nLoss Landscape # Loss landscape is a representation of the loss values around the weight space of the network. Loss landscape helps researchers see how well a neural network has been trained and gives researchers new insights on their models.\nFigure 2. An example of a loss landscape of a deep learning model. DNNs are trained by optimizing a loss function with an stochastic gradient descent (SGD) variant. The loss landscapes of these networks have been shown to contain infinitely many global minimizers that are reachable with the help of SGD. One reason for the abundance of minima is overparameterization, which leads different functions to behave similarly on the training data. Permutation and scaling invariances also lead to functionally identical minima that differ in the weight space. Prior works stated that the optima of loss functions are connected by simple curves over which training and test accuracy are nearly constant (no loss barrier). This is called mode connectivity. Other researchers conjectured that if the permutation invariances of neural networks are taken into account, these optima are linearly mode connected, i.e. the linear path connecting these two models has no loss barrier. In the paper, the authors pay attention on how permutation between models could lead to similar or identical loss landscapes.\nModel Interpolation # Model interpolation is a technique that blends two or more models to create an intermediate model. This process is mostly done by averaging the model weights. Researchers found out that if fine-tuned models lie in a single low error basin, then the weight averaging performs similarly to ensembling, which combines the output of multiple fine-tuned model to hopefully obtain a better result. It is however not guaranteed that fine-tuned models (starting from the same initialization) will reside in the same loss basin. Prior work on linear interpolation-based model merging has focused on improving the algorithms used to bring the hidden units of two networks into alignment, in order to reduce the barrier to interpolation between them.\nPermutation-based Merging # Feed-Forward Layers # In this section, we explain how the authors of the paper used permutation to find the similarities between two distinct models and merge them. In short, permutation order that maximizes the cross-correlation of the models is calculated and the permutation is used to change the weight ordering.\nIn more details, given two models \\(\\theta_A\\) and \\(\\theta_B\\) trained from distinct initializations, the authors compute post-activation features for each layer or sublayer parameter \\(\\text{W}_l\\subset \\theta\\) in order to compute the similar parts across models. The researchers compute \\(d\\) -dimensional activations across \\(n\\) tokens from both models \\(\\text{X}_A, \\text{X}_B\\in \\mathbb{R}^{n\\times d}\\) . Then, the feature relatedness via cross-correlation is computed as\n\\[C=\\text{corr}(\\text{X}_A, \\text{X}_B)=\\frac{\\mathbb{E}[(\\text{X}_A-\\boldsymbol{\\mu}_{\\text{X}_A})^\\text{T}(\\text{X}_B-\\boldsymbol{\\mu}_{\\text{X}_B})]}{\\boldsymbol{\\sigma}_{\\text{X}_A}\\boldsymbol{\\sigma}_{\\text{X}_B}},\\] where \\(\\boldsymbol{\\sigma}\\) is a standard deviation vector, and \\(\\boldsymbol{\\mu}\\) is a mean vector. The features are standardized since the magnitude of features values can vary greatly depending on the initialization. Next, the permutation that gives the highest correlation score is computed, and is declared as the optimal computation. More specifically, given \\(C\\in\\mathbb{R}^{d\\times d}\\) and a permutation mapping \\(\\pi\\) , the optimal permutation is computed as follows:\n\\[\\text{arg}\\max_\\pi \\sum_{i=1}^{d} C(i, \\pi(i)).\\] cf. The above problem is solved using the Jonker-Volgenant algorithm.\nNext, the permutation mapping \\(\\pi\\) is converted to a permutation matrix \\(\\text{P}\\) . The matrix is then multiplied to the original weight matrix of \\(B\\) denoted as \\(\\text{W}_l^B \\subset \\theta_B\\) . Then the permuted weight matrix \\(\\text{P}\\text{W}_l^B\\) closely resembles the weight \\(A\\) , denoted as \\(\\text{W}_l^A \\subset \\theta_A\\) . Denoting the modified model parameters as \\(\\theta_B\u0026#39;\\) , the final merged model is computed as \\(\\lambda\\theta_A\u0026#43;(1-\\lambda)\\theta_B\\) for some \\(\\lambda\\in[0,1]\\) .\ncf. If permutation matrix \\(\\text{P}\\) is multiplied in layer \\(l\\) , then \\(\\text{P}^{\\text{T}}=\\text{P}^{-1}\\) is applied in the next layer to unpermute the ordering, i.e.,\n\\[\\text{W}_{l\u0026#43;1}^{B\u0026#39;} \\leftarrow \\text{W}_{l\u0026#43;1}^{B}\\text{P}^{\\text{T}}.\\] Multi-head Attentions # Figure 3. Different head alignments occuring from different initialization. Though the 5 heads of the multi-head attention all connect to the same 5 features, the order of the heads may differ. As aformentioned, multi-head attention mechanism can be challenging to deal with due to its unique properties. The authors propose using permutation on each attention head separately and not permuting features between attention heads.\nMore specifically, multi-head attention parameters include parameters from key, query, value, and linear layer each denoted as \\(\\text{W}_K\\) , \\(\\text{W}_Q\\) , \\(\\text{W}_V\\) , and \\(\\text{W}_O\\) . For each key, query, and value weights, the whole parameter \\(\\text{W} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}\\) is partitioned into \\(H\\) attention heads each of output dimension \\(d_k = d_{\\text{model}}/H\\) . Permutation should be operated on each attention head separately, in order to apply a permutation to full weight matrices and maintain the functional equivalence of the overall model. This is because the final hidden vector from MHA reflects a concatenation of the result from each head, which are computed separately with weights \\(\\text{W}_{K_i} ,\\text{W}_{Q_i} , \\text{W}_{V_i}\\) for head \\(i\\) . As can be seen from figure 123, since the models are trained from different initializations, the correspondence of their attention heads may differ in addition to the correspondence of features within each head. The features are extracted just after the attention computation and before the linear layer. The features are used to compute \\(C\\) , and then the correlation matrix is partitioned by heads into \\(d_k \\times d_k\\) correlation matrices, for each potential attention head pair. Next, optimal permutation for each unique head pair \\((j, k)\\) is computed. Each head\u0026rsquo;s internal permutation is computed and stored, and the cost is computed as\n\\[\\text{cost}(j,k)=\\max_\\pi \\sum_{i=1}^{d_k} C_{jk}(i,\\pi(i)),\\] where \\(C_{jk}\\) refers to the specific partition of the overall correlation matrix. The outer head correspondence permutation is computed as\n\\[\\pi_{\\text{outer}}=\\text{arg}\\max_\\pi \\sum_{h=1}^{H} \\text{cost}(h,\\pi(h)).\\] The algorithm returns a permuting matrix \\(\\text{P}_{\\text{MHA}}\\) , which is applied to each of \\(\\text{W}_V\\) , \\(\\text{W}_K\\) and \\(\\text{W}_Q\\) .\nResidual Connections # Each transformer layer comes with two residual connections, as can be seen from Figure 1. The residual connections can be formulated as follows:\n\\[\\begin{aligned} x_a^r\u0026amp;=\\text{LN}(\\text{W}_O \\text{MHA}(x) \u0026#43; x),\\\\ x_f^r\u0026amp;=\\text{LN}(\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; x_a^r). \\end{aligned}\\] The input and output of both sublayers are added to create a new output. This implies that if a permutation operation is applied to the output state, the permutation should be the same for both addends. Also, since the inputs passes through the LayerNorm module, the permutation to the output should also permute the features of the LayerNorm module also. Ignoring the parameters of the LayerNorm,\n\\[\\begin{aligned} \\text{P}x_f^r\u0026amp;=\\text{P}(\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; x_a^r)\\\\ \u0026amp;=\\text{P}\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; \\text{P}x_a^r\\\\ \u0026amp;=\\text{P}\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; \\text{P}(\\text{W}_O \\text{MHA}(x) \u0026#43; x) \\end{aligned}\\] Since the input to each layer must be permuted ( \\(\\text{P}x\\) ), and the output of each layer is also permuted ( \\(\\text{P}x_f^r\\) ), the entire transformer architecture uses the same \\(\\{\\text{P}, \\text{P}^{\\text{T}}\\}\\) matrices for all weights involved in residual connections.\nModels, Tasks, Datasets and Evaluation settings # In this work, the authors investigated 5 different BERT models from the MultiBERTs reproductions seeds 1 through 5 (See \u0026lsquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u0026rsquo; \u0026amp; \u0026lsquo;The MultiBERTs: BERT Reproductions for Robustness Analysis\u0026rsquo;). Each model has the following properties:\nEach model comes from a bert-base-uncased checkpoint. Different random initialization and random ordering. Same original BERT vocabulary and tokenizer. To test the baseline method, they used the masked language modeling task while employing the validation set of the Wikitext-103 benchmark as the evaluation data. Next, they extracted over one million sentences from the Books corpus. In classification tasks, they employed fine-tuned models with randomly initialized classification head with pooling layer and classification layer weights. The authors kept the head initializations the same across the models. They used the General Language Understanding Evaluation (GLUE) benchmark excluding WNLI. As a baseline for comparison, vanilla averaging is defined as:\n\\[\\theta_{avg} = \\frac{1}{2}(\\theta_A\u0026#43;\\theta_B)\\] In this work, they defined new evaluation definitions. They defined loss-barriers as (\u0026lsquo;M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations\u0026rsquo;): \\[\\max_{\\lambda} \\mathcal{L}(\\lambda\\theta_A \u0026#43; (1 - \\lambda)\\theta_B) - \\frac{1}{2}(\\mathcal{L}(\\theta_A) \u0026#43; \\mathcal{L}(\\theta_B))\\] A masking probability of \\(p = 0.15\\) across block sizes of 128 tokens were used to compute MLM loss/pseudo-perplexity. For \\(N\\) masked samples in the text \\(\\textbf{W}\\) , pseudo-perplexity is defined as:\n\\[\\mathrm{Pseudo-PPL}(\\textbf{W};\\theta) = 2^{-\\frac{1}{N} \\sum_{i=1}^{N}\\log_{2}\\,p_\\theta(\\omega_i|\\textbf{W}_{\\backslash{i}})}\\] Results # By component # First, they found that the merging all feed-forward sublayers and/or merging all multi-headed attention sublayers reduces the pseudo-perplexity compared to the baseline. Remakably, combination of them leads to the reducuction of the perplexity by about 7 times at \\(\\lambda = 0.5\\) (See Figure 4). The reduced barrier suggests that a lower loss path has formed among these models, indicating a connection between the minima with a barrier similar to what they report.\nFigure 4. Results of pseudo-perplexity scores of 10 MultiBERTs with vanilla averaging, merging all feed-forward sublayers, and merging all multi-headed attention sublayers and all multi-headed attention sublayers. Next, they investigated how well these Transformers learn similar representations of the model. The average feature correlations of both the Feed-Forward layer and the attention pre-merged/merged with our method are calculated. The aligned models show higher average feature correlations than the orignal models. However, these values are no more than 0.3 because some pre-trained transformers can be sparsely activated and be pruned heavily leading to lower average feature correlations (Li et al.,2023; Dalvi et al., 2020).\nFigure 5. Results of average feature correlations between 10 masked language model pairs. Multi-headed attention # Next, they investigated loss barrier of Head-Permutation multi-headed attention approach. It is worth noting that this approach maintains head structure while allowing different head correspondences (Head-Perm). The proposed method exhibits lower loss barrier than method using simple attention averaging (Vanilla Attention Avg.), method that ignores the multiheaded structure of the weight parameters (Ignore-Heads), and method that does not allow for different head correspondences across different models (Monotonic) while exhibiting clear attention head boundaries of the correlation matrix (See Figure 5 and Table 1).\nMethod Loss Barrier\u0026darr; Std. Err. Vanilla Attention Avg. 4.31 0.21 Monotonic Head Alignment 4.13 0.20 Ignore-Heads 3.97 0.25 Head-Perm 3.71 0.23 Table 1. Loss Barriers of 10 MultiBERTs merged with feed-forward and attention components merged. Figure 6. Results of a correlation matrix between the first multi-headed attention layer from two different MultiBERTs models. Residual Stream # Next, the effect of the permutation alignment involving residual connection parameters is discussed. Repeated Add/Norm components sharing the permutation operations reduce the permutation symmetries and available residual stream parameters. The identity permutation which uses the identity matrix \\({I_d}\\) exhibits the lowest loss barrier because only one pair of \\({\\{P, P^T\\}}\\) is in the residual stream. We note that the seperate permutation approach, despite it having the largest loss barrier and no valid symmetry, has largest degrees of freedom.\nMethod Loss Barrier\u0026darr; Std. Err. Identity 4.95 0.38 First 7.58 0.19 Last 7.41 0.18 All 7.34 0.22 Seperate 9.38 0.49 Table 2. Loss Barriers of merged MultiBERTs with only residual components merged. Amount of Data # Moreover, they investigate the effect of the amount of sentences on the loss barrier. Despite the combination of feed-forward and attention layers, there is no strong directional relationship between the amount of data and the loss barrier. It seems that some variations are attributed by the quality of the data (See Figure 7).\nFigure 7. Results of loss barrier respect to the amount of sentences. GLUE results # Finally, they compared the loss barriers of their method to those of vanilla averaging approach for eight different GLUE tasks including residual permutations (See Figure 8 and Table 3). Vanilla averaging (STS-B) exhibits the highest loss, but some tasks show that the vanilla averaging outperforms their approach. They observe inconsistent loss reduction, with lower loss barriers than those of the masked language modeling setting. They also observe that lower loss pattern than either parent model at about \\(\\lambda = 0.15\\) and \\(\\lambda = 0.85\\) . Interestingly, M-shaped curve can be found in some vanilla merges between these fine-tuned models. In this perspective, their method could be extended to explore lower loss paths between finely-tuned minima. However, the selection of optimal data for the lowest loss, understanding of fine-tuned models and pre-connectivity in the loss landscape are remained for future work.\nFigure 8. Loss barrier curves for 8 GLUE tasks for vanilla interpolation and our strategy. Vanilla averaging Proposed Barrier Error Barrier Error MNLI-mm 0.61 0.03 0.72 0.08 QQP 1.37 0.09 1.20 0.11 QNLI 0.64 0.04 0.77 0.06 SST-2 0.42 0.04 0.36 0.07 CoLA 1.31 0.14 1.11 0.13 STS-B 5.15 0.44 4.24 0.35 MRPC 2.74 0.08 1.93 0.11 RTE 0.53 0.04 0.41 0.05 Table 3. Comparison of loss bariers between fine-tuned BERT model across 8 GLUE tasks for vanilla interpolation and our strategy. Conclusion # In this work, the authors develop a new strategy for model mergring based on permutation mapping and demonstrates reduced loss barriers between masked languaged models with different initialziation compared to vanilla merging. Next, they extend their approach to fine-tuned models. The authors suggest that understanding the connectedness between models lead to achieving sharpness of minima and smoothness Transformer loss space. Moreover, it can open up new possibilities for improving design optimization methods, ensembles of models, and additional merging techniques. Specifically, this paper shows that permutation invariances of Transformer model is considered to characterize the geometric features of minima. Finally, they shad the light on the relationships between fine-tuned models, Transformer width and loss barriers, and the data for characterize the relationship between Transformer minima.\nFurther research is possible, examining the proposed techniques on a more complicated or larger dataset. Also, more sophisticated inspection on the aformentioned M-shaped loss curve is needed.\nReferences # Paper: https://arxiv.org/abs/2403.00986\nAttention mechanism: https://arxiv.org/abs/1706.03762\nLoss landscape explanation: https://arxiv.org/abs/1712.09913\nBERT: https://aclanthology.org/N19-1423/\nMultiBERTs: https://arxiv.org/abs/2106.16163\nWikitext-103 benchmark: https://arxiv.org/abs/1609.07843\nBooks corpus: https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html\nGLUE: https://arxiv.org/abs/1804.07461\nWNLI: https://arxiv.org/abs/1810.04805\nLoss barrier: https://arxiv.org/abs/1803.0363\n"}]