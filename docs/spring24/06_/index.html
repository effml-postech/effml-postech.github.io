<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="XC-CACHE: Cross-Attending to Cached Context for Efficient LLM Inference # Submitted on 23 Apr 2024 by João Monteiro1, Étienne Marcotte1,, Pierre-André Noël1,, Valentina Zantedeschi1,*, David Vázquez1, Nicolas Chapados1, 2, Christopher Pal1, 2, Perouz Taslakian11, ServiceNow Research. Posted by HyunDong Kim, Sangil Han
Background Knowledge # In-context Learning(ICL) # In-context Learning (ICL) is a technique frequently used with Large Language Models (LLMs), such as GPT-3. This technique leverages the knowledge acquired during the pre-training phase, enabling the model to understand and apply new contextual information effectively.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/spring24/06_/">
  <meta property="og:site_name" content="Efficient ML Systems">
  <meta property="og:title" content="Efficient ML Systems">
  <meta property="og:description" content="XC-CACHE: Cross-Attending to Cached Context for Efficient LLM Inference # Submitted on 23 Apr 2024 by João Monteiro1, Étienne Marcotte1,, Pierre-André Noël1,, Valentina Zantedeschi1,*, David Vázquez1, Nicolas Chapados1, 2, Christopher Pal1, 2, Perouz Taslakian11, ServiceNow Research. Posted by HyunDong Kim, Sangil Han
Background Knowledge # In-context Learning(ICL) # In-context Learning (ICL) is a technique frequently used with Large Language Models (LLMs), such as GPT-3. This technique leverages the knowledge acquired during the pre-training phase, enabling the model to understand and apply new contextual information effectively.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="website">
<title>06 | Efficient ML Systems</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" >
<link rel="canonical" href="http://localhost:1313/docs/spring24/06_/">
<link rel="stylesheet" href="/book.min.309b7ed028807cdb68d8d61e26d609f48369c098dbf5e4d8c0dcf4cdf49feafc.css" integrity="sha256-MJt&#43;0CiAfNto2NYeJtYJ9INpwJjb9eTYwNz0zfSf6vw=" crossorigin="anonymous">
  <script defer src="/flexsearch.min.js"></script>
  <script defer src="/en.search.min.4a691cc797b715a80e21a2b605e5ac5cce9cd5c81f6aa58b04b3886e2f42ffd7.js" integrity="sha256-Smkcx5e3FagOIaK2BeWsXM6c1cgfaqWLBLOIbi9C/9c=" crossorigin="anonymous"></script>

  

<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/docs/spring24/06_/index.xml" title="Efficient ML Systems" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>Efficient ML Systems</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>












  



  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-8c2bdf4d8f82cc2d8f16541464d61b3d" class="toggle" checked />
    <label for="section-8c2bdf4d8f82cc2d8f16541464d61b3d" class="flex justify-between">
      <a role="button" class="">Spring24</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/00_taco_example/" class="">00 Taco Example</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/01_/" class="">01</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/02_/" class="">02</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/03_/" class="">03</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/04_/" class="">04</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/05_/" class="">05</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/06_/" class="active">06</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/07_/" class="">07</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/08_/" class="">08</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/09_/" class="">09</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/10_/" class="">10</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/11_/" class="">11</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/12_/" class="">12</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/13_/" class="">13</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/14_/" class="">14</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/15_/" class="">15</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/16_/" class="">16</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/17_/" class="">17</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/18_/" class="">18</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/19_/" class="">19</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/20_/" class="">20</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/21_/" class="">21</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/22_/" class="">22</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/23_/" class="">23</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/24_/" class="">24</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/25_/" class="">25</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>















</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>06</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#background-knowledge">Background Knowledge</a>
      <ul>
        <li><a href="#in-context-learningicl">In-context Learning(ICL)</a></li>
        <li><a href="#llm">LLM</a></li>
        <li><a href="#caching">Caching</a></li>
        <li><a href="#transformer">Transformer</a></li>
        <li><a href="#self-attention">Self-Attention</a></li>
      </ul>
    </li>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#methods">Methods</a>
      <ul>
        <li><a href="#caching-representations-and-xc-caching">Caching Representations and XC-CACHING</a></li>
        <li><a href="#low-memory-usage-high-precision-xc-caching">Low memory usage, High precision XC-CACHING</a></li>
      </ul>
    </li>
    <li><a href="#results">Results</a>
      <ul>
        <li><a href="#compare-with-other-methods">Compare with other methods</a></li>
        <li><a href="#expanded-evaluation-of-context-conditioning-methods">Expanded Evaluation of Context Conditioning Methods</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#related-works">Related works</a>
      <ul>
        <li><a href="#decoders-as-encoders">Decoders as encoders</a></li>
      </ul>
    </li>
    <li><a href="#discussion-future-research-directions"><strong>Discussion: Future Research Directions</strong></a>
      <ul>
        <li><a href="#problems-and-concerns"><strong>Problems and Concerns</strong></a></li>
        <li><a href="#future-directions">Future Directions</a></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="xc-cache-cross-attending-to-cached-context-for-efficient-llm-inference">
  XC-CACHE: Cross-Attending to Cached Context for Efficient LLM Inference
  <a class="anchor" href="#xc-cache-cross-attending-to-cached-context-for-efficient-llm-inference">#</a>
</h1>
<p>Submitted on 23 Apr 2024 by João Monteiro1, Étienne Marcotte1,<em>, Pierre-André Noël1,</em>, Valentina Zantedeschi1,*, David Vázquez1, Nicolas Chapados1, 2, Christopher Pal1, 2, Perouz Taslakian11, ServiceNow Research.
Posted by HyunDong Kim, Sangil Han</p>
<h2 id="background-knowledge">
  Background Knowledge
  <a class="anchor" href="#background-knowledge">#</a>
</h2>
<h3 id="in-context-learningicl">
  In-context Learning(ICL)
  <a class="anchor" href="#in-context-learningicl">#</a>
</h3>
<p>In-context Learning (ICL) is a technique frequently used with Large Language Models (LLMs), such as GPT-3. This technique leverages the knowledge acquired during the pre-training phase, enabling the model to understand and apply new contextual information effectively. ICL allows the model to perform specific tasks efficiently without requiring additional training or fine-tuning.</p>
<p align="center">
    <img src='./icl.png' width="650">
    <br>
    <i>Fig. 1 A comparative illustration of ICL and CoT prompting</i>
</p>
In ICL, a single prompt is composed of a task description, demonstration, and query. When this prompt is passed through a Large Language Model (LLM), it results in obtaining the desired response. The model understands how to perform a specific task within a given context through examples. This process involves setting the context, providing examples, and then generating appropriate outputs based on the combined information. ICL allows the model to perform specific tasks efficiently without requiring additional training or fine-tuning. typically uses prompting to condition the generation of decoder-only language models based on reference information. 
<p>However, just-in-time processing of context is inefficient due to the quadratic cost of self-attention operations, making caching desirable. Yet, caching transformer states can demand nearly as much space as the model parameters themselves, posing a challenge when the appropriate context is not known in advance.</p>
<h3 id="llm">
  LLM
  <a class="anchor" href="#llm">#</a>
</h3>
<p>LLM(Large language model) is a technology that understands and creates language-like text from vast amounts of data
For example, Translation, Fixing typos, question-answer, summary, &hellip;</p>
<p><strong>- Emergent abilities of LLM</strong>
“the ability that are not present in small but arise in large models”</p>
<ul>
<li>J.Wei et al,, “Emergent abilities of large language models,”
⇒ <strong>Large Language Models that can be performed on unlearned tasks</strong></li>
</ul>
<p>“Emergence is when quantitative changes in a system result in qualitative changes in behavior.”</p>
<ul>
<li>“More is Different” by Nobel prize-winning physicist Phulip Anderson</li>
</ul>
<p>NLP vs LLM
NLP (Natural Language Processing)</p>
<ul>
<li>Covers general language analysis
LLM (Large-Language Model)</li>
<li>Subset of NLP</li>
<li>Based on &ldquo;Large&rdquo; dataset</li>
</ul>
<h3 id="caching">
  Caching
  <a class="anchor" href="#caching">#</a>
</h3>
<p align="center">
    <img src='./caching.png' width="350">
    <br>
    <i>Fig. 2 The way of Caching</i>
</p>
Caching involves storing data or computation results in a temporary storage area called a cache. When a request is made, the system first checks the cache. If the data is found (cache hit), it is retrieved from the cache; otherwise (cache miss), it is fetched from the source, processed, and stored in the cache.
<p>Advantages of caching include faster access to frequently used data, efficient resource use by reducing redundant computations, and improved service availability during network issues. However, challenges include maintaining data consistency and the need for additional memory.</p>
<p>Common uses of caching are web page caching for faster loading, database query caching to reduce load and response times, and API caching to decrease the number of API calls and improve performance. Caching is vital for enhancing performance in large-scale and data-intensive applications.</p>
<h3 id="transformer">
  Transformer
  <a class="anchor" href="#transformer">#</a>
</h3>
<p align="center">
    <img src='./transformer.png' width="550">
    <br>
    <i>Fig. 3 Procedure of learning in Transfrer model</i>
</p>
In Transformer models, the Attention Mechanism helps the model focus on important parts of the input data. It uses three main components: Key, Value, and Query, which are vectors derived from input tokens. Keys capture token features, Values contain the token content, and Queries represent the current token's features. The attention process calculates match scores between Queries and Keys, normalizes these scores with softmax, and computes a weighted sum of Values based on these probabilities. This allows the model to understand complex relationships within the data, enabling accurate and contextually appropriate responses.
<h3 id="self-attention">
  Self-Attention
  <a class="anchor" href="#self-attention">#</a>
</h3>
<p>Finding the relationship between a sentence and a sentence.
In other words, looking at the relationship between words and words within a sentence.</p>
<ul>
<li>Attention</li>
</ul>
<p>The basic idea of attention is to once again refer to the entire input sentence in the encoder at each time step at which the decoder predicts the output word. However, rather than considering all input words in the same proportion, we will focus more on the input word part related to the word to be predicted at that point.</p>
<p>The attention function is usually expressed as follows</p>
<p>Attention(Q, K, V) = Attention Value</p>
<p>The attention function individually calculates the similarity with all keys for a given query and reflects these similarities in each value associated with that key. Then, all of the &lsquo;Values&rsquo; reflecting this similarity are summed and returned.</p>
<p>These results are called Attention Value.
Q: Concealed states in decoder cells at all time points
K: Silver states of encoder cells at all time points
V: Concealed states of encoder cells at all time points</p>
<p>Self-attention means that Q, K, and V are all the same. In the transformer, three attention types are used: Encoder Self-attention, Masked Decoder Self-attention, and Encoder-Decoder Attention. Naturally, both self-attention have the same Q, K, and V of themselves, but in the third Encoder-Decoder Attention, Query is the vector of the decoder and Key and Value are the vectors of the encoder.</p>
<p align="center">
    <img src='./selfattention.png' width="550">
    <br>
    <i>Fig. 4 The structure of selfattention procedure</i>
</p>
<p>To intuitively understand the self-attention of the encoder, it means that the similarity between words in the input sentence is obtained.</p>
<p align="center">
    <img src='./word.png' width="300">
    <br>
    <i>Fig. 5 Self-attention mechanism showing the similarity between words</i>
</p>
<p><strong>Get Q, K, V vectors</strong>
As described above, the self-attention operates based on the word vector of the input sentence. However, in practice, in order to perform the self-attention, a process of obtaining the Q-vector, the K-vector, and the V-vector from the word vectors which are the initial inputs of the encoder is required. In this case, the Q-vector, the K-vector, and the V-vector have a lower dimension, unlike the dimension of the initial input, and in transformer, each 512-dimensional word vector is converted into a 64-dimensional Q-vector, the K-vector, and the V-vector from the initial input.</p>
<p>The above value of 64 is determined by another hyperparameter of transformer, num_heads. In transformer, the value of num_heads is set to 8, and thus it is necessary to convert the value into a Q vector, K vector, and V vector in 512/8=64 dimensions.</p>
<p align="center">
    <img src='./word_example.png' width="450">
    <br>
    <i>Fig. 6 The process of converting a 512-dimensional embedding vector into Q, K, and V vectors</i>
</p>
<p>For example, the 512-dimensional embedding vector of the word &ldquo;student&rdquo; is multiplied by a Q weight, K weight, and V weight matrix with 512 X 64 dimensions, respectively, to obtain Q, K, and V vectors. If Q, K, and V vectors are obtained, it is the same as the existing attention mechanism from now on. Each Q vector obtains an attention score for all K vectors, obtains an attention distribution, and then uses it to weight the V vector to obtain an attention value or a context vector. And this is repeated for all Q vectors.</p>
<h2 id="introduction">
  Introduction
  <a class="anchor" href="#introduction">#</a>
</h2>
<p>This paper addresses these challenges by introducing models that use cross-attention, inspired by the encoder-decoder architecture, to condition generation on reference text without a prompt. The approach leverages pre-trained decoder-only models and trains only a small number of added layers. The authors use Question-Answering (QA) as a testbed to evaluate these models&rsquo; ability to perform conditional generation.</p>
<p align="center">
    <img src='./approach.png' width="650">
    <br>
    <i>Fig. 6 Faster inference in context-conditional language modeling</i>
</p>
<p>These four approaches highlight various strategies for efficient context processing in large language models.</p>
<p>(a) depicts a scenario where a user’s query must be interpreted within a given context to generate an answer. In this case, the query and answer are small (light), but the context is large (heavy). This results in a time complexity of O(|context|²) for the LLM.</p>
<p>(b) explains <a href="https://arxiv.org/abs/2005.14165"><em><strong>In-Context Learning (ICL)</strong></em></a> and <a href="https://arxiv.org/abs/2005.11401"><em><strong>Retrieval-Augmented Generation (RAG)</strong></em></a>  which use the query to look up the context from a finite corpus, but still remain inefficient with large contexts.</p>
<p>(c) can be preprocessed into a cache, enabling fast inference on a given query. This <a href="https://arxiv.org/abs/1706.03762"><em><strong>approach</strong></em></a> has a time complexity of O(|context|(|query| + |answer|)).</p>
<p>(d) is the method that the author proposed named <strong>XC-CACHE</strong>. It is implemented in two ways that leverage pre-trained decoder-only models and add a separate encoder to process the context: one approach uses the frozen decoder as an encoder (called XC-LLAMA), and the other uses a small bidirectional encoder (called XC-LLAMAENC).</p>
<h2 id="methods">
  Methods
  <a class="anchor" href="#methods">#</a>
</h2>
<h3 id="caching-representations-and-xc-caching">
  Caching Representations and XC-CACHING
  <a class="anchor" href="#caching-representations-and-xc-caching">#</a>
</h3>
<p><a href="https://en.wikipedia.org/wiki/Cache_%28computing%29"><strong>Cache</strong></a> is typically a form of memory that allows for fast access, storing data that is frequently used or needed repeatedly. The main purpose of a cache is to improve processing speed and enhance the overall efficiency of the system. There are three types of elements called ‘key’, ‘value’, ‘query’ which can approaches to Caching.</p>
<p>KV(Key-Value) Caching is to store the (past) key and value states generated while processing context.  As an example, for <a href="https://arxiv.org/abs/2307.09288"><em><strong>LLAMA 2-7B</strong></em></a> using 16 bits precision shows that the smaller per-token cache teh sizes are more desirable. JIT(Just-In-Time Key-Value Caching)-KV Caching is an alternative approach involves storing the (past) hidden states of the model in the cache. At inference time, once these hidden states are loaded on GPU, we can recover the full keys and values in O(|context|). These two KV and JIT-KV Caching model both entail two types of costs while yielding identical results: the size of the cache and the operations required during inference. So <strong>XC-Caching</strong> is presented as an effective way to improve inference speed while significantly reducing memory usage.</p>
<p align="center">
    <img src='./architectures.png' width="650">
    <br>
    <i>Fig. 7 XC-LLAMA’s architectures. A decoder-only model implements encoder-decoder architectures</i>
</p>
<p>Finetuning out in a parameter-efficient fashion via training only a small number of cross-attention layers.
(a) The architecture uses a small bidirectional encoder and multiple self-attention and cross-attention layers to process the context and prompt.
(b) The architecture uses only a decoder, mainly training the cross-attention layers to process the context and prompt.</p>
<h3 id="low-memory-usage-high-precision-xc-caching">
  Low memory usage, High precision XC-CACHING
  <a class="anchor" href="#low-memory-usage-high-precision-xc-caching">#</a>
</h3>
<p>QA is ideal for testing the methods as it requires efficient external information retrieval and incorporation during generation. They focus on training for question-answering using datasets with context, query, and answer triplets. They build a training dataset by standardizing and combining the training partitions of five publicly available and diverse datasets: <a href="https://aclanthology.org/Q19-1026/"><em><strong>NATURAL QUESTIONS</strong></em></a> (NQ), <a href="https://arxiv.org/abs/1809.09600"><em><strong>HOTPOTQA</strong></em></a>, <a href="https://aclanthology.org/2022.tacl-1.27/"><em><strong>TOPIOCQA</strong></em></a>, <a href="https://arxiv.org/abs/1611.09268"><em><strong>MS MARCO</strong></em></a>, and <a href="https://aclanthology.org/P18-2124/"><em><strong>SQUAD-V2</strong></em></a>. Each example in the resulting dataset contains a query (natural-language question), an answer (expected output), and one or more contexts (e.g., knowledge base articles), with at least one context containing the answer, referred to as the reference context.</p>
<p>In addition to training on the primary QA tasks, they optimize their models on context repetition tasks, named multitask training strategy. They use the pre-trained <a href="https://arxiv.org/abs/2307.09288">LLAMA 2</a> to create variations of XC-LLAMA. For XC-LLAMAENC, a version with a dedicated encoder, they fine-tune the <a href="https://arxiv.org/abs/2004.05150">LONGFORMER</a>, increasing its input length to 6,000 tokens. They add one cross-attention layer every few self-attention layers, specifically using a 5-6 configuration. Training is done with the <a href="https://arxiv.org/abs/1711.05101">ADAMW optimizer</a>, a batch size of 256, for 40,000 steps (4 epochs), with a linear learning rate scheduler.</p>
<p>They compare their models to ICL methods for generating context-based answers, noting that the contexts often have a low signal-to-noise ratio. They use metrics like F1 SCORE, RECALL, METEOR, and ROUGEL, with a focus on F1 and also evaluate BERTSCORE between predictions and ground-truth answers.</p>
<h2 id="results">
  Results
  <a class="anchor" href="#results">#</a>
</h2>
<h3 id="compare-with-other-methods">
  Compare with other methods
  <a class="anchor" href="#compare-with-other-methods">#</a>
</h3>
<p align="center">
    <img src='./comparison.png' width="450">
    <br>
    <i>Table 1. QA performance on three diverse information-seeking tasks</i>
</p>
<p>They compare their method to existing conditional generation approaches. The main baseline is ICL, using context as part of the prompt, with results for LLAMA 2-CHAT and GPT-3.5-TURBO. FiD, a state-of-the-art T5-based QA model, is also included for comparison.</p>
<p>Results (Table 1) show that cross-attending to contexts (XC-LLAMA or XC-LLAMAENC) significantly improves performance compared to prompting (LLAMA 2-CHAT). This approach is more broadly applicable and practical as it avoids the high variance induced by prompting. Even when using the decoder as an encoder, cross-attention to contexts outperforms ICL, suggesting that trained cross-attention layers compensate for sub-optimal encoder representations.</p>
<h3 id="expanded-evaluation-of-context-conditioning-methods">
  Expanded Evaluation of Context Conditioning Methods
  <a class="anchor" href="#expanded-evaluation-of-context-conditioning-methods">#</a>
</h3>
<p>The previous section&rsquo;s results show that adding and fine-tuning dedicated parameters for context-conditioning improves performance over prompting. Based on this, they expand their evaluation to consider alternative approaches using a small number of additional parameters for conditioning generation on reference contexts. They focus on both prediction performance and computational efficiency, assessing how well different models can pre-process and cache context representations.</p>
<p>They fine-tune LORA adapters applied to the same LLAMA 2 decoder used for XC-LLAMA. This fine-tuning significantly improves QA accuracy over ICL baselines but requires storing all KV states throughout every layer, leading to high costs. In contrast, encoder models only need to cache the hidden states of their last layer, reducing space requirements.</p>
<p align="center">
    <img src='./table_2.png' width="450">
    <br>
    <i>Table 2. Cache memory footprint per context token</i>
</p>
<p>As shown in Table 2, XC-LLAMA variants greatly reduce the caching footprint by storing only the last hidden states of the encoder. XC-LLAMAENC further reduces space requirements due to the lower dimensional representation of LONGFORMER compared to LLAMA 2. This reduction in cache size is practically significant, especially for large datasets like Wikipedia, as it decreases memory usage and communication costs, enabling longer generation or larger batch sizes during inference.</p>
<p align="center">
    <img src='./table_3.png' width="450">
    <br>
    <i>Table 3. QA performance </i>
</p>
<p>The Pareto set includes ICL models fine-tuned with LORA, which have higher BERTSCORE but require substantial caching space, and encoder models, which slightly sacrifice prediction accuracy but significantly reduce memory footprint. Detailed QA results in Table 3 include the GPT-3.5 TURBO ICL baseline and FiD, fine-tuned on our training dataset. FiD is included for performance reference but is not directly comparable as it does not support caching pre-processed contexts. Their models achieve significant space savings with only a slight reduction in prediction accuracy, advantageous in various practical scenarios.</p>
<h2 id="conclusion">
  Conclusion
  <a class="anchor" href="#conclusion">#</a>
</h2>
<p>They introduced XC-LLAMA, a method to convert a pre-trained decoder-only language model into an encoder-decoder architecture that can generate outputs based on both encoder inputs and decoder queries. This is done by adding cross-attention layers between the self-attention layers of the pre-trained decoder. Two methods for defining the encoder are discussed: using a copy of the decoder or introducing a small, trainable bi-directional encoder. This architecture reduces caching space by over 300 times. In QA settings, XC-LLAMA shows higher prediction accuracy compared to standard In-Context Learning (ICL) methods with LLAMA 2 or GPT-3.5 TURBO. It also achieves accuracy levels nearly on par with caching-intensive fine-tuned prompted models, offering a more practical, caching-efficient alternative.</p>
<h2 id="related-works">
  Related works
  <a class="anchor" href="#related-works">#</a>
</h2>
<h3 id="decoders-as-encoders">
  Decoders as encoders
  <a class="anchor" href="#decoders-as-encoders">#</a>
</h3>
<p align="center">
    <img src='./22.png' width="450">
    <br>
    <i>Fig 8. GRITLM architecture and format</i>
</p>
<p><a href="https://arxiv.org/abs/2402.09906"><em><strong>GRIT</strong></em></a> converts a pretrained causal decoder into a bi-directional encoder,yielding sentence-level embeddings while maintaining its ability to perform autoregressive generation of text. However, unlike the models they consider, this conversion requires fine-tuning all model parameters instead of additional ones. Parameterefficient approaches to turn decoders into encoders were also proposed.</p>
<h2 id="discussion-future-research-directions">
  <strong>Discussion: Future Research Directions</strong>
  <a class="anchor" href="#discussion-future-research-directions">#</a>
</h2>
<h3 id="problems-and-concerns">
  <strong>Problems and Concerns</strong>
  <a class="anchor" href="#problems-and-concerns">#</a>
</h3>
<ol>
<li>Fake News and Misinformation
Large language models (LLMs) like GPT-3.5 and LLAMA have shown an ability to generate text that is almost indistinguishable from human-written content. This can be misused to create fake news or misinformation, posing a significant challenge. Research should focus on developing mechanisms to detect and mitigate the spread of misinformation. One potential direction is to enhance the model&rsquo;s ability to verify information against trusted sources before generating responses</li>
<li>Evaluate different datasets and tasks
While QA tasks are an important area in NLP, there are many other tasks in this field. For example, text classification(IMDb), sentiment analysis(Sentiment140,SST-2), machine translation(WMT), and information(MS MARCO) retrieval are all diverse tasks within NLP. Each task has unique characteristics and requirements, so evaluating the efficiency of XC-CACHE across various tasks is essential.</li>
</ol>
<h3 id="future-directions">
  Future Directions
  <a class="anchor" href="#future-directions">#</a>
</h3>
<ol>
<li>Improving Caching Techniques
The XC-CACHE method has demonstrated significant efficiency gains. Further refinement of caching techniques could lead to even greater reductions in memory usage and computational cost. Exploring alternative caching strategies and optimizing cross-attention mechanisms may yield more efficient models without compromising performance.</li>
<li>Context-Aware Models
Developing models that can dynamically adjust their context based on the specific query could improve accuracy and relevance. Future work could focus on adaptive models that utilize context more intelligently, potentially incorporating real-time data retrieval and updating caches accordingly.</li>
</ol>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">


  <div><a class="flex align-center" href="https://github.com/effml-postech/blog-post/commit/571e86f4a37172e0b9245d0cc1fe7f83c5d50c8c" title='Last modified by hyundong Kim | May 25, 2024' target="_blank" rel="noopener">
      <img src="/svg/calendar.svg" class="book-icon" alt="" />
      <span>May 25, 2024</span>
    </a>
  </div>



  <div>
    <a class="flex align-center" href="https://github.com/effml-postech/blog-post/edit/main//content/docs/spring24/06_/_index.md" target="_blank" rel="noopener">
      <img src="/svg/edit.svg" class="book-icon" alt="" />
      <span>Edit this page</span>
    </a>
  </div>


</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#background-knowledge">Background Knowledge</a>
      <ul>
        <li><a href="#in-context-learningicl">In-context Learning(ICL)</a></li>
        <li><a href="#llm">LLM</a></li>
        <li><a href="#caching">Caching</a></li>
        <li><a href="#transformer">Transformer</a></li>
        <li><a href="#self-attention">Self-Attention</a></li>
      </ul>
    </li>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#methods">Methods</a>
      <ul>
        <li><a href="#caching-representations-and-xc-caching">Caching Representations and XC-CACHING</a></li>
        <li><a href="#low-memory-usage-high-precision-xc-caching">Low memory usage, High precision XC-CACHING</a></li>
      </ul>
    </li>
    <li><a href="#results">Results</a>
      <ul>
        <li><a href="#compare-with-other-methods">Compare with other methods</a></li>
        <li><a href="#expanded-evaluation-of-context-conditioning-methods">Expanded Evaluation of Context Conditioning Methods</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#related-works">Related works</a>
      <ul>
        <li><a href="#decoders-as-encoders">Decoders as encoders</a></li>
      </ul>
    </li>
    <li><a href="#discussion-future-research-directions"><strong>Discussion: Future Research Directions</strong></a>
      <ul>
        <li><a href="#problems-and-concerns"><strong>Problems and Concerns</strong></a></li>
        <li><a href="#future-directions">Future Directions</a></li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












