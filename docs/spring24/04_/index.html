<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Better &amp; Faster Large Language Models via Multi-token Prediction # Posted by Jinoh Cho and Seonghyeon Park
Authors: Gloeckle et al. Institution : FAIR at Meta, CERMICS Ecole des Ponts ParisTech and LISN Universite Paris-Saclay Preliminaries # Language Modeling and Next-Token Prediction Task # Learning through a next-token prediction task has been a mainstream for language modeling. The goal of a next-token prediction task is to maximize the probability of the next token $x_{t&#43;1}$, given the history of previous tokens $x_{t:1} = x_1, \ldots, x_t$.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/spring24/04_/">
  <meta property="og:site_name" content="Efficient ML Systems">
  <meta property="og:title" content="Efficient ML Systems">
  <meta property="og:description" content="Better &amp; Faster Large Language Models via Multi-token Prediction # Posted by Jinoh Cho and Seonghyeon Park
Authors: Gloeckle et al. Institution : FAIR at Meta, CERMICS Ecole des Ponts ParisTech and LISN Universite Paris-Saclay Preliminaries # Language Modeling and Next-Token Prediction Task # Learning through a next-token prediction task has been a mainstream for language modeling. The goal of a next-token prediction task is to maximize the probability of the next token $x_{t&#43;1}$, given the history of previous tokens $x_{t:1} = x_1, \ldots, x_t$.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="website">
<title>04 | Efficient ML Systems</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" >
<link rel="canonical" href="http://localhost:1313/docs/spring24/04_/">
<link rel="stylesheet" href="/book.min.309b7ed028807cdb68d8d61e26d609f48369c098dbf5e4d8c0dcf4cdf49feafc.css" integrity="sha256-MJt&#43;0CiAfNto2NYeJtYJ9INpwJjb9eTYwNz0zfSf6vw=" crossorigin="anonymous">
  <script defer src="/flexsearch.min.js"></script>
  <script defer src="/en.search.min.83231fe0f488d1863066fd351c42462dcfb3dfe9e1abae22188925238b5be6ba.js" integrity="sha256-gyMf4PSI0YYwZv01HEJGLc&#43;z3&#43;nhq64iGIklI4tb5ro=" crossorigin="anonymous"></script>

  

<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/docs/spring24/04_/index.xml" title="Efficient ML Systems" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>Efficient ML Systems</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>












  



  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-8c2bdf4d8f82cc2d8f16541464d61b3d" class="toggle" checked />
    <label for="section-8c2bdf4d8f82cc2d8f16541464d61b3d" class="flex justify-between">
      <a role="button" class="">Spring24</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/00_taco_example/" class="">00 Taco Example</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/01_/" class="">01</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/02_/" class="">02</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/03_/" class="">03</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/04_/" class="active">04</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/05_/" class="">05</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/06_/" class="">06</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/07_/" class="">07</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/08_/" class="">08</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/09_/" class="">09</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/10_/" class="">10</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/11_/" class="">11</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/12_/" class="">12</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/13_/" class="">13</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/15_/" class="">15</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/16_/" class="">16</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/17_/" class="">17</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/18_/" class="">18</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/19_/" class="">19</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/20_/" class="">20</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/21_/" class="">21</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/22_/" class="">22</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/23_/" class="">23</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/24_/" class="">24</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/25_/" class="">25</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/14_/" class="">14</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>















</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>04</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#language-modeling-and-next-token-prediction-task">Language Modeling and Next-Token Prediction Task</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#multi-token-prediction-task">Multi-Token Prediction Task</a></li>
        <li><a href="#memory-efficient-implementation">Memory-Efficient Implementation</a></li>
        <li><a href="#faster-inference-with-self-speculative-decoding">Faster Inference with Self-Speculative Decoding</a></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="better--faster-large-language-models-via-multi-token-prediction">
  Better &amp; Faster Large Language Models via Multi-token Prediction
  <a class="anchor" href="#better--faster-large-language-models-via-multi-token-prediction">#</a>
</h1>
<p><em>Posted by Jinoh Cho and Seonghyeon Park</em></p>
<ul>
<li>Authors: Gloeckle et al.</li>
<li>Institution : FAIR at Meta, CERMICS Ecole des Ponts ParisTech and LISN Universite Paris-Saclay</li>
</ul>
<h1 id="preliminaries">
  Preliminaries
  <a class="anchor" href="#preliminaries">#</a>
</h1>
<h3 id="language-modeling-and-next-token-prediction-task">
  Language Modeling and Next-Token Prediction Task
  <a class="anchor" href="#language-modeling-and-next-token-prediction-task">#</a>
</h3>
<p>Learning through a next-token prediction task has been a mainstream for language modeling. The goal of a next-token prediction task is to maximize the probability of the next token $x_{t+1}$, given the history of previous tokens $x_{t:1} = x_1, \ldots, x_t$. This can be formulated as follow:</p>
<p>$$
L_1 = - \sum_{t} \log P_{\theta}(x_{t+1} \mid x_{t:1}),
$$</p>
<p>where $P_{\theta}$ represents large language model under training.</p>
<h1 id="core-idea">
  Core Idea
  <a class="anchor" href="#core-idea">#</a>
</h1>
<h3 id="multi-token-prediction-task">
  Multi-Token Prediction Task
  <a class="anchor" href="#multi-token-prediction-task">#</a>
</h3>
<p>In this work, authors propose to learn language modeling from a multi-token prediction rather than a next-token prediction. At each position of the training corpus, the model is instructed to predict $n$ future tokens at once. Thus, the training objective is changed as follow:</p>
<p>$$
L_n = - \sum_{t} \log P_{\theta}(x_{t+n:t+1} \mid x_{t:1}) = - \sum_{t}\sum_{i=1}^{n} \log P_{\theta}(x_{t+i} \mid x_{t:1}).
$$</p>
<h3 id="memory-efficient-implementation">
  Memory-Efficient Implementation
  <a class="anchor" href="#memory-efficient-implementation">#</a>
</h3>
<p>Directly training language models by minimizing the multi-token prediction loss could result in high GPU memory usage, severly limiting the allowable batch-size. Thus, authors propose to carefully adapt the sequence of forward and backward operations for each prediction head rather than operating forward and backword operations simultaneusly for all heads. This could result in reducing peak GPU memory usage $O(nV+d)$ into $O(V+d)$. Here, the $n$ and $V$ denote the number of head and vocabulary size, respectively. Note that $d$ is the vector dimension of shared transformer trunk.</p>
<p align="center">
    <img src='./Memory Efficient.png' width="800">
</p>
<h3 id="faster-inference-with-self-speculative-decoding">
  Faster Inference with Self-Speculative Decoding
  <a class="anchor" href="#faster-inference-with-self-speculative-decoding">#</a>
</h3>
<p>For speed up in inference time, authors utilize self-speculative decoding (Stern et al., 2018) scheme. Specifically, instead of iteratively predicting a next single token for the given token sequence, authors directly generate n-token using n independent output heads in a single step. This significantly speed up the decoding stage.</p>
<p align="center">
    <img src='./Faster Inference.png' width="800">
</p>
<h1 id="result">
  Result
  <a class="anchor" href="#result">#</a>
</h1>
<h1 id="why-does-it-work">
  Why does it work?
  <a class="anchor" href="#why-does-it-work">#</a>
</h1>
<h1 id="conclusion">
  Conclusion
  <a class="anchor" href="#conclusion">#</a>
</h1>
<h1 id="discussion">
  Discussion
  <a class="anchor" href="#discussion">#</a>
</h1>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">


  <div><a class="flex align-center" href="https://github.com/effml-postech/blog-post/commit/e8c4df15a000f938148379a25d2a78760aa46ed0" title='Last modified by Jinoh Cho | May 22, 2024' target="_blank" rel="noopener">
      <img src="/svg/calendar.svg" class="book-icon" alt="" />
      <span>May 22, 2024</span>
    </a>
  </div>



  <div>
    <a class="flex align-center" href="https://github.com/effml-postech/blog-post/edit/main//content/docs/spring24/04_/_index.md" target="_blank" rel="noopener">
      <img src="/svg/edit.svg" class="book-icon" alt="" />
      <span>Edit this page</span>
    </a>
  </div>


</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#language-modeling-and-next-token-prediction-task">Language Modeling and Next-Token Prediction Task</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#multi-token-prediction-task">Multi-Token Prediction Task</a></li>
        <li><a href="#memory-efficient-implementation">Memory-Efficient Implementation</a></li>
        <li><a href="#faster-inference-with-self-speculative-decoding">Faster Inference with Self-Speculative Decoding</a></li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












