<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="ViTAR: Vision Transformer with Any Resolution # Authors: Qihang Fan, Quanzeng You, Xiaotian Han, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang
Posted by Jungwon Lee, Minsang Seok
What is Vision Transformer? # Vision Transformer (ViT) is an innovative approach to computer vision that leverages the principles of the Transformer architecture, which was originally designed for natural language processing tasks. ViT has recently emerged as a competitive alternative to Convolutional Neural Networks (CNNs) that are currently state-of-the-art in different image recognition computer vision tasks.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/spring24/18_/">
  <meta property="og:site_name" content="Efficient ML Systems">
  <meta property="og:title" content="Efficient ML Systems">
  <meta property="og:description" content="ViTAR: Vision Transformer with Any Resolution # Authors: Qihang Fan, Quanzeng You, Xiaotian Han, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang
Posted by Jungwon Lee, Minsang Seok
What is Vision Transformer? # Vision Transformer (ViT) is an innovative approach to computer vision that leverages the principles of the Transformer architecture, which was originally designed for natural language processing tasks. ViT has recently emerged as a competitive alternative to Convolutional Neural Networks (CNNs) that are currently state-of-the-art in different image recognition computer vision tasks.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="website">
<title>18 | Efficient ML Systems</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" >
<link rel="canonical" href="http://localhost:1313/docs/spring24/18_/">
<link rel="stylesheet" href="/book.min.309b7ed028807cdb68d8d61e26d609f48369c098dbf5e4d8c0dcf4cdf49feafc.css" integrity="sha256-MJt&#43;0CiAfNto2NYeJtYJ9INpwJjb9eTYwNz0zfSf6vw=" crossorigin="anonymous">
  <script defer src="/flexsearch.min.js"></script>
  <script defer src="/en.search.min.19047dfa29663a6878455c545bcb4afbbab899bcc70e677aaf0cfc5bb0392959.js" integrity="sha256-GQR9&#43;ilmOmh4RVxUW8tK&#43;7q4mbzHDmd6rwz8W7A5KVk=" crossorigin="anonymous"></script>

  

<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/docs/spring24/18_/index.xml" title="Efficient ML Systems" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>Efficient ML Systems</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>












  



  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-8c2bdf4d8f82cc2d8f16541464d61b3d" class="toggle" checked />
    <label for="section-8c2bdf4d8f82cc2d8f16541464d61b3d" class="flex justify-between">
      <a role="button" class="">Spring24</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/00_taco_example/" class="">00 Taco Example</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/01_/" class="">01</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/02_/" class="">02</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/03_/" class="">03</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/04_/" class="">04</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/05_/" class="">05</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/06_/" class="">06</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/07_/" class="">07</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/08_/" class="">08</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/09_/" class="">09</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/10_/" class="">10</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/11_/" class="">11</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/12_/" class="">12</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/13_/" class="">13</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/14_/" class="">14</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/15_/" class="">15</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/16_/" class="">16</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/17_/" class="">17</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/18_/" class="active">18</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/19_/" class="">19</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/20_/" class="">20</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/21_/" class="">21</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/22_/" class="">22</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/23_/" class="">23</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/24_/" class="">24</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/25_/" class="">25</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>















</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>18</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#what-is-vision-transformer">What is Vision Transformer?</a>
      <ul>
        <li><a href="#key-components-of-vit">Key Components of ViT</a></li>
      </ul>
    </li>
    <li><a href="#challenge-multi-resolution-vit-modeling">Challenge: Multi-Resolution ViT Modeling</a></li>
    <li><a href="#vitar-vision-transformer-with-any-resolution-1">ViTAR: Vision Transformer with Any Resolution</a>
      <ul>
        <li><a href="#1-adaptive-token-merger-atm-module">1. Adaptive Token Merger (ATM Module)</a></li>
        <li><a href="#2-fuzzy-positional-encoding-fpe">2. Fuzzy Positional Encoding (FPE)</a></li>
      </ul>
    </li>
    <li><a href="#vitar-shows-superior-performance-with-any-resolution">ViTAR shows superior performance with any resolution</a>
      <ul>
        <li><a href="#image-classification">Image Classification</a></li>
        <li><a href="#object-detection">Object Detection</a></li>
      </ul>
    </li>
    <li><a href="#discussion">Discussion</a>
      <ul>
        <li><a href="#applicability-to-diffusion-models">Applicability to Diffusion Models</a></li>
        <li><a href="#applicability-to-large-language-models-llms">Applicability to Large Language Models (LLMs)</a></li>
        <li><a href="#can-grid-attention-replace-convolution">Can Grid Attention Replace Convolution?</a></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="vitar-vision-transformer-with-any-resolution">
  <strong>ViTAR: Vision Transformer with Any Resolution</strong>
  <a class="anchor" href="#vitar-vision-transformer-with-any-resolution">#</a>
</h1>
<p><em>Authors: Qihang Fan, Quanzeng You, Xiaotian Han, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang</em></p>
<p><em>Posted by Jungwon Lee, Minsang Seok</em></p>
<hr>
<h2 id="what-is-vision-transformer">
  What is Vision Transformer?
  <a class="anchor" href="#what-is-vision-transformer">#</a>
</h2>
<p>Vision Transformer (ViT) is an innovative approach to computer vision that leverages the principles of the Transformer architecture, which was originally designed for natural language processing tasks. ViT has recently emerged as a competitive alternative to Convolutional Neural Networks (CNNs) that are currently state-of-the-art in different image recognition computer vision tasks.</p>
<p align="center">
  <img src="./ViT.png" alt="." width="700" height="400" > 
</p>
<p>Vision Transformer architecture consists of a series of Transformer blocks, each containing a multi-head self-attention layer and a feed-forward layer. This structure allows ViT to capture complex relationships within an image more effectively than traditional convolutional layers.</p>
<h3 id="key-components-of-vit">
  Key Components of ViT
  <a class="anchor" href="#key-components-of-vit">#</a>
</h3>
<p>The key coomponents of ViT are described below:</p>
<h4 id="a-patch-embedding">
  A. Patch Embedding
  <a class="anchor" href="#a-patch-embedding">#</a>
</h4>
<ul>
<li>Instead of processing the entire image as a whole, ViT divides the input image into fixed-size patches (e.g., 16x16 pixels).
Each patch is then flattened into a single vector, essentially treating each patch as a &ldquo;token&rdquo; similar to how words are treated in text processing. These flattened patch vectors are linearly projected to a desired embedding dimension. This projection helps in transforming the patches into a suitable format for the Transformer model.</li>
</ul>
<h4 id="b-positional-encoding">
  B. Positional Encoding
  <a class="anchor" href="#b-positional-encoding">#</a>
</h4>
<ul>
<li>Since Transformers are permutation-invariant and do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings. These encodings provide information about the position of each patch in the original image.</li>
</ul>
<h4 id="c-self-attention">
  C. Self Attention
  <a class="anchor" href="#c-self-attention">#</a>
</h4>
<ul>
<li>
<p>The self-attention layer calculates attention weights for each pixel in the image based on its relationship with all other pixels.</p>
</li>
<li>
<p>For each input vector X, three new vectors are created through learned linear transformations: Query (Q), Key (K), and Value (V), where 
<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \(W_{Q}, W_{K}, W_{V}\)
</span>
 are learnd weight matrices.</p>
</li>
</ul>
<span>
  \[Q = XW_Q, K=XW_K, V=XW_V\]
</span>

<ul>
<li>The attention score for each pair of input vectors is calculated using the dot product of their Query and Key vectors:</li>
</ul>
<span>
  \[Attention Score = Q K^T\]
</span>

<ul>
<li>These scores indicate how much focus the model should place on one part of the input when considering another part.</li>
</ul>
<span>
  \[Attention Output = softmax(\frac{Q \dot K^T}{\sqrt{d_k}}V)\]
</span>

<ul>
<li>The attention scores are scaled by the square root of the dimensionality of the Key vectors to prevent excessively large values that could destabilize training. The scaled attention scores are passed through a softmax function to obtain the attention weights. This ensures that the weights are normalized (summing to one) and highlight the relative importance of each input vector. Each input vector is then updated by computing a weighted sum of the Value vectors, using the attention weights.</li>
</ul>
<h4 id="c-multi-head-self-attention-mhsa">
  C. Multi-Head Self Attention (MHSA)
  <a class="anchor" href="#c-multi-head-self-attention-mhsa">#</a>
</h4>
<ul>
<li>The multi-head attention extends self-attention mechanism by allowing the model to attend to different parts of the input sequence simultaneously. Each &ldquo;head&rdquo; in the multi-head attention mechanism can capture different features, leading to a richer and more nuanced representation of the image.</li>
</ul>
<h4 id="d-feedforward-neural-networks">
  D. Feedforward Neural Networks:
  <a class="anchor" href="#d-feedforward-neural-networks">#</a>
</h4>
<ul>
<li>Each self-attention layer is followed by a feedforward neural network that further processes the information.
These networks consist of fully connected layers and typically include activation functions and normalization.</li>
</ul>
<p>If interested in more details about ViT, please refer to the following paper. &ldquo;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&rdquo;</p>
<hr>
<h2 id="challenge-multi-resolution-vit-modeling">
  Challenge: Multi-Resolution ViT Modeling
  <a class="anchor" href="#challenge-multi-resolution-vit-modeling">#</a>
</h2>
<p>Shortcoming of ViT is revealed when receiving multi-resolution images as input. There are limits to its application in actual use environments because ViT cannot process images of various resolutions well.</p>
<p>The most common method used to address this problem is to apply interpolation to positional encoding before feeding it into the ViT. This approach allows for some compensation of positional information even when the input resolution changes. However, this method has shown significant performance degradation in image classification tasks.</p>
<p>Recently, ResFormer proposed adding depth-wise convolution to the existing positional encoding method when performing global-local positional embedding, enabling it to work well even with unseen resolutions. (Chu et al., 2023; Tian et al., 2023).</p>
<p align="center">
  <img src="./ResFormer_pe.png" alt="." width="600" height="350" > 
</p>
<p>However, ResFormer has three drawbacks.</p>
<ul>
<li>Shows high performance only in a relatively small range of resolutions (Degradation significantly when resolution is greater than 892)</li>
<li>It cannot be used with self-supervised learning methods like masked auto-encoding (MAE).</li>
<li>Computation cost increases as input resolution increases, which has a negative impact on the training and inference process.</li>
</ul>
<hr>
<h2 id="vitar-vision-transformer-with-any-resolution-1">
  ViTAR: Vision Transformer with Any Resolution
  <a class="anchor" href="#vitar-vision-transformer-with-any-resolution-1">#</a>
</h2>
<p align="center">
  <img src="./ViTAR_overall.png" alt="." width="600" height="200" > 
</p>
<p>To address this issue, ViTAR introduces two key innovations.</p>
<ul>
<li>
<ol>
<li>Adaptive Token Merger : A novel module for dynamic resolution adjustment, designed with a single Transformer block to achieve highly efficient incremental token integration.</li>
</ol>
</li>
<li>
<ol start="2">
<li>Fuzzy positional encoding : A novel positional encoding to ensure consistent positional awareness across multiple resolutions, thereby preventing overfitting to any specific training resolution.</li>
</ol>
</li>
</ul>
<hr>
<h3 id="1-adaptive-token-merger-atm-module">
  1. Adaptive Token Merger (ATM Module)
  <a class="anchor" href="#1-adaptive-token-merger-atm-module">#</a>
</h3>
<p align="center">
  <img src="./ATM.png" alt="." width="500" height="300" > 
</p>    
<p>Adaptive Token Merger (ATM) module is designed to efficiently process and merge tokens of different resolutions in a neural network using a simple structure that includes GridAttention and FeedForward network (FFN). ATM Module takes tokens processed through patch embedding as input. ATM Module specially processes the inputs of different resolutions M times to reduce them to the same preset size <span>
  \(G_{h} \times G_{w}\)
</span>
 before fed into the MHSA.</p>
<p align="center">
  <img src="./grid_attention.png" alt="." width="600" height="300" > 
</p>
<p>The detailed process for ATM is as follows:</p>
<p>First, ATM divides the tokens of shape <span>
  \((H \times W)\)
</span>
 into a grid of size <span>
  \(G_{th} \times G_{tw}\)
</span>
.</p>
<p>For simplicity, we&rsquo;ll use above Figure as an example.
In the figure, we can see <span>
  \(H=4\)
</span>
, <span>
  \(W=4\)
</span>
, <span>
  \(G_{th}=2\)
</span>
, and <span>
  \(G_{tw}=2\)
</span>
.(We assume that H is divisible by <span>
  \(G_{th}=2\)
</span>
 and W is divisible by <span>
  \(G_{tw}=2\)
</span>
. The number of tokens in each grid would then be <span>
  \(H/G_{th} × W/G_{tw}\)
</span>
, which is 2x2.</p>
<p>Within each grid, the module performs a special operation called Grid Attention.</p>
<h4 id="gridattention">
  GridAttention
  <a class="anchor" href="#gridattention">#</a>
</h4>
<p>For a specific grid, we suppose its tokens are denoted as <span>
  \({x_{ij}}\)
</span>
, where <span>
  \(0 \leq i &lt; H/G_{th}\)
</span>
 and <span>
  \(0 \leq j &lt; W/G_{tw}\)
</span>
.</p>
<ul>
<li>Average Pooling: First, it averages the tokens within a grid to create a mean token.</li>
<li>Cross-Attention: Using this mean token as the Query, and all the grid tokens as Key and Value, it applies cross-attention to merge all tokens in the grid into a single token.</li>
</ul>
<span>
  \[x_{avg} = AvgPool(\{x_{ij}\})\]
</span>

<span>
  \[GridAttn(\{x_{ij}\}) = x_{avg} &#43; Attn(x_{avg}, \{x_{ij}\}, \{x_{ij}\})\]
</span>

<p>After passing through GridAttention, the fused token is fed into a standard Feed-Forward Network to complete channel fusion, thereby completing one iteration of merging token. GridAttention and FFN undergo multiple iterations and all iterations share the same weights.</p>
<p>During these iterations, we gradually decrease the value of <span>
  \((G_{th} , G_{tw})\)
</span>
, until <span>
  \(G_{th} = G_{h}\)
</span>
 and <span>
  \(G_{tw} = G_{w}\)
</span>
. (typically set <span>
  \(G_h = G_w = 14\)
</span>
, in standard ViT)</p>
<p>This iteration process effectively reduces the number of tokens even when the resolution of the image is large, and with enough iterations, this size can be reduced effectively. This has the advantage of being computationally efficient because when performing subsequent MHSA calculations, we always use the same size tokens as input, regardless of resolution.</p>
<hr>
<p>For Ablation study, ViTAR-S Model is used to compare with AvgPool which is another token fusion method. The results of the comparison demonstrate that ATM significantly improves the model&rsquo;s performance and resolution adaptability. Specifically, at a resolution of 4032, our proposed ATM achieves a 7.6% increase in accuracy compared with the baseline.</p>
<p align="center">
  <img src="./result_ATM.png" alt="." width="600" height="250" > 
</p>
<hr>
<h3 id="2-fuzzy-positional-encoding-fpe">
  2. Fuzzy Positional Encoding (FPE)
  <a class="anchor" href="#2-fuzzy-positional-encoding-fpe">#</a>
</h3>
<p>Existing ViT Models generally use learnable positional encoding or sin-cos positional encoding. However, they do not have the ability to handle various input resolutions because these methods are sensitive to input resolution. In response to this, ResFormer attempted to solve this problem through convolution-based positional embedding.</p>
<p>However, convolution-based positional embedding is not suitable for use in self-supervised learning such as masked auto-encoding (MAE). This is because the method can extract and utilize the complete spatial feature only if it has all adjacent patches, but in the case of MAE, some of the image patches are masked. This makes it difficult for the model to conduct large-scale learning.</p>
<p>Fuzzy Positional Encoding(FPE) differs from the previously mentioned methods. It enhances the model&rsquo;s resolution robustness without introducing specific spatial structures like convolutions. Therefore, it can be applied to self-supervised learning frameworks. This property enables ViTAR to be applied to large-scale, unlabeled training sets for training, aiming to obtain a more powerful vision foundation model.</p>
<p align="center">
  <img src="./FPE.png" alt="." width="500" height="300" > 
</p>
<p>Initially, the learnable positional embedding is randomly initialized and used as the model&rsquo;s positional embedding. At this time, FPE provides only fuzzy positional information and experiences changes within a certain range. Specifically, assuming that the exact coordinates of the target token are (i, j), the fuzzy positional information is (i + s1, j + s2). s1 and s2 satisfy -0.5 ≤ s1, s2 ≤ 0.5 and follows uniform distribution.</p>
<p>During training, randomly generated coordinate offsets are added to the reference coordinates during the training process, and grid samples for learnable location embeddings are performed based on the newly generated coordinates to generate fuzzy location encoding.</p>
<p>In case of inference, precise positional encoding is used instead of FPE. When there is a change in input resolution, interpolation is performed on learnable positional embedding. This has strong positional resilience because it was somehow seen and used in the FPE used in the training phase.</p>
<hr>
<p>To compare the impact of different positional encodings on the model’s resolution generalization ability, several positional encoding methods were used. This includes commonly used sin-cos absolute position encoding (APE), conditional position encoding (CPE), global-local positional encoding (GLPE) in ResFormer, Relative Positional Bias (RPB) in Swin, and FPE. Note that only APE and FPE are compatible with the MAE framework.ViTAR-S is used for experiments without MAE, and ViTAR-M is used for experiments with MAE. As a result, FPE exhibits a significantly pronounced advantage in resolution generalization capability. Additionally, under the MAE self-supervised learning framework, FPE also demonstrates superior performance relative to APE.</p>
<p align="center">
  <img src="./result_FPE.png" alt="." width="400" height="300" > 
</p>
<hr>
<h2 id="vitar-shows-superior-performance-with-any-resolution">
  ViTAR shows superior performance with any resolution
  <a class="anchor" href="#vitar-shows-superior-performance-with-any-resolution">#</a>
</h2>
<h3 id="image-classification">
  Image Classification
  <a class="anchor" href="#image-classification">#</a>
</h3>
<p>ViTAR is trained on ImageNet-1K form scratch and it demonstrates excellent classification accuracy across a considerable range of resolutions. Especially, when the resolution of the input image exceeds 2240, ViTAR is capable of inference at lower computational cost. In contrast, traditional ViT architectures (DeiT and ResFormer) cannot perform high resolution inference due to computational resource limitations.</p>
<p align="center">
  <img src="./result_image_classification.png" alt="." width="600" height="600" > 
</p>
<p>As can be seen in the pareto frontier figure, ViTAR has high performance for various resolution images and can also be used for high resolution images of 2240 or higher.</p>
<p align="center">
  <img src="./result1.png" alt="." width="500" height="300" > 
</p>
<h3 id="object-detection">
  Object Detection
  <a class="anchor" href="#object-detection">#</a>
</h3>
<p>For object detection, COCO dataset is used ATM iterates only once because it does not utilize the multi-resolution training strategy in this experiment. If <span>
  \(\frac{H}{G_{th}}\)
</span>
 and <span>
  \(\frac{w}{G_{tw}}\)
</span>
 in ATM are fixed to 1, the results indicate that ViTAR achieves performance in both object detection and instance segmentation. And if setting <span>
  \(\frac{H}{G_{th}}\)
</span>
 and <span>
  \(\frac{w}{G_{tw}}\)
</span>
 to 2 in ATM, ATM module reduces approximately 50% of the computational cost while maintaining high precision in dense predictions, demonstrating its effectiveness.</p>
<p align="center">
  <img src="./result_object_detection.png" alt="." width="800" height="350" > 
</p>
<hr>
<h2 id="discussion">
  Discussion
  <a class="anchor" href="#discussion">#</a>
</h2>
<h3 id="applicability-to-diffusion-models">
  Applicability to Diffusion Models
  <a class="anchor" href="#applicability-to-diffusion-models">#</a>
</h3>
<ul>
<li>It is currently challenging to generate images of various resolutions with generative models like Diffusion Models. Additionally, many diffusion models with ViT structures have been proposed recently (e.g. DiT, PixArt-α, Sora). Can the proposed method be applied to Diffusion Models as well? However, one consideration for applying it to diffusion models is how to effectively upscale the reduced size obtained through Grid Attention to ensure that the input and output sizes are the same.</li>
</ul>
<h3 id="applicability-to-large-language-models-llms">
  Applicability to Large Language Models (LLMs)
  <a class="anchor" href="#applicability-to-large-language-models-llms">#</a>
</h3>
<ul>
<li>In LLMs, when receiving long context as input, positional embeddings are sometimes added using interpolation like this case. Would applying Fuzzy Positional Embedding (FPE) help handle long context inputs better? Or, just like training a network on low-resolution images to perform well on high-resolution images, can a network trained on short context in LLM maintain good performance on long context input?</li>
</ul>
<h3 id="can-grid-attention-replace-convolution">
  Can Grid Attention Replace Convolution?
  <a class="anchor" href="#can-grid-attention-replace-convolution">#</a>
</h3>
<ul>
<li>The operation of GridAttention is quite similar to the process performed by kernels in Convolution when calculating each grid. However, ATM maintains parameter efficiency by sharing weights. We expect that applying GridAttention to existing CNN structures (e.g., VGG, ResNet) will allow us to design more efficient architectures.</li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">


  <div><a class="flex align-center" href="https://github.com/effml-postech/blog-post/commit/ab2ac28d8751e054faee23f20940d81b327df9e2" title='Last modified by postech-sms | May 22, 2024' target="_blank" rel="noopener">
      <img src="/svg/calendar.svg" class="book-icon" alt="" />
      <span>May 22, 2024</span>
    </a>
  </div>



  <div>
    <a class="flex align-center" href="https://github.com/effml-postech/blog-post/edit/main//content/docs/spring24/18_/_index.md" target="_blank" rel="noopener">
      <img src="/svg/edit.svg" class="book-icon" alt="" />
      <span>Edit this page</span>
    </a>
  </div>


</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#what-is-vision-transformer">What is Vision Transformer?</a>
      <ul>
        <li><a href="#key-components-of-vit">Key Components of ViT</a></li>
      </ul>
    </li>
    <li><a href="#challenge-multi-resolution-vit-modeling">Challenge: Multi-Resolution ViT Modeling</a></li>
    <li><a href="#vitar-vision-transformer-with-any-resolution-1">ViTAR: Vision Transformer with Any Resolution</a>
      <ul>
        <li><a href="#1-adaptive-token-merger-atm-module">1. Adaptive Token Merger (ATM Module)</a></li>
        <li><a href="#2-fuzzy-positional-encoding-fpe">2. Fuzzy Positional Encoding (FPE)</a></li>
      </ul>
    </li>
    <li><a href="#vitar-shows-superior-performance-with-any-resolution">ViTAR shows superior performance with any resolution</a>
      <ul>
        <li><a href="#image-classification">Image Classification</a></li>
        <li><a href="#object-detection">Object Detection</a></li>
      </ul>
    </li>
    <li><a href="#discussion">Discussion</a>
      <ul>
        <li><a href="#applicability-to-diffusion-models">Applicability to Diffusion Models</a></li>
        <li><a href="#applicability-to-large-language-models-llms">Applicability to Large Language Models (LLMs)</a></li>
        <li><a href="#can-grid-attention-replace-convolution">Can Grid Attention Replace Convolution?</a></li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












