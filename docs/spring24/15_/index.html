<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Training LLMs over Neurally Compressed Text # TL;DR # This paper addresses the problem of training large language models (LLMs) using highly compressed text, which offers several advantages: (i) faster training, (ii) improved serving efficiency, and (iii) easier handling of long text spans. However, strong compression can sometimes produce opaque outputs and difficult to use for learning.
To overcome this, Equal-Info Windows is proposed, a novel compression technique where text is segmented into blocks which are each compressed in to the same bit length.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/spring24/15_/">
  <meta property="og:site_name" content="Efficient ML Systems">
  <meta property="og:title" content="Efficient ML Systems">
  <meta property="og:description" content="Training LLMs over Neurally Compressed Text # TL;DR # This paper addresses the problem of training large language models (LLMs) using highly compressed text, which offers several advantages: (i) faster training, (ii) improved serving efficiency, and (iii) easier handling of long text spans. However, strong compression can sometimes produce opaque outputs and difficult to use for learning.
To overcome this, Equal-Info Windows is proposed, a novel compression technique where text is segmented into blocks which are each compressed in to the same bit length.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="website">
<title>15 | Efficient ML Systems</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" >
<link rel="canonical" href="http://localhost:1313/docs/spring24/15_/">
<link rel="stylesheet" href="/book.min.309b7ed028807cdb68d8d61e26d609f48369c098dbf5e4d8c0dcf4cdf49feafc.css" integrity="sha256-MJt&#43;0CiAfNto2NYeJtYJ9INpwJjb9eTYwNz0zfSf6vw=" crossorigin="anonymous">
  <script defer src="/flexsearch.min.js"></script>
  <script defer src="/en.search.min.69d539257fd7dad2bc289dd230cf62a9dc850368a661706e7ea773611e9e96b7.js" integrity="sha256-adU5JX/X2tK8KJ3SMM9iqdyFA2imYXBufqdzYR6elrc=" crossorigin="anonymous"></script>

  

<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/docs/spring24/15_/index.xml" title="Efficient ML Systems" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>Efficient ML Systems</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>












  



  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-8c2bdf4d8f82cc2d8f16541464d61b3d" class="toggle" checked />
    <label for="section-8c2bdf4d8f82cc2d8f16541464d61b3d" class="flex justify-between">
      <a role="button" class="">Spring24</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/00_taco_example/" class="">00 Taco Example</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/01_/" class="">01</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/02_/" class="">02</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/03_/" class="">03</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/04_/" class="">04</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/05_/" class="">05</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/06_/" class="">06</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/07_/" class="">07</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/08_/" class="">08</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/09_/" class="">09</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/10_/" class="">10</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/11_/" class="">11</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/12_/" class="">12</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/13_/" class="">13</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/14_/" class="">14</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/15_/" class="active">15</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/16_/" class="">16</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/17_/" class="">17</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/18_/" class="">18</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/19_/" class="">19</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/20_/" class="">20</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/21_/" class="">21</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/22_/" class="">22</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/23_/" class="">23</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/24_/" class="">24</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/25_/" class="">25</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>















</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>15</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#tldr"><strong>TL;DR</strong></a></li>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#large-language-models-llms-and-compression">Large Language Models (LLMs) and compression</a></li>
        <li><a href="#motivation">Motivation</a></li>
        <li><a href="#challenges-of-llms-over-compressed-text">Challenges of LLMs over compressed text</a></li>
      </ul>
    </li>
    <li><a href="#method">Method</a>
      <ul>
        <li><a href="#training-data">Training data</a></li>
        <li><a href="#training-m1">Training M1</a></li>
        <li><a href="#compression">Compression</a></li>
        <li><a href="#tokenization-of-compressed-text">Tokenization of compressed text</a></li>
        <li><a href="#training-m2">Training M2</a></li>
      </ul>
    </li>
    <li><a href="#experiments">Experiments</a>
      <ul>
        <li><a href="#baselines">Baselines</a></li>
        <li><a href="#evaluation-metrics">Evaluation metrics</a></li>
      </ul>
    </li>
    <li><a href="#results--analysis">Results &amp; analysis</a>
      <ul>
        <li><a href="#equal-info-windows-make-arithmetic-codingac-learnable">Equal-Info Windows make Arithmetic Coding(AC) learnable</a></li>
        <li><a href="#window-size">Window size</a></li>
        <li><a href="#size-of-m2-vocabulary">Size of M2 vocabulary</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a>
      <ul>
        <li><a href="#discussion">Discussion</a></li>
        <li><a href="#future-reserach-direction">Future reserach direction</a></li>
      </ul>
    </li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="training-llms-over-neurally-compressed-text">
  Training LLMs over Neurally Compressed Text
  <a class="anchor" href="#training-llms-over-neurally-compressed-text">#</a>
</h1>
<h2 id="tldr">
  <strong>TL;DR</strong>
  <a class="anchor" href="#tldr">#</a>
</h2>
<p>This paper addresses the problem of <strong>training large language models (LLMs) using highly compressed text</strong>, which offers several advantages: (i) faster training, (ii) improved serving efficiency, and (iii) easier handling of long text spans. However, strong compression can sometimes produce opaque outputs and difficult to use for learning.</p>
<p>To overcome this, <strong>Equal-Info Windows</strong> is proposed, a novel compression technique where text is segmented into blocks which are each compressed in to the same bit length. Experiments demonstrate that this method significantly outperforms <em>byte-level baselines</em> in both (i) perplexity and (ii) inference speed. Although the perplexity is worse than that of <em>subword tokenizers</em>, the method benefits from shorter sequence lengths, leading to reduced latency. Additionally, the paper provides an analysis of properties that contribute to learnability.</p>
<h2 id="introduction">
  Introduction
  <a class="anchor" href="#introduction">#</a>
</h2>
<h3 id="large-language-models-llms-and-compression">
  Large Language Models (LLMs) and compression
  <a class="anchor" href="#large-language-models-llms-and-compression">#</a>
</h3>
<p>LLMs are mostly trained over subword tokens, whereby the tokens are produced by tokenizers are <em>compressors</em> that typically achive ~4 compression. The advantages of using compressed text can be analyzed as the follwoing.</p>
<ol>
<li><strong>Efficiency</strong>
LLMs can process more text for the same computational cost, <em>effectively</em> increasing the amount of data seen during training and improving performance. This efficiency also reduces serving costs and inference latency by requiring fewer sequential autoregressive steps.</li>
<li><strong>Longer context</strong>
LLMs can model longer contextual dependencies by reducing the sequence length, enabling transformers to handle longer contexts efficiently. This extended context is crucial for applications like document retrieval and answering coding questions with provided documentation.</li>
<li><strong>Distribution of Compute</strong>
Information is spread more uniformly across the sequence, meaning each token represents an equal amount of information. This ensures the model allocates more compute to complex text spans, similar to &ldquo;Adaptive Computation Time&rdquo; but with dense, identical operations applied at each position.</li>
</ol>
<h3 id="motivation">
  Motivation
  <a class="anchor" href="#motivation">#</a>
</h3>
<p>Among many compression methods, <a href="#ac">Arithmetic coding (AC)</a> has been known to reach near-optimal compression rate for a particular model. Motivated by this, the authors suggest the following compresion method.</p>
<ol>
<li>A Small language model “M1” is trained over raw byte sequences</li>
<li>A frozen &ldquo;M1&rdquo; is used to compress pretraining corpus text by applying a standard compression algorithm, like AC.</li>
<li>The compressed bitstream is chunked into tokens, which are used to train a second language model “M2”, that directly reads and writes neural-compressed text.</li>
</ol>
<p><img src="./fig1.png" alt="method overview" title="Method overview"/></p>
<h3 id="challenges-of-llms-over-compressed-text">
  Challenges of LLMs over compressed text
  <a class="anchor" href="#challenges-of-llms-over-compressed-text">#</a>
</h3>
<ol>
<li><strong>Learnability</strong>
Strong compression can make the bitstream appear random and difficult to interpret. If M1 compresses too strongly, the bitstream may become too noisy for M2 to detect any meaningful signal. Therefore, M2 must accurately simulate M1&rsquo;s behavior and understand the compression process, which is complex and requires high-precision numerical state management. Also, M2 needs to learn the compression procedure itself.</li>
<li><strong>Numerical stability</strong>
Compression methods can be sensitive to the precise model probabilities used. To achieve lossless compression, it is critical that the probabilities from M1 match exactly during both compression and decompression. However, ensuring this match is difficult in practice due to numerical noise in LLM inference, especially when running on parallel hardware.</li>
<li><strong>Multi-model inference</strong>
For inference, multiple models simultaneously need to run and stored. Assuming M1 is relatively small, this additional overhead may not be a significant drawback compared to a standard tokenizer, which is also a separate model required for tokenizing text input and detokenizing LLM outputs.</li>
</ol>
<h2 id="method">
  Method
  <a class="anchor" href="#method">#</a>
</h2>
<p>Briefly, the training process can be summarized as the following steps.</p>
<ol>
<li>
<h3 id="training-data">
  Training data
  <a class="anchor" href="#training-data">#</a>
</h3>
</li>
</ol>
<p>All training data used is English web text from <a href="#c4">C4 dataset</a>. 128 documents are concatenated to generate a long sequence of text. This results an average length 277,760 bytes (128 documents), whic are split into individual examples and shuffled using the deterministic dataset functionality from <a href="#seqio">SeqIO</a>.</p>
<ol start="2">
<li>
<h3 id="training-m1">
  Training M1
  <a class="anchor" href="#training-m1">#</a>
</h3>
</li>
</ol>
<p>Authors use a decoder-only Transformer model, where the final validation performance of the M1 model is 1.457 bits/byte. Similar to how tokenizers are trained, M1 and M2 are both trained on the C4 training data, but the final validation data used to evaluate M2 is unseen during M1 training.</p>
<ol start="3">
<li>
<h3 id="compression">
  Compression
  <a class="anchor" href="#compression">#</a>
</h3>
</li>
</ol>
<p>Now, modeling compressed text can  be difficult because of language modeling model not able to track the state variables used in Arithmetic Coding.</p>
<p><img src="./fig2.png" alt="equal information window" title="Example of equal information window."/></p>
<p>To weaken the coding component of AC compression, the authors rest the AC encoder once it has output a set number of bits, creating windows of fixed size where each window is an independently AC-compressed sequence.</p>
<p>For example in the figure above, text is encoded into a series of N-bit windows. To determine each successive window, the remaining text is encoded byte-by-byte via Arithmetic Coding until no more bytes can be added without exceeding the target bit threshold, here 16 bits. Both M1 and the AC algorithm are reset at each step, so no information persists across windows.</p>
<ol start="4">
<li>
<h3 id="tokenization-of-compressed-text">
  Tokenization of compressed text
  <a class="anchor" href="#tokenization-of-compressed-text">#</a>
</h3>
</li>
</ol>
<p>Training M2 directly over the bits from the compression method would be not ideal. Therefore, the bitstreams are converted into tokens, using a vocbulary size of $2^N$, i.e., grouping every $N$ bits to a token.</p>
<p>Another critical point to consider it the token compression ratio $L_{iT}/L_{oT}$, the ratio between the input and output token sequence lengths. This metric measures the weakening of Arithmetic coding. Note that the meaning of “token” can differ between the input and output sequences.</p>
<p><img src="./fig3.png" alt="weakening AC" title="Weakening the coding component of Arithmetic coding"/></p>
<ol start="5">
<li>
<h3 id="training-m2">
  Training M2
  <a class="anchor" href="#training-m2">#</a>
</h3>
</li>
</ol>
<p>Finally, the M2 model is trained for 200,000 steps with a batch size of 256 and a sequence length of 512, cumulatively training on 26.2 billion tokens. Models are trained at four sizes with 25M, 113M, 403M, and 2B parameters, excluding embedding parameters.</p>
<h2 id="experiments">
  Experiments
  <a class="anchor" href="#experiments">#</a>
</h2>
<h3 id="baselines">
  Baselines
  <a class="anchor" href="#baselines">#</a>
</h3>
<p>The M2 model is compared with two standard tokenization methods.</p>
<ul>
<li><strong>Bytes</strong>
<ul>
<li>Train directly over UTF-8 bytes</li>
<li>Byte tokenizer from <a href="#byte">ByT5</a></li>
<li>Models sees 26.2 billion bytes total</li>
</ul>
</li>
<li><strong>SetencePiece</strong>
<ul>
<li>Train on tokenized text</li>
<li>SentencePiece vocabulary of 32,000 tokens from <a href="#t5">T5</a>.</li>
<li>Models see 112 billion bytes total</li>
</ul>
</li>
</ul>
<h3 id="evaluation-metrics">
  Evaluation metrics
  <a class="anchor" href="#evaluation-metrics">#</a>
</h3>
<p>One major point is that models cannot be directly compared on “per-token” metrics such as negative log likelihood loss. Rather, following previous works, perplexity in terms of “bits-per-byte”, $[bits/byte] = (L_{oT} /L_{iT} ) \ell / ln(2)$ is used. Models are also compared on how much computation (FLOPs) are required to perform inference over a given length of raw text (bytes). When validating the models, C4 validation set is used. Models are run over 20 batches or ~2.6 million tokens.</p>
<h2 id="results--analysis">
  Results &amp; analysis
  <a class="anchor" href="#results--analysis">#</a>
</h2>
<p>Obviously, simply training over nerual-compressed fails in terms of both bits/byte and inference FLOPs. Moreover, SentencePiece shows quite a impressive performance among the baselines.</p>
<h3 id="equal-info-windows-make-arithmetic-codingac-learnable">
  Equal-Info Windows make Arithmetic Coding(AC) learnable
  <a class="anchor" href="#equal-info-windows-make-arithmetic-codingac-learnable">#</a>
</h3>
<p>EqualInfoAC[b=16, v=256] outperforms byte-level baselines, improving bits/byte performance and reducing FLOPs/byte due to shorter sequence lengths. EqualInfoAC[b=16, v=65k] models achieve competitive bits/byte performance and require fewer autoregressive steps than SentencePiece models, which can reduce generation latency. Despite SentencePiece&rsquo;s slight edge in bits/byte when FLOPs/byte are constant, EqualInfoAC&rsquo;s shorter sequences offer a significant advantage in latency-sensitive applications.
<img src="./fig4.png" alt="Equal-Info Windows" title="Performance of EqualInfoAC across various window sizes"/></p>
<h3 id="window-size">
  Window size
  <a class="anchor" href="#window-size">#</a>
</h3>
<p>Analyzing bits-per-token reveals a discernible pattern: longer window lengths pose greater challenges to learning, resembling the complexity of running Arithmetic Coding over the entire sequence. Shorter window sizes(16-bit windows) yield the best performance in terms of bits/byte. However, longer window sizes(128-bit windows) exhibit better compression rates despite limited learning by the models beyond a uniform distribution.</p>
<h3 id="size-of-m2-vocabulary">
  Size of M2 vocabulary
  <a class="anchor" href="#size-of-m2-vocabulary">#</a>
</h3>
<p>Using a larger 16-bit vocabulary (v=65k) for tokenizing compressed text leads to a doubling in token compression rate, evident from the leftward shift of each curve depicted in the figure below.</p>
<p><img src="./fig5.png" alt="Size of M2 vocabulary" title="Size of M2 vocabulary"/></p>
<h2 id="conclusion">
  Conclusion
  <a class="anchor" href="#conclusion">#</a>
</h2>
<p>To sum up, the paper can be summarized as the following.</p>
<ul>
<li>The naive approach does not work, requiring a realtively simple modiciation.</li>
<li>Compression Equal Info Windows results beter performance than byte-level models in terms of perplexity and inference cost, and close to the performance of SentencePiece tokenization</li>
<li>Worse perplexity than subword tokenizers for models trained with the same parameter count, but offers benefit of shorter sequence lengths.</li>
</ul>
<h3 id="discussion">
  Discussion
  <a class="anchor" href="#discussion">#</a>
</h3>
<p>Though the Equal Info Windows shows quite impressive performance compared to byte-level models, worse preplexity compared to subword tokenizers is critical. Since the proposed methods contains the compression of bytes to tokens, this limitations be more highlighted.</p>
<h3 id="future-reserach-direction">
  Future reserach direction
  <a class="anchor" href="#future-reserach-direction">#</a>
</h3>
<ul>
<li>Variational length compression
Currently, nearly the same length of bits are compressed into tokens. If compresingvariable length of bits to tokens, i.e., dictionary of variable lengths is possible would be a interseting research.</li>
<li>Hierarhical compression
Bits are compress into tokens, and tokens are generated by the M2 model. The concept of using compression multiple times could be extended more, compressing list of tokens into another set of tokens. In other words, a hierarhical compression would also be a interesting research.</li>
</ul>
<h2 id="reference">
  Reference
  <a class="anchor" href="#reference">#</a>
</h2>
<ol>
<li><a name="ac"></a> I. H. Witten, R. M. Neal, and J. G. Cleary. Arithmetic Coding for Data Compression. Communications of The Acm, 30(6):520–540, June 1987</li>
<li><a name="c4"></a> C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research (JMLR 2020), 21(140):1–67, 2020.</li>
<li><a name="seqio"></a> A. Roberts, H. W. Chung, G. Mishra, A. Levskaya, J. Bradbury, D. Andor, S. Narang, B. Lester, C. Gaffney, A. Mohiuddin, C. Hawthorne, A. Lewkowycz, A. Salcianu, M. van Zee, J. Austin, S. Goodman, L. B. Soares, H. Hu, S. Tsvyashchenko, A. Chowdhery, J. Bastings, J. Bulian, X. Garcia, J. Ni, A. Chen, K. Kenealy, K. Han, M. Casbon, J. H. Clark, S. Lee, D. Garrette, J. Lee-Thorp, C. Raffel, N. Shazeer, M. Ritter, M. Bosma, A. Passos, J. Maitin-Shepard, N. Fiedel, M. Omernick, B. Saeta, R. Sepassi, A. Spiridonov, J. Newlan, and A. Gesmundo. Scaling Up Models and Data with t5x and seqio. Journal of Machine Learning Research, 24(377):1–8, 2023.</li>
<li><a name="byte"></a> L. Xue, A. Barua, N. Constant, R. Al-Rfou, S. Narang, M. Kale, A. Roberts, and C. Raffel. ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models. Transactions of the Association for Computational Linguistics, 10:291–306, 2022.</li>
<li><a name="t5"></a> C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research (JMLR 2020), 21(140):1–67, 2020.</li>
</ol>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">


  <div><a class="flex align-center" href="https://github.com/effml-postech/blog-post/commit/2640b0cc920998e9ef31154d1978021323f2bd52" title='Last modified by effml-postech | May 22, 2024' target="_blank" rel="noopener">
      <img src="/svg/calendar.svg" class="book-icon" alt="" />
      <span>May 22, 2024</span>
    </a>
  </div>



  <div>
    <a class="flex align-center" href="https://github.com/effml-postech/blog-post/edit/main//content/docs/spring24/15_/_index.md" target="_blank" rel="noopener">
      <img src="/svg/edit.svg" class="book-icon" alt="" />
      <span>Edit this page</span>
    </a>
  </div>


</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#tldr"><strong>TL;DR</strong></a></li>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#large-language-models-llms-and-compression">Large Language Models (LLMs) and compression</a></li>
        <li><a href="#motivation">Motivation</a></li>
        <li><a href="#challenges-of-llms-over-compressed-text">Challenges of LLMs over compressed text</a></li>
      </ul>
    </li>
    <li><a href="#method">Method</a>
      <ul>
        <li><a href="#training-data">Training data</a></li>
        <li><a href="#training-m1">Training M1</a></li>
        <li><a href="#compression">Compression</a></li>
        <li><a href="#tokenization-of-compressed-text">Tokenization of compressed text</a></li>
        <li><a href="#training-m2">Training M2</a></li>
      </ul>
    </li>
    <li><a href="#experiments">Experiments</a>
      <ul>
        <li><a href="#baselines">Baselines</a></li>
        <li><a href="#evaluation-metrics">Evaluation metrics</a></li>
      </ul>
    </li>
    <li><a href="#results--analysis">Results &amp; analysis</a>
      <ul>
        <li><a href="#equal-info-windows-make-arithmetic-codingac-learnable">Equal-Info Windows make Arithmetic Coding(AC) learnable</a></li>
        <li><a href="#window-size">Window size</a></li>
        <li><a href="#size-of-m2-vocabulary">Size of M2 vocabulary</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a>
      <ul>
        <li><a href="#discussion">Discussion</a></li>
        <li><a href="#future-reserach-direction">Future reserach direction</a></li>
      </ul>
    </li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












