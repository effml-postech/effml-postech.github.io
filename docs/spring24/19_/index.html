<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning # Posted by: Sungbin Shin and Dongyeop Lee
The incredible versatility of large language models (LLM) on a wide variety of tasks has attracted many to leverage its power to perform specific tasks of interest (i.e., downstream task) via a process called fine-tuning, which consists of re-training the whole pretrained network on a new set of data. However, its intensive computation and memory cost have led to the development of efficient techniques, particularly Parameter-Efficient Fine-Tuning (PEFT) approaches such as adapters, prompt weights, and, arguably one of the most widely used approaches, LoRA.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/spring24/19_/">
  <meta property="og:site_name" content="Efficient ML Systems">
  <meta property="og:title" content="Efficient ML Systems">
  <meta property="og:description" content="LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning # Posted by: Sungbin Shin and Dongyeop Lee
The incredible versatility of large language models (LLM) on a wide variety of tasks has attracted many to leverage its power to perform specific tasks of interest (i.e., downstream task) via a process called fine-tuning, which consists of re-training the whole pretrained network on a new set of data. However, its intensive computation and memory cost have led to the development of efficient techniques, particularly Parameter-Efficient Fine-Tuning (PEFT) approaches such as adapters, prompt weights, and, arguably one of the most widely used approaches, LoRA.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="website">
<title>19 | Efficient ML Systems</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" >
<link rel="canonical" href="http://localhost:1313/docs/spring24/19_/">
<link rel="stylesheet" href="/book.min.309b7ed028807cdb68d8d61e26d609f48369c098dbf5e4d8c0dcf4cdf49feafc.css" integrity="sha256-MJt&#43;0CiAfNto2NYeJtYJ9INpwJjb9eTYwNz0zfSf6vw=" crossorigin="anonymous">
  <script defer src="/flexsearch.min.js"></script>
  <script defer src="/en.search.min.0f57e94bc0c8098ee323e2a76a1bd56be5109b0347f202135049ff3246b6b4a3.js" integrity="sha256-D1fpS8DICY7jI&#43;KnahvVa&#43;UQmwNH8gITUEn/Mka2tKM=" crossorigin="anonymous"></script>

  

<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/docs/spring24/19_/index.xml" title="Efficient ML Systems" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>Efficient ML Systems</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>












  



  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-8c2bdf4d8f82cc2d8f16541464d61b3d" class="toggle" checked />
    <label for="section-8c2bdf4d8f82cc2d8f16541464d61b3d" class="flex justify-between">
      <a role="button" class="">Spring24</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/00_taco_example/" class="">00 Taco Example</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/01_/" class="">01</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/02_/" class="">02</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/03_/" class="">03</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/04_/" class="">04</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/05_/" class="">05</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/06_/" class="">06</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/07_/" class="">07</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/08_/" class="">08</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/09_/" class="">09</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/10_/" class="">10</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/11_/" class="">11</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/12_/" class="">12</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/13_/" class="">13</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/14_/" class="">14</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/15_/" class="">15</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/16_/" class="">16</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/17_/" class="">17</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/18_/" class="">18</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/19_/" class="active">19</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/20_/" class="">20</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/21_/" class="">21</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/22_/" class="">22</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/23_/" class="">23</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/24_/" class="">24</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/25_/" class="">25</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>















</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>19</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#background">Background</a>
      <ul>
        <li><a href="#parameter-efficient-fine-tuning">Parameter-Efficient Fine-Tuning</a></li>
        <li><a href="#low-rank-adaptation-lora">Low-Rank Adaptation (LoRA)</a></li>
        <li><a href="#large-scale-optimization-algorithms">Large-scale Optimization Algorithms</a></li>
      </ul>
    </li>
    <li><a href="#method">Method</a>
      <ul>
        <li><a href="#motivation-training-pattern-of-lora">Motivation: training pattern of LoRA</a></li>
        <li><a href="#layerwise-importance-sampled-adamw-lisa">Layerwise Importance Sampled AdamW (LISA)</a></li>
      </ul>
    </li>
    <li><a href="#experiments">Experiments</a>
      <ul>
        <li><a href="#efficiency">Efficiency</a></li>
        <li><a href="#performance">Performance</a></li>
        <li><a href="#ablation-study-on-hyperparameter">Ablation study on hyperparameter</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="lisa-layerwise-importance-sampling-for-memory-efficient-large-language-model-fine-tuning">
  LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
  <a class="anchor" href="#lisa-layerwise-importance-sampling-for-memory-efficient-large-language-model-fine-tuning">#</a>
</h1>
<p>Posted by: Sungbin Shin and Dongyeop Lee</p>
<p>The incredible versatility of large language models (LLM) on a wide variety of tasks has attracted many to leverage its power to perform specific tasks of interest (i.e., downstream task) via a process called fine-tuning, which consists of re-training the whole pretrained network on a new set of data.
However, its intensive computation and memory cost have led to the development of efficient techniques, particularly Parameter-Efficient Fine-Tuning (PEFT) approaches such as adapters, prompt weights, and, arguably one of the most widely used approaches, LoRA.</p>
<p>However, LoRA&rsquo;s performance has yet reach the level of full fine-tuning in all settings.
To overcome this shortcoming, this work investigates into the training statistics of LoRA in each layer to discover that the distribution of layerwise weight norms of LoRA is uncommonly skewed, suggesting varying levels of importance of each layer.</p>
<p>This naturally brings forth their <strong>L</strong>ayerwise <strong>I</strong>mportance <strong>S</strong>ampled <strong>A</strong>dam (<strong>LISA</strong>), which suggests selectively updating only the essential layers of the LLM sampled based on their importance and leaving others untouched.
This allows for the performance of traditional full-parameter fine-tuning with the parameter count of LoRA, indicating a large potential of LISA as a promising alternative to LoRA.</p>
<h2 id="background">
  Background
  <a class="anchor" href="#background">#</a>
</h2>
<h3 id="parameter-efficient-fine-tuning">
  Parameter-Efficient Fine-Tuning
  <a class="anchor" href="#parameter-efficient-fine-tuning">#</a>
</h3>
<p>Parameter-efficient fine-tuning (PEFT) methods adapt pre-trained models by fine-tuning only a subset of their parameters. These methods fall into three categories:</p>
<ul>
<li>
<p>Prompt Learning Methods: Focus on optimizing the input token or embedding while keeping the model parameters frozen. This approach generally has the lowest training cost.</p>
</li>
<li>
<p>Adapter Methods: Introduce an auxiliary module with fewer parameters than the original model. Only the adapter module is updated during training.</p>
</li>
<li>
<p>Selective Methods: These methods, closely related to LISA (Layerwise Importance Sampled AdamW), focus on fine-tuning a fraction of the model&rsquo;s parameters without adding extra modules. They include techniques like AutoFreeze, FreezeOut, and SmartFRZ, which improve training efficiency by selectively freezing layers based on importance, but these strategies are often complex and not widely adopted for large language models due to compatibility issues with modern memory reduction techniques.</p>
</li>
</ul>
<h3 id="low-rank-adaptation-lora">
  Low-Rank Adaptation (LoRA)
  <a class="anchor" href="#low-rank-adaptation-lora">#</a>
</h3>
<p><img src="./lora.png" alt="LoRA" /></p>
<p>Low-Rank Adaptation (LoRA) is a technique commonly used for training large language models (LLMs) efficiently.
It reduces the number of trainable parameters by employing low-rank matrices, which significantly lessens computational and memory costs.
This method is particularly compatible with models featuring linear layers, allowing seamless integration without altering the model architecture.
LoRA can also be combined with other techniques like quantization and Mixture of Experts to further enhance performance and efficiency.</p>
<h3 id="large-scale-optimization-algorithms">
  Large-scale Optimization Algorithms
  <a class="anchor" href="#large-scale-optimization-algorithms">#</a>
</h3>
<p>Efforts to optimize large language models (LLMs) focus on improving efficiency through various methods. Layerwise optimization, pioneered by Hinton and furthered by Bengio, has shown benefits in sequential layer pre-training. Techniques like LARS and LAMB improve generalization in large batch settings. Common optimization methods include Adam and AdamW. Recent approaches like MeZO and Sophia aim to reduce training costs and speed up processes but face challenges such as complexity and performance drops. GaLore attempts memory-efficient training via low-rank gradient projection. Despite these advances, LoRA-variant methods combined with AdamW remain dominant for fine-tuning large LLMs, though there&rsquo;s room for improvement.</p>
<h2 id="method">
  Method
  <a class="anchor" href="#method">#</a>
</h2>
<h3 id="motivation-training-pattern-of-lora">
  Motivation: training pattern of LoRA
  <a class="anchor" href="#motivation-training-pattern-of-lora">#</a>
</h3>
<p>To take a closer look at the effectiveness of LoRA, we measure the weight norm over different layers during fine-tuning.
Specifically, the value for 
<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \( \ell \)
</span>
-th layer is calculated as the mean over <span>
  \( T \)
</span>
 iterations, i.e.,
<span>
  \[  w^{(\ell)} \triangleq \texttt{mean-weight-norm}(\ell) = \frac{1}{T} \sum_{t=1}^{T} \|\theta^{(\ell)}_t\|_2,\]
</span>

where <span>
  \( \theta^{(\ell)}_t \)
</span>
 is the parameter of <span>
  \( \ell \)
</span>
-th layer at <span>
  \( t \)
</span>
-th iteration during fine-tuning.</p>
<p><img src="./weightnorm.png" alt="weightnorm" /></p>
<p>The results of the weight norm over different layers for GPT2 and LLaMA-2-7B models are presented above.
Here, the red and blue lines each correspond to the results of full fine-tuning and LoRA fine-tuning.
We observe the following intriguing phenomenons.</p>
<ul>
<li>For LoRA fine-tuning, weight norm has a skewed distribution; the values are significantly large for embedding layers and the final LM head whereas those for intermediate layers are much smaller.</li>
<li>For full fine-tuning, the results are very different from LoRA; the values are more uniformly distributed.</li>
</ul>
<p>Overall, the results imply that <em>LoRA treats layerwise importance in a very different way compared to full fine-tuning</em>.</p>
<h3 id="layerwise-importance-sampled-adamw-lisa">
  Layerwise Importance Sampled AdamW (LISA)
  <a class="anchor" href="#layerwise-importance-sampled-adamw-lisa">#</a>
</h3>
<p>LISA aims to follow the training pattern of LoRA while avoiding its limitation of limited expressiveness due to low-rank nature.
Specifically, LISA borrows the idea from importance sampling and only updates a subset of layers at each iteration to simulate the training pattern of LoRA.
Since the weight norm of LoRA is significantly larger for the first and last layers compared to the intermediate ones, the sampling probability for each layer is determined as <span>
  \(  \{p_\ell\}_{\ell=1}^{N_L} = \{1.0, \gamma/N_L, \gamma/N_L, \dots, \gamma/N_L, 1.0\}  \)
</span>
, where <span>
  \( N_L \)
</span>
 is the total number of layers and <span>
  \( \gamma \)
</span>
 is the hyperparameter to control the number of layers to update at each iteration.
In other words, the first and last layers are always updated while each intermediate layer is updated with probability <span>
  \( \gamma/N_L \)
</span>
.</p>
<p><img src="./algorithm.png" alt="weightnorm" /></p>
<p>The algorithm pseudocode is presented above.
For each round, it samples the layers to update and fine-tune the models with AdamW for <span>
  \( K \)
</span>
 iterations.</p>
<h2 id="experiments">
  Experiments
  <a class="anchor" href="#experiments">#</a>
</h2>
<h3 id="efficiency">
  Efficiency
  <a class="anchor" href="#efficiency">#</a>
</h3>
<p>Since LISA only updates a subset of parameters at each iteration, it is expected to consume less memory compared to full fine-tuning.
We precisely measure the memory cost for different versions of LISA compared to full fine-tuning and LoRA.</p>
<p><img src="./peakmemory.png" alt="weightnorm" /></p>
<p>The above table presents the peak GPU memory consumption for full fine-tuning, LoRA, and LISA.
Here, LoRA is experimented with different ranks and LISA is experimented with different versions of E+H (updating embedding and head layer), E+H+2L (updating E, H, and two intermediate layers), and E+H+4L (updating E, H, and four intermediate layers).
As we can see, LISA consumes much less memory compared to full fine-tuning and even less memory than LoRA; for LLaMA-2-70B model, LISA with E+H+2L requires 75B memory while LoRA with rank 128 requires 79GB memory.</p>
<p><img src="./memory.png" alt="weightnorm" /></p>
<p>More detailed analysis regarding the memory consumption is as above.
In particular, LISA consumes much less activation memory compared to LoRA as it does not require additional parameters to update unlike LoRA which introduces a new set of parameters.</p>
<p><img src="./time.png" alt="weightnorm" /></p>
<p>Also, LISA can even accelerate the training speed; as seen in the above figure, LISA demonstrates near 2.9 times and 1.5 times speedups when compared to full fine-tuning and LoRA respectively.
Overall, LISA is more efficient than full fine-tuning and LoRA in terms of both memory and training time.</p>
<h3 id="performance">
  Performance
  <a class="anchor" href="#performance">#</a>
</h3>
<p>In the previous experiments, we have observed that LISA significantly reduces the memory consumption and training time compared to full fine-tuning and LoRA.
Does LISA achieve efficiency without significant performance degradation compared to full fine-tuning?
Surprisingly, we find that LISA actually demonstrates better results than full fine-tuning.</p>
<p><img src="./result-moderate.png" alt="weightnorm" /></p>
<p>First, we measure the performance for moderate-size language models having several billions of parameters.
As you can observe in the above table, LISA outperforms LoRA and even full fine-tuning (FT) by a large margin.</p>
<p><img src="./result-large.png" alt="weightnorm" /></p>
<p>To further investigate the performance of LISA on large-scale models, we additionally test the method on LLaMA-2-70B having 70B parameters.
As shown in the above table, LISA outperforms LoRA for all tasks and even surpasses full fine-tuning on most tasks.</p>
<h3 id="ablation-study-on-hyperparameter">
  Ablation study on hyperparameter
  <a class="anchor" href="#ablation-study-on-hyperparameter">#</a>
</h3>
<p>LISA has two hyperparameters: <span>
  \( \gamma \)
</span>
 for determining the number of layers to update at each iteration, and <span>
  \( K \)
</span>
 for determining how many iterations to perform with the sampled layers (i.e., sampling frequency).</p>
<p><img src="./hyperparam.png" alt="weightnorm" /></p>
<p>The effects of two hyperparameters are presented in the above table.
First, it is observed that larger <span>
  \( \gamma \)
</span>
 lead to increased performance, which is quite expected since more number of layers are updated at each iteration.
However, this also leads to the increased memory consumption, i.e., there exists a trade-off between performance and efficiency.
Second, larger <span>
  \( K \)
</span>
 leads to increased performance by more frequently changing the layers to update.
However, too larger value can lead to inferior performance (see the result when <span>
  \( K =122\)
</span>
 ).</p>
<h2 id="conclusion">
  Conclusion
  <a class="anchor" href="#conclusion">#</a>
</h2>
<p>This paper propose Layerwise Importance Sampled AdamW (LISA), an optimization algorithm that randomly freezes layers of LLM based on a given probability.
Inspired from observations of LoRA’s skewed weight norm distribution, a simple and memory-efficient freezing paradigm is introduced for LLM training, which achieves significant performance improvements over LoRA on downstream fine-tuning tasks with various models, including LLaMA-2-70B.
Further experiments on domain-specific training also demonstrate its effectiveness, showing LISA’s huge potential as a promising alternative to LoRA for LLM training.</p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">


  <div><a class="flex align-center" href="https://github.com/effml-postech/blog-post/commit/caf60ad586aca3ad166cad7d01aca3f77a6b0358" title='Last modified by Dongyeop Lee | May 25, 2024' target="_blank" rel="noopener">
      <img src="/svg/calendar.svg" class="book-icon" alt="" />
      <span>May 25, 2024</span>
    </a>
  </div>



  <div>
    <a class="flex align-center" href="https://github.com/effml-postech/blog-post/edit/main//content/docs/spring24/19_/_index.md" target="_blank" rel="noopener">
      <img src="/svg/edit.svg" class="book-icon" alt="" />
      <span>Edit this page</span>
    </a>
  </div>


</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#background">Background</a>
      <ul>
        <li><a href="#parameter-efficient-fine-tuning">Parameter-Efficient Fine-Tuning</a></li>
        <li><a href="#low-rank-adaptation-lora">Low-Rank Adaptation (LoRA)</a></li>
        <li><a href="#large-scale-optimization-algorithms">Large-scale Optimization Algorithms</a></li>
      </ul>
    </li>
    <li><a href="#method">Method</a>
      <ul>
        <li><a href="#motivation-training-pattern-of-lora">Motivation: training pattern of LoRA</a></li>
        <li><a href="#layerwise-importance-sampled-adamw-lisa">Layerwise Importance Sampled AdamW (LISA)</a></li>
      </ul>
    </li>
    <li><a href="#experiments">Experiments</a>
      <ul>
        <li><a href="#efficiency">Efficiency</a></li>
        <li><a href="#performance">Performance</a></li>
        <li><a href="#ablation-study-on-hyperparameter">Ablation study on hyperparameter</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












