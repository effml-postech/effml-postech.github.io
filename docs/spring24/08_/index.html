<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="MIXTURE OF LORA EXPERTS # Background # What is LoRA? # LoRA is a methodology for effective fine-tuning large-scale pretrained models.
Models such as OPT, LLaMA, and CLIP demonstrate remarkable performance when fine-tuned for various downstream tasks. However, full fine-tuning of these massive models requires substantial computational resources. LoRA enables parameter-efficient fine-tuning by keeping the pretrained model&rsquo;s weights frozen and adding trainable low-rank decomposition matrices.
In the above figure, only the matrices A and B are trained, with dimensions (d x r) and (r x d) respectively.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/spring24/08_/">
  <meta property="og:site_name" content="Efficient ML Systems">
  <meta property="og:title" content="Efficient ML Systems">
  <meta property="og:description" content="MIXTURE OF LORA EXPERTS # Background # What is LoRA? # LoRA is a methodology for effective fine-tuning large-scale pretrained models.
Models such as OPT, LLaMA, and CLIP demonstrate remarkable performance when fine-tuned for various downstream tasks. However, full fine-tuning of these massive models requires substantial computational resources. LoRA enables parameter-efficient fine-tuning by keeping the pretrained model’s weights frozen and adding trainable low-rank decomposition matrices.
In the above figure, only the matrices A and B are trained, with dimensions (d x r) and (r x d) respectively.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="website">
<title>08 | Efficient ML Systems</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" >
<link rel="canonical" href="http://localhost:1313/docs/spring24/08_/">
<link rel="stylesheet" href="/book.min.309b7ed028807cdb68d8d61e26d609f48369c098dbf5e4d8c0dcf4cdf49feafc.css" integrity="sha256-MJt&#43;0CiAfNto2NYeJtYJ9INpwJjb9eTYwNz0zfSf6vw=" crossorigin="anonymous">
  <script defer src="/flexsearch.min.js"></script>
  <script defer src="/en.search.min.ab7f0c68bb5499f16bd5f47d1e40970ab51995077468aa4d673edeb75e6da1cd.js" integrity="sha256-q38MaLtUmfFr1fR9HkCXCrUZlQd0aKpNZz7et15toc0=" crossorigin="anonymous"></script>

  

<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/docs/spring24/08_/index.xml" title="Efficient ML Systems" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>Efficient ML Systems</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>












  



  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-8c2bdf4d8f82cc2d8f16541464d61b3d" class="toggle" checked />
    <label for="section-8c2bdf4d8f82cc2d8f16541464d61b3d" class="flex justify-between">
      <a role="button" class="">Spring24</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/00_taco_example/" class="">00 Taco Example</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/01_/" class="">01</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/02_/" class="">02</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/03_/" class="">03</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/04_/" class="">04</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/05_/" class="">05</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/06_/" class="">06</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/07_/" class="">07</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/08_/" class="active">08</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/09_/" class="">09</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/10_/" class="">10</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/11_/" class="">11</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/12_/" class="">12</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/13_/" class="">13</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/14_/" class="">14</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/15_/" class="">15</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/16_/" class="">16</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/17_/" class="">17</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/18_/" class="">18</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/19_/" class="">19</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/20_/" class="">20</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/21_/" class="">21</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/22_/" class="">22</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/23_/" class="">23</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/24_/" class="">24</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/25_/" class="">25</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>















</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>08</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#background">Background</a>
      <ul>
        <li><a href="#what-is-lora">What is LoRA?</a></li>
        <li><a href="#loras-composistion">LoRAs Composistion</a></li>
        <li><a href="#mixture-of-experts">Mixture-of-Experts</a></li>
      </ul>
    </li>
    <li><a href="#mixture-of-lora-experts-1">Mixture of LoRA experts</a>
      <ul>
        <li><a href="#motivations">Motivations</a></li>
        <li><a href="#method">Method</a></li>
        <li><a href="#training">Training</a></li>
      </ul>
    </li>
    <li><a href="#results">Results</a></li>
    <li><a href="#analyisis-and-limitations">Analyisis and Limitations</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="mixture-of-lora-experts">
  MIXTURE OF LORA EXPERTS
  <a class="anchor" href="#mixture-of-lora-experts">#</a>
</h1>
<!--
제목으로 바꾸기
처음에 어그로 끌 수 있는 내용 먼저
-->
<h2 id="background">
  Background
  <a class="anchor" href="#background">#</a>
</h2>
<h3 id="what-is-lora">
  What is LoRA?
  <a class="anchor" href="#what-is-lora">#</a>
</h3>
<p><em>LoRA is a methodology for effective fine-tuning large-scale pretrained models.</em></p>
<p>Models such as OPT, LLaMA, and CLIP demonstrate remarkable performance when fine-tuned for various downstream tasks. However, full fine-tuning of these massive models requires substantial computational resources. LoRA enables parameter-efficient fine-tuning by keeping the pretrained model&rsquo;s weights frozen and adding trainable low-rank decomposition matrices.</p>
<p align="center">
    <img src=./LoRA.png> 
</p>
<p>In the above figure, only the matrices A and B are trained, with dimensions (d x r) and (r x d) respectively. By setting r &laquo; d, the number of parameters to be trained can be reduced. These trained matrices are then added to the existing pretrained weights, allowing tuning without affecting the inference speed of the original model.</p>
<h3 id="loras-composistion">
  LoRAs Composistion
  <a class="anchor" href="#loras-composistion">#</a>
</h3>
<p>The common solution to further improve the performance of LoRA is to compose multiple trained LoRAs. Research on LoRA composition can be broadly categorized into the following two methodologies.</p>
<p><strong>Linear arithmetic composition.</strong> It is a method of directly adding multiple LoRAs. This approach is simple and has been effective in the NLP and Vision-Language domain, but it can result in the loss of pre-trained model&rsquo;s generative capabilities or the individual characteristics of each LoRA.</p>
<p><strong>Reference tuning-based composition</strong> tackles the above limitations of linear arithmetic method by introducing gradient fusion and controllable sampling, but is requires retaining when incorporating different LoRAs or creating new masks, which results non-trivial computational costs.</p>
<p align="center">
    <img src=./lora_comp.png> 
</p>
<h3 id="mixture-of-experts">
  Mixture-of-Experts
  <a class="anchor" href="#mixture-of-experts">#</a>
</h3>
<h2 id="mixture-of-lora-experts-1">
  Mixture of LoRA experts
  <a class="anchor" href="#mixture-of-lora-experts-1">#</a>
</h2>
<h3 id="motivations">
  Motivations
  <a class="anchor" href="#motivations">#</a>
</h3>
<ol>
<li>Direct linear arithmetic composition reduced the generative power of the model, while normalized linear arithmetic composition retained the generative power of the model but lost its LORA character.</li>
</ol>
<p align="center">
    <img src=./motiv1_1.png align="center" width="40%">
    <img src=./motiv1_2.png align="center" width="40%">
    <figcaption align="center">
<p align="center">
    <img src=./motiv1_3.png width="700">
</p>
In the V&L domain, directly composing multiple trained LoRAs into the original embedding caused significant parameter variations and meaningless output, while normalization compromised their original characteristics. 
<br/>
In the NLP domain, composing four or more LoRAs within the FLAN-T5 model resulted in disordered output, and weight normalization across five datasets decreased the performance, suggesting adverse effects on the intrinsic qualities of the trained LoRAs.
<br/>
<br/>
2. Each layer of the trained LoRA represented a unique characteristic, which cumulatively defined the overall properties of the LoRA.
<p align="center">
    <img src=./motiv2_1.png align="center" width="40%">
    <img src=./motiv2_2.png align="center" width="53%">
    <figcaption align="center">
</p>
(Right: Observed that different layers of LoRA encode distinct features, such as dog coat color and facial features.,<br/>
left: When evaluated on a subset of datasets, there were significant differences in performance across the different layers of LoRA.) 
<p><strong>So, The conjecture is that adjusting the characteristics by varying the layer-specific weights according to the desired domain objective will result in a more effective composition of trained LORAs.</strong></p>
<h3 id="method">
  Method
  <a class="anchor" href="#method">#</a>
</h3>
<p align="center">
    <img src=./Method1.png align="center" width="400">
</p>
<details>
    <summary>See related formulas</summary>
        <b>Symbols</b> <br/>
        input $x \in \mathbb{R} ^ {L \times d}$ <br/>
        L: sequence length <br/>
        d: dim of $x$ <br/>
        Multi attention layer : $$\mathcal{f}_{Attn} (\centerdot)$$ <br/>
        Feed forward neural network layer: $$\mathcal{f}_{FFN} (\centerdot)$$   <br/>
        LN: layer normalization <br/>
        Trained LORAs $$\Omega = \left\{ \Delta \Theta \right\}^N_{i=0}$$ <br/>
        learnable gating function $$\mathcal{G} (\centerdot)$$ <br/>
        The weight of the $i^{th}$ trained LorA $$\mathcal{G}_i (\centerdot)$$ <br/>
        Concatenation operation: $$\oplus$$ <br/>
        Learnable parameter $e \in \mathbb{R} ^ {N^2 \times L \times d}$ <br/>
        Learnable temperature scalar $\tau$ <br/>
        <br/>
        <b>Freezing part</b>
        $$x^\prime_{\theta} = x + \mathcal{f}_{Attn} (LN(x)|\theta)$$ <br/>
        $$\mathbf{F}_\theta (x) = x^\prime_{\theta} + \mathcal{f}_{Attn} (LN(x^\prime_{\theta})|\theta)$$ <br/>
        <br/>
        <b>LoRA part</b>
        $$x^\prime_{\Delta \Theta_i} = x + \mathcal{f}_{Attn} (LN(x)|\Delta \Theta_i)$$ <br/>
        The output of each LoRA $$\mathbf{E} _{\Delta \Theta_i} (x) = x^\prime_{\Delta \Theta_i} + \mathcal{f}_{FFN} (LN(x^\prime_{\Delta \Theta_i})|\Delta \Theta_i)$$ <br/>
        The output of all LoRA $$\mathbf{E}_\Omega (x) = Normalization(\mathbf{E}_{\Delta \Theta_0} (x) \oplus \ldots \oplus \mathbf{E}_{\Delta \Theta_{N-1}} (x)) \in \mathbb{R} ^ {N \times L \times d}$$ <br/>
        Flatten and dot product operation $$\epsilon = Flatten(\mathbf{E}_\Omega (x))^T \centerdot e,  \epsilon \in \mathbb{R} ^ N$$ <br/>
        Gate value for each LoRA $$\mathcal{G} (\epsilon_i) = \frac {exp(^{\epsilon_i} /_ \tau)} {\displaystyle\sum_{j=1}^N {exp(^{\epsilon_j} /_ \tau)}} $$ <br/>
        Final output of the gating function $${\tilde{\mathbf{E}}_\Omega (x)} = \displaystyle\sum_{i=0}^N {\mathcal{G} (\epsilon_i) \centerdot \mathbf{E} _{\Delta \Theta_i} (x)} , {\tilde{\mathbf{E}}_\Omega (x)} \in \mathbb{R} ^ {L \times d} $$ <br/>
        <b>Final output of Transformer block</b>
        $$\mathcal{O}(x) = {\mathbf{F}_\theta (x)} + {\tilde{\mathbf{E}}_\Omega(x)} $$ 
</details> 
<h3 id="training">
  Training
  <a class="anchor" href="#training">#</a>
</h3>
<p>The final loss function used in MoLE is as follows:</p>
<p align="left">
    <img src=./training5.png width="200">
</p>
Alpha is a coefficient for weight balancing. 
<br/>
<p><strong>Gating Balacing Loss</strong></p>
<p align="center">
    <img src=./training1.png width="400">
</p>
As shown in Figure 5 (a), the average entropy of the distribution probabilities from the gating functions gradually decreases as training progresses. In Figure 5 (b), we can see a gating probability of 64% for LoRA β among the three LoRAs, indicating that the gating function tends to converge to a state where it assigns large weights to well-performing LoRAs in the early stages. This can result in a significantly larger impact from a few specific LoRAs compared to others, potentially leading to biased outcomes. <br/>
<br/>
To avoid this, the author created a gating balancing loss.<br/>
The gating balancing loss helps prevent bias by ensuring that the loss value decreases as the model becomes less biased. <br/>
<br/>
<p align="left">
    <img src=./training2.png width="200">
</p>
<details>
    <summary>See related Symbols</summary>
    M: the nu of blocks where gating functions are placed <br/>
    N: num of LoRAs
</details>     
<br/>
<p><strong>Domain-specific Loss</strong>
<br/>
In V&amp;L, Using a loss in CLIP(Radford et al,20221b) <br/></p>
<p align="left">
    <img src=./training3.png width="300">
</p>
<p>In NLP, Using a loss in FLAN-T5(Chung et al,2022)</p>
<p align="left">
    <img src=./training4.png width="200">
</p>
<h2 id="results">
  Results
  <a class="anchor" href="#results">#</a>
</h2>
<h2 id="analyisis-and-limitations">
  Analyisis and Limitations
  <a class="anchor" href="#analyisis-and-limitations">#</a>
</h2>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">


  <div><a class="flex align-center" href="https://github.com/effml-postech/blog-post/commit/8476505508d1e08b3499ad4aa44f53d0679d7a81" title='Last modified by darwin406 | May 22, 2024' target="_blank" rel="noopener">
      <img src="/svg/calendar.svg" class="book-icon" alt="" />
      <span>May 22, 2024</span>
    </a>
  </div>



  <div>
    <a class="flex align-center" href="https://github.com/effml-postech/blog-post/edit/main//content/docs/spring24/08_/_index.md" target="_blank" rel="noopener">
      <img src="/svg/edit.svg" class="book-icon" alt="" />
      <span>Edit this page</span>
    </a>
  </div>


</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#background">Background</a>
      <ul>
        <li><a href="#what-is-lora">What is LoRA?</a></li>
        <li><a href="#loras-composistion">LoRAs Composistion</a></li>
        <li><a href="#mixture-of-experts">Mixture-of-Experts</a></li>
      </ul>
    </li>
    <li><a href="#mixture-of-lora-experts-1">Mixture of LoRA experts</a>
      <ul>
        <li><a href="#motivations">Motivations</a></li>
        <li><a href="#method">Method</a></li>
        <li><a href="#training">Training</a></li>
      </ul>
    </li>
    <li><a href="#results">Results</a></li>
    <li><a href="#analyisis-and-limitations">Analyisis and Limitations</a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












