<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting # Author: Liu F, Tang Y, Liu Z, Ni Y, Han K, Wang Y Written by Nayoung Kwon and Jiwoong Im
Introduction # The growing demand for rapid and efficient inference in large language models (LLMs) faces a significant bottleneck
Decoding \(K\) tokens requires \(K\) sequential runs of the model. ⇒ LLM inference is slow.
To address this issue, speculative decoding has been introduced as a promising approach to accelerate LLM inference without altering the output quality.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/spring24/05_/">
  <meta property="og:site_name" content="Efficient ML Systems">
  <meta property="og:title" content="Efficient ML Systems">
  <meta property="og:description" content="Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting # Author: Liu F, Tang Y, Liu Z, Ni Y, Han K, Wang Y Written by Nayoung Kwon and Jiwoong Im
Introduction # The growing demand for rapid and efficient inference in large language models (LLMs) faces a significant bottleneck
Decoding \(K\) tokens requires \(K\) sequential runs of the model. ⇒ LLM inference is slow.
To address this issue, speculative decoding has been introduced as a promising approach to accelerate LLM inference without altering the output quality.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="website">
<title>05 | Efficient ML Systems</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" >
<link rel="canonical" href="http://localhost:1313/docs/spring24/05_/">
<link rel="stylesheet" href="/book.min.309b7ed028807cdb68d8d61e26d609f48369c098dbf5e4d8c0dcf4cdf49feafc.css" integrity="sha256-MJt&#43;0CiAfNto2NYeJtYJ9INpwJjb9eTYwNz0zfSf6vw=" crossorigin="anonymous">
  <script defer src="/flexsearch.min.js"></script>
  <script defer src="/en.search.min.bb782c6e4bffd59850a229fee25dec53730b0d1895bb722758be85c67a9d84f7.js" integrity="sha256-u3gsbkv/1ZhQoin&#43;4l3sU3MLDRiVu3InWL6FxnqdhPc=" crossorigin="anonymous"></script>

  

<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/docs/spring24/05_/index.xml" title="Efficient ML Systems" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>Efficient ML Systems</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>












  



  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-8c2bdf4d8f82cc2d8f16541464d61b3d" class="toggle" checked />
    <label for="section-8c2bdf4d8f82cc2d8f16541464d61b3d" class="flex justify-between">
      <a role="button" class="">Spring24</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/00_taco_example/" class="">00 Taco Example</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/01_/" class="">01</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/02_/" class="">02</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/03_/" class="">03</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/04_/" class="">04</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/05_/" class="active">05</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/06_/" class="">06</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/07_/" class="">07</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/08_/" class="">08</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/09_/" class="">09</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/10_/" class="">10</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/11_/" class="">11</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/12_/" class="">12</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/13_/" class="">13</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/14_/" class="">14</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/15_/" class="">15</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/16_/" class="">16</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/17_/" class="">17</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/18_/" class="">18</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/19_/" class="">19</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/20_/" class="">20</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/21_/" class="">21</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/22_/" class="">22</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/23_/" class="">23</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/24_/" class="">24</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/spring24/25_/" class="">25</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>















</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>05</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#backgrounds">Backgrounds</a>
      <ul>
        <li><a href="#what-is-speculative-decoding">What is speculative decoding?</a></li>
        <li><a href="#kangaroo-self-speculative-decoding">Kangaroo: Self-speculative decoding</a></li>
      </ul>
    </li>
    <li><a href="#layer-early-exiting">Layer Early Exiting</a>
      <ul>
        <li><a href="#evaluation-metrics">Evaluation Metrics</a></li>
        <li><a href="#adapter-network-as-self-drafting-model">Adapter Network as Self-Drafting Model</a></li>
      </ul>
    </li>
    <li><a href="#draft-early-exiting">Draft Early Exiting</a></li>
    <li><a href="#experiments">Experiments</a></li>
    <li><a href="#discussion-and-conclusion">Discussion and Conclusion</a>
      <ul>
        <li><a href="#limitations-and-future-work">Limitations and future work:</a></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="kangaroo-lossless-self-speculative-decoding-via-double-early-exiting">
  Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting
  <a class="anchor" href="#kangaroo-lossless-self-speculative-decoding-via-double-early-exiting">#</a>
</h1>
<ul>
<li>Author: Liu F, Tang Y, Liu Z, Ni Y, Han K, Wang Y</li>
</ul>
<p>Written by Nayoung Kwon and Jiwoong Im</p>
<h2 id="introduction">
  Introduction
  <a class="anchor" href="#introduction">#</a>
</h2>
<p>The growing demand for rapid and efficient inference in large language models (LLMs) faces a significant bottleneck</p>
<ul>
<li>Decoding 
<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \(K\)
</span>
 tokens requires <span>
  \(K\)
</span>
 sequential runs of the model.</li>
</ul>
<p>⇒ LLM inference is slow.</p>
<p>To address this issue, speculative decoding has been introduced as a promising approach to accelerate LLM inference without altering the output quality. This method leverages two key observations about LLM inference:</p>
<ul>
<li>Many tokens can be predicted with minimal computational overhead.</li>
<li>LLM inference is predominantly constrained by memory bandwidth rather than arithmetic computations.
<code>Speculative decoding</code> reduce the need for frequent memory operations on their parameters by focusing computational efforts on validating pre-drafted tokens, thus enhancing inference efficiency.</li>
</ul>
<p>However, existing speculative decoding such as <strong>Medusa</strong> and <strong>Lookahead</strong> still face limitations, such as high inference latency and suboptimal token acceptance rates.
This papaer proposed Kangaroo to address this challenge.</p>
<h2 id="backgrounds">
  Backgrounds
  <a class="anchor" href="#backgrounds">#</a>
</h2>
<h3 id="what-is-speculative-decoding">
  What is speculative decoding?
  <a class="anchor" href="#what-is-speculative-decoding">#</a>
</h3>
<p>Speculative decoding is an apporach to accelerate LLM inference.</p>
<p><strong>Draft model:</strong> Additional model to accelerate inference (also known drafter)</p>
<p><strong>Verifier or target model</strong>: Original large LLM</p>
<p align="center">
    <img src='./speculative decoding.png' width="700">
</p>
<p align="center">
    Fig. 1 Contrast to autoregressive decoding and speculative decoding
</p>
<p><strong>Left model</strong>: The target LLM generates K tokens in K forward steps, which is a &ldquo;serial&rdquo; process.</p>
<p><strong>Right model</strong>: The drafter generates tokens in parallel. Each generated token is then verified with a verification step.</p>
<p><code>Speculative decoding</code> can be implemented through methods such as independent drafting and self-drafting.</p>
<p><strong>Independent Drafting</strong>: This approach uses a small language model (LM) from the same series as the target LLM.</p>
<ul>
<li>Requires additional training and increases computational complexity by integrating separate target and drafting models.</li>
</ul>
<p><strong>Self-Speculative Decoding</strong>: This method utilizes the target LLM itself.</p>
<ul>
<li>Employs techniques such as Blockwise Decoding, Medusa, and early exiting to reduce computational burden.</li>
<li>Computational efficiency can also be achieved through layer skipping.</li>
</ul>
<h3 id="kangaroo-self-speculative-decoding">
  Kangaroo: Self-speculative decoding
  <a class="anchor" href="#kangaroo-self-speculative-decoding">#</a>
</h3>
<p><strong>Kangaroo</strong> refers to the self-speculative decoding method, utilizing a fixed shallow sub-network of the original (target) large LLM.</p>
<p align="center">
    <img src='./compare.png' width="700">
</p>
<p align="center">
    Fig. 2 Comparison of variouus self-drafting speculative docding methods
</p>
In each decoding step, drafted tokens must be verified in parallel to ensure alignment with the target LLM, which determines the token acceptance rate. High token acceptance rates are crucial for the efficiency of this process. However, methods like Medusa have yet to achieve satisfactory token acceptance rates, as evidenced by performance metrics (see left graph). On the other hand, the Lookahead method achieves a high token acceptance rate but has a very low speedup ratio (see right graph).
Addressing these trade-off, **Kangaroo** offers a solution by training a lightweight and efficient adapter module integrated with a fixed subnetwork of the target LLM, enhancing both the acceptance rate and overall speedup.
<h2 id="layer-early-exiting">
  Layer Early Exiting
  <a class="anchor" href="#layer-early-exiting">#</a>
</h2>
<p>The author has proposed a novel self-speculative decoding framework, named Kangaroo. Kangaroo utilizes double early exiting mechanisms, layer early exiting and draft early exiting. Layer early exiting suggests the equivalent self-draft small model exiting early from the fixed shallow layers of the large LLM and connecting to an adapter network to generate draft tokens. While this strategy is commonly used for self-speculative decoding frameworks, Kangaroo has further investigated suitable architectures of the adapter module and offered a low-cost approach to train a lightweight model. Draft early exiting uses early exiting at suitable points during the drafting phase to avoid unnecessary computational overhead on more challenging tokens.</p>
<h3 id="evaluation-metrics">
  Evaluation Metrics
  <a class="anchor" href="#evaluation-metrics">#</a>
</h3>
<p>Speculative decoding is often evaluated using two primary metrics: walltime speedup ratio and compression rate. Given a speculative decoding algorithm, we assume that <span>
  \(N\)
</span>
 tokens should be generated via the drafting model. As the drafting model predicts multiple tokens in each decoding step and multiple tokens can be accepted by the large model in a step, we record the number of accepted tokens per step as a list <span>
  \( S = \[s_1, s_2, \dots, s_{|S|}\] \)
</span>
, where <span>
  \( \sum_k s_k = N \)
</span>
 and <span>
  \( |S| \)
</span>
 denotes the number of steps. Then, the compression rate (CR) is defined as:
<span>
  \[\text{CR} = \frac{1}{|S|} \sum_k s_k.\]
</span>

However, once a draft token is rejected during the verification, all subsequent tokens sampled from the drafting model will be discarded. Therefore, CR does not accurately reflect the acceptance levels for tokens at varying distances, and the author has proposed a new evaluation metric named <em>consistent token acceptance rate</em>.</p>
<p>The consistent token acceptance rate <span>
  \( \text{CTAR}(w) \)
</span>
 is calculated as:
<span>
  \[\text{CTAR}(w) = \frac{1}{|S|} \sum_k \mathbb{I} (s_k - w &gt; 0),\]
</span>

where <span>
  \(\mathbb{I}(\cdot)\)
</span>
 denotes an indicator function and <span>
  \( w \)
</span>
 denotes a window size. CTAR can be interpreted as a rate of the number of steps to accept over <span>
  \( w \)
</span>
 tokens.</p>





<img src="./CTAR.png" alt="." width="600" height="600">
<p>Figure 1 represents the empirical CTARs for <span>
  \(w = 1,2,\dots,6 \)
</span>
 of self-drafting speculative decoding frameworks including Kangaroo on the mathematical reasoning subtask of Spec-Bench [1].</p>
<p>[1] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. <em>arXiv preprint arXiv:2401.07851</em>, 2024.</p>
<h3 id="adapter-network-as-self-drafting-model">
  Adapter Network as Self-Drafting Model
  <a class="anchor" href="#adapter-network-as-self-drafting-model">#</a>
</h3>
<p>We assume the target LLM has <span>
  \( L \)
</span>
 layers and the self-draft model <span>
  \( \mathcal{M}^s \)
</span>
 consists of shallow sub-network <span>
  \( \mathcal{M}^b[:l] \)
</span>
, which is first <span>
  \( l \)
</span>
 layers of the target LLM <span>
  \(\mathcal{M}^b\)
</span>
, and a adapter network <span>
  \( \mathcal{A} \)
</span>
. The drafting model reuses the LM head of the target LLM, and the overall parameters of the target LLM, such as shallow sub-network, remaining layers of LLM, and LM head, are frozen during the training of the adapter network. In Kangaroo, the adapter <span>
  \( \mathcal{A} \)
</span>
 only encompasses one multi-head attention and two normalization layers. The author emphasizes the feed-forward network (FFN) of the transformer block is too heavy in parameters but redundant, which is presented in the ablation study of the adapter architecture in the Experiments Section.</p>





<img src="./Kangaroo.png" alt="." width="600" height="600">
<p>Figure 2 illustrates the framework of Kangaroo. The lightweight drafting model <span>
  \( \mathcal{M}^s \)
</span>
, including shallow sub-network and adapter network, predicts the draft tokens autoregressively until draft early exiting occurs. The strategy of draft early exiting will be explained later in the Draft Early Exiting Section. Then, the hidden states, computed in shallow sub-network, are processed in the remaining layers of LLM to generate prediction results. The draft tokens and the original prediction results are now compared for verification, and</p>
<h2 id="draft-early-exiting">
  Draft Early Exiting
  <a class="anchor" href="#draft-early-exiting">#</a>
</h2>
<h2 id="experiments">
  Experiments
  <a class="anchor" href="#experiments">#</a>
</h2>
<h2 id="discussion-and-conclusion">
  Discussion and Conclusion
  <a class="anchor" href="#discussion-and-conclusion">#</a>
</h2>
<p>Kangaroo with a double early-exit mechanism ensures both efficiency and high performance.</p>
<p>Several advantages:</p>
<ul>
<li><strong>Low-Cost Training</strong>: The shared KV cache and computation between the self-speculative draft model and the large LLM
→ only the adapter network requires additional deployment.</li>
<li><strong>Efficiency</strong>: Experiments on Spec-Bench demonstrate that Kangaroo achieves up to 1.7× speedup, outperforming existing methods with significantly fewer additional parameters (67M compared to 591M for Medusa).</li>
<li><strong>Flexibility</strong>: By focusing on reducing inference latency and optimizing token acceptance rates, Kangaroo ensures that performance remains robust across various tasks without incurring substantial overhead.</li>
</ul>
<p>Compare with others:</p>
<p>Kangaroo&rsquo;s performance surpasses other speculative decoding methods, such as Medusa and Lookahead, particularly in terms of end-to-end speedup and token acceptance rates (see Fig.2 in introduction). The double early-exit mechanism plays a crucial role in maintaining this balance by efficiently handling easier tokens and exiting early when confidence is lower than predefined threshold, thus minimizing latency.</p>
<h3 id="limitations-and-future-work">
  Limitations and future work:
  <a class="anchor" href="#limitations-and-future-work">#</a>
</h3>
<ul>
<li><strong>Enhanced Confidence Metrics</strong>:
<ul>
<li>Although Kangaroo introduces a confidence-based mechanism, it retains the original issue of discarding generated tokens that do not meet the confidence threshold.</li>
<li>Currently, confidence is measured only on the top-1 prediction. Future work could explore alternative metrics such as entropy or other measures to provide a more robust assessment of token validity.</li>
</ul>
</li>
<li><strong>Alternative Networks for Verification</strong>:
<ul>
<li>The use of an adapter network in Kangaroo shows promising results. However, experimenting with different network architectures could yield even better performance. Future research could investigate various types of networks to replace the adapter, potentially improving both the efficiency and accuracy of the speculative decoding process.</li>
</ul>
</li>
<li><strong>Expanding Beyond Self-Speculative Decoding</strong>:
<ul>
<li>While Kangaroo leverages the target LLM for self-speculative decoding, exploring independent drafting models from the same series might offer additional insights. Balancing the computational trade-offs between these approaches could lead to more optimized frameworks.</li>
</ul>
</li>
<li><strong>Adaptive Early-Exit Mechanisms</strong>:
<ul>
<li>The current implementation of early exits in Kangaroo could be refined by dynamically adjusting the confidence thresholds based on the context or specific tasks. This adaptation could further reduce unnecessary computations and improve the overall efficiency of the model.</li>
</ul>
</li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">


  <div><a class="flex align-center" href="https://github.com/effml-postech/blog-post/commit/60c93a704d66e1aa9fc5a1429ab5c2fb2c73ad4a" title='Last modified by effml-postech | May 22, 2024' target="_blank" rel="noopener">
      <img src="/svg/calendar.svg" class="book-icon" alt="" />
      <span>May 22, 2024</span>
    </a>
  </div>



  <div>
    <a class="flex align-center" href="https://github.com/effml-postech/blog-post/edit/main//content/docs/spring24/05_/_index.md" target="_blank" rel="noopener">
      <img src="/svg/edit.svg" class="book-icon" alt="" />
      <span>Edit this page</span>
    </a>
  </div>


</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#backgrounds">Backgrounds</a>
      <ul>
        <li><a href="#what-is-speculative-decoding">What is speculative decoding?</a></li>
        <li><a href="#kangaroo-self-speculative-decoding">Kangaroo: Self-speculative decoding</a></li>
      </ul>
    </li>
    <li><a href="#layer-early-exiting">Layer Early Exiting</a>
      <ul>
        <li><a href="#evaluation-metrics">Evaluation Metrics</a></li>
        <li><a href="#adapter-network-as-self-drafting-model">Adapter Network as Self-Drafting Model</a></li>
      </ul>
    </li>
    <li><a href="#draft-early-exiting">Draft Early Exiting</a></li>
    <li><a href="#experiments">Experiments</a></li>
    <li><a href="#discussion-and-conclusion">Discussion and Conclusion</a>
      <ul>
        <li><a href="#limitations-and-future-work">Limitations and future work:</a></li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












