[{"id":0,"href":"/docs/spring24/00_taco_example/","title":"00 Taco Example","section":"Spring24","content":" Example : Content # This paper propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss. By doing so, they avoid decoding based on text-guided generative models\u0026mdash;known for high generative diversity\u0026mdash;and effectively utilize the semantic information of text at a global level.\nExample : Using KaTeX for math equation # KaTeX shortcode let you render math typesetting in markdown document. See KaTeX\nHere is some inline example: \\(\\pi(x)\\) , rendered in the same line. And below is display example, having display: block \\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi\\] Text continues here!!!\n"},{"id":1,"href":"/docs/spring24/01_/","title":"01","section":"Spring24","content":" Is Bigger Edit Batch Size Always Better? - An Empirical Study on Model Editing with Llama-3 # Authors: Junsang Yoon, Akshat Gupta, Gopala Anumanchipalli\nPosted by Jin Hyun, Gyuhyun Jung\nBackground # What is model editing? # Fig 1. Concept of model editing. The rapidly evolving field of artificial intelligence faces the challenge of keeping large language models (LLMs) up-to-date with new information, as traditional retraining methods are time-consuming and resource-intensive. As shown in figure, an alternative is model editing proposed in (Sinitsin et al., 2020). It enables data-efficient alterations to the behavior of models.\nFig 2. Example of model editing in case of MEMIT. Model editing modifies stored facts within a model and corrects inaccuracies without retraining. Techniques such as ROME (Rank-One Model Editing) (Meng et al., 2022a), MEMIT (Mass Editing Memory in Transformer) (Meng et al., 2022b), and EMMET (Equality-constrained Mass Model Editing algorithm for Transformers) (Gupta et al., 2024), known as \u0026ldquo;locate-and-edit\u0026rdquo; algorithms, have emerged to optimize the preservation-memorization (PM) objective. These methods directly modify specific areas of the model and are applicable to any transformer-based LLMs, offering a more efficient way to update models without retraining.\nHow model editing works? # For a relation \\((s,r,o)\\) expressed as a tuple in the form of (subject, relation, object). In model editing, we aim to update the memory of the existing model with new facts by learning about a new object \\((s,r,o^*)\\) . Model editing directly reform the weight by objective function, called the preservation-memorization objective. This objective consists of two parts, a preservation term and a memorization term. Below equation shows how ROME works with preservation term and memorization term.\n\\( \\argmin_{\\hat{W}} \\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\quad \\text{s.t.} \\quad \\hat{W} k_e = v_e \\\\Preservation\\_term=\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\\\ Memorization\\_term=\\hat{W} k_e = v_e \\) Where W represents the weights of the feedforward layer we want to edit, k is a key-vector representative of a fact, \\(v_e\\) is the desired output, and \\(K_0 =[k_1^0 |k_2^0 |\\cdots| k_0^N]\\) is a matrix consisting of facts we want to preserve. Above equation is optimized by follwing gradient.\n\\(\\hat{W} = W_0 \u0026#43; \\Delta \\quad \\text{where} \\\\ \\Delta = (v_e - W_0 k_e) \\frac{k_e^T C_0^{-1}}{k_e^T C_0^{-1} k_e} \\) For MEMIT model editing. it optimizes same objectives with ROME, but performance memorization using a least-square constraint, which allows for a closed-form solution. It has similar form with ROME method, but it multiplies \\(\\lambda\\) term, which is hyperparameter, to preservation term. Also, it combines memorization term for minimize target\n\\(\\argmin_{\\hat{W}} \\lambda\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \u0026#43; \\left\\| \\hat{W} K_E - V_E \\right\\|\\\\Preservation\\_term=\\lambda\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\\\ Memorization\\_term=\\hat{W} K_E - V_E \\) \\(V_E\\) is stacked matrix of \\(v_e\\) vectors, and fact is represented by a pair of vectors denoted as key ( \\(k_e\\) ) and value ( \\(v_e\\) ). This objective has similar solution of ROME, followed by below equations.\n\\(\\hat{W} = W_0 \u0026#43; \\Delta \\quad \\text{where} \\\\ \\Delta = (V_E - W_0 K_R)K^T_E (\\lambda C_0 \u0026#43; K_E^T K_E^T)^{-1} \\) In EMMET, it shows model editing is possible with batched facts. It is possible by allowing memorization happens using an equality-constraint. EMMET objective and gradient solution is followed by below equations.\n\\(\\argmin_{\\hat{W}} \\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\|\\quad \\text{s.t.} \\hat{W} k_i^e = v_i^e \\quad \\forall i \\in [1, 2, \\cdots, E] \\\\Preservation\\_term=\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\\\ Memorization\\_term=\\hat{W} k_i^e = v_i^e \\quad \\forall i \\in [1, 2, \\cdots, E] \\\\ \\hat{W} = W_0 \u0026#43; \\Delta \\quad \\text{where} \\\\ \\Delta = (V_E - W_0 K_R)(K_E^T C_0^{-1}K_E)^{-1}K_E^TC_0^{-1} \\) How model editing performance is estimated? # Model performance is estimated with 4 main scores, and these scores are bsed on how model editing works with expressions of correct facts in \\((s,r,o^{c})\\) and false facts in \\((s,r,o^{*})\\) .\nEfficacy Score (ES) # ES measures if the new fact, which we want to edit, is successfully edited to model. It is measured by percentage where \\(\\mathbb{P}[o^*] \u0026gt; \\mathbb{P}[o^{c}]\\) , which means the portion of correct edition result from predictions.\nParaphrase Score (PS) # PS measures model\u0026rsquo;s ability to generalize following an edit. It is measured by where P(new fact) \u0026gt; P(old fact) under paraphrases of the query prompt.\nNeighborhood Score (NS) # NS represents the specificity of model editing. To measure NS, we collect a set of nearby subjects \\(s_n\\) for which \\((s_n,r,o^{c})\\) holds true. Then we test \\(\\mathbb{P}[o^*] \u0026gt; \\mathbb{P}[o^{c}]\\) , reporting the success fraction asn NS.\nComposite Score (S) # S represents the overall performance. It combines aspect of edit success, generalization, and specificity. It is calculated as the harmonic mean of Edit Success (ES), Paraphrase Score (PS), and Neighborhood Score (NS). It provies overall efficacy of model edits.\nExperiments \u0026amp; Results # What is the Optimal Layer for Model Editing? # Investigating the effectiveness of hidden states in LLMS for recalling facts using causal tracing showed that subject’s last token within the feed-forward networks at intermediate layer plays a significant role. (Meng et al., 2022b)\nMotivation : Later work showed that layers deemed important during causal tracing did not always translate to model editing performance. Therefore, this work focused on finding the optimal layer for model editing layer empirically.\nSteps for finding optimal layer\nMake 1000 non-sequential edits from the CounterFact (Meng et al., 2022a) dataset at each layer of the Llama-3 model. Calculate various model metrics(ES, PS, NS, S) to evaluate their impact. The layer that achieves the highest score is selected as the most suitable for targeted interventions. Fig 3. Post-edit performance of various metrics for Llama3-8b model using MEMIT and ROME on various layers. Eqch layer is edited with 1000 facts, one at a time and non-sequentially. Fig 4. Post-edit performance of various metrics on Llama2-7b for MEMIT on various layers. Evaluation results showed that layer 1 for Llama-3 outperformed on numerous metrics. Furthermore this trend was also shown in previous version, Llama-2, as seen in Figure 6. Here, MEMIT and ROME have very similar performance for model editing across layer of a model.\n→ Why? : Both algorithms optimize for the same objective with difference in the memorization constraints. This shows that memorization constraints plays minor effect on editing performance.\nOptimal way of Scaling Up model editing? # After finding the optimal layer, scaling of model editing on the same model can happen in two ways : batch editing \u0026amp; sequential-batched editing.\n1. Batch Editing :\nA large number(batch size) of knowledge edits are performed on the model with the same update. This work stick to editing a single layer of the model.\nExperimental settings\nTargeting layer1 in Llama-3 with batch size 16, 64, 256, 1024, and 4096 for Batched editing. Evaluation Results of Batch Editing\nFig 5. Various metric results (PS, NS, ES, S) after a batch edit (16, 64, 256, 1024, 4096) on MEMIT and EMMET respectively. For both MEMIT \u0026amp; EMMET editing, metrics are seen to consistently fall with larger batches, with NS being the most pronounced to fall. ES is most resilient metric to edits. PS, only metric to do so, seen to increase dramatically between batch sizes of 16 and 64. The similar trend between two editing techniques reflect the similarity in their optimization objectives.\n2. Sequential-batched Editing :\nSequential Editing is an alternate way to scale up model editing where facts are added sequentially to a model.\nThis work proposes optimal way to scale model editing that strikes a balance between Batch Editing \u0026amp; Sequential Editing.\nSequential-batched editing sequentially edit many batch of facts at a time. And the experiment was conducted going from batch size of 1 up to 4096. (1, 64, 256, 1024, 4096)\nFig 6. Single layer sequential editing performance for various batch sizes on MEMIT and EMMET respectively. Experimental results according to figures above showed that larger batch sizes are actually worse for model performance than sequential edits with smaller batches. In contrast, larger batch sizes seem to be better for metrics in NS : while batch edits are less successful in general, it is better in preserving locality of edits. This results were concluded to optimal batch size of 1024 for both MEMIT and EMMET. Increasing batch-size beyond that lead to larger model degradation and better editing results can be achieved by sequential-batched editing with smaller batch sizes.\nConclusion # This work examines several model editing techniques in the context of the newly released Llama-3 model and there are some conclusion as follows:\nEarlier layers may be more optimal intervention points. Model editing techniques that share same optimization objectives shows similar trends in layer and editing. Smaller, frequent sequential batch size edits have a superior performance. Batch size of 1024 for MEMIT and EMMET is optimal batchsize with sequential-batched editing. The authors argue that the current trend of pushing towards bigger edit batch sizes for scaling model editing may have limitations. Instead, they propose that future research should focus on methods that combine both batched and sequential editing to optimize performance while minimizing model degradation. Also, future work was proposed for experiments on multi-layer intervention for edits, as well as experiments against other popular models and algorithms, including methods that are hyper-network based.\nDiscussions, and research direction proposal from post writers # The paper empirically analyzes the performance of model editing based on batch size. It would be more beneficial for model editing research if the theoretical reasons behind the overall metrics decreasing as batch size increases are elucidated, rather than just empirically.\nWhile the work presents a hybrid format combining sequential editing and batch editing, it lacks in-depth analysis of the strengths and weaknesses of both approaches. Additionally, it is important to ensure that the individual characteristics of techniques such as ROME, MEMIT, and EMMET are appropriately integrated into editing optimization.\nAnalyzing the reasons behind the improvement in performance when layers are edited later in the network (NS) and the improvement when batch size is increased (PS) could help in identifying the optimal point for multi-layer editing\nIt seems necessary to investigate how many layers should be edited in multi-layer editing to achieve effective results beyond single-layer editing.\nReferences # Implementation code: Link.\nYunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang. 2023. Editing large language models: Problems, methods, and opportunities. arXiv preprint arXiv:2305.13172.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, Artem Babenko. 2020. Editable neural networks. arXiv preprint arXiv:2004.00345.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022a. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359–17372.\nKevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. 2022b. Massediting memory in a transformer. arXiv preprint arXiv:2210.07229.\nAkshat Gupta, Dev Sajnani, and Gopala Anumanchipalli. 2024. A unified framework for model editin. arXiv preprint arXiv:2403.14236.\n"},{"id":2,"href":"/docs/spring24/02_/","title":"02","section":"Spring24","content":" Spectrally Pruned Gaussian Fields with Neural Compensation (SUNDAE) # *Authors: Yang, Runyi, et al\n*Team: Donggeon Lee, Chiho Yoon\nSummary # 3D representation, which is the basis for many VR/AR and robotics applications, has long been an area of interest in computer vision and graphics. With the advent of neural radiation fields (NeRFs) Link, several methods have emerged to improve the quality and efficiency of NeRFs.\nConventional 3D Gaussian Splatting (3DGS) # Fig. 1. Comparison of 3D Gaussian Splatting to previous NeRF technologies. One of the recent hot topics in the NeRF field, 3D Gaussian Splatting (3DGS), demonstrates high quality and particularly fast, near real-time rendering speeds (about 100FPS).\nPros: Superior rendering speed and quality Cons: High memory consumption Proposed SUNDAE # Fig. 2. Comparison of 3D gaussian splatting and proposed SUNDAE It constructs a memory-efficient Gaussian field using spectral pruning and neural compensation. It considers the relationship between primitives, reducing memory usage while maintaining rendering quality. It significantly reduces memory consumption while preserving high rendering quality. Code: https://runyiyang.github.io/projects/SUNDAE/. Introduction # Fig. 3. Conceptual illustration of vanilla 3DGS, SUNDAE spectral pruning technique, and neural compensation. 3D Gaussian Splatting (3DGS) Link # Recently, 3DGS has been proposed as a novel 3D scene representation, utilizing a set of 3D positions, opacity, anisotropic covariance, and spherical harmonic (SH) coefficients to represent a 3D scene (left panel of Fig. 2). 3DGS demonstrates notable advantages in rendering speed, rendering quality, and training time. But it requires a large storage.\nSpectral graph pruning # Gaussian fields utilize a collection of Gaussian primitives as the representation of the scene. As these primitives are irregularly distributed in 3D space, they propose a graph-based data structure, rather than regular structures like grids, to capture the relationship between these primitives (middle panel of Fig. 2).\nNeural compensation # To address an inevitable decrease in rendering quality, they employ a neural compensation head to compensate for this quality loss (right panel of Fig. 2).\nContributions # A newly proposed primitive pruning framework for Gaussian fields based upon the spectrum of primitive graphs. A novel feature splatting and mixing module to compensate for the performance drop caused by the pruning. State-of-the-art results, in terms of both quality and speed, on various benchmarks with low memory footprint. Methods # We have 4 steps for the method\n3D Gaussian Splatting Warm Up Spectral Graph Pruning Neural Compensation Continuous Pruning as a Strategy The overall framework is in Fig.\nFig. 4. The overall framework of SUNDAE with pipeline and graph-based pruning. Let’s see how each step works.\n1. 3D Gaussian Splatting Warm up # SUNDAE initialize Gaussian centers of the vanilla 3D Gaussian Splating as the first step for generating a dense representation using Gaussian primitives. Then, an effective densification strategy is used to increase the primitives.\n- Gaussian Primitive Initialization # The point cloud $P_c$ is the first input for representing the 3D scene using Gaussian primitives $P$. Then they turn 3D coordinates $x \\in P_c$ into Gaussian primitives $p \\in P$ by the following equation:\n$p(x)=exp(-1/2(x)^T\\Sigma^-1(x))$ where the $\\Sigma$ is defined as 3D covariance matrix. They also use an ellipsoid configuration. The decomposition of $\\Sigma$ is achieved with scaling matrix $S$ and a rotation matrix $R$, as expressed in the equation:\n$\\Sigma=RSS^TR^T$ - Gaussian Primitive Densification # They optimize all parameters of Gaussian primitives and integrate a densification strategy to improve representation power during the training process.\n2. Spectral Graph Pruning # After warm-up, a dense representation incurs significant storage consumption. To efficiently prune redundant primitives, they used the graph signal processing theory and construct a graph based on Gaussian primitives.\n- Graph Signal Processing Preliminaries # Graph shift: Graph shift represents the connections between nodes in a weighted graph, typically represented by a weighted adjacency matrix. It quantitatively describes the relationships between nodes using the weights of edges.\nGraph signal: Graph signal is a mapping assigning values to each node in a graph, utilized to model interactions between nodes.\nGraph Fourier Transform: Graph Fourier Transform is the process of expanding a graph signal using the eigenbasis of the graph shift, enabling analysis of the structure and interactions within the graph.\n- Graph Construction # Given a set of Gaussian Primitives %P% , they construct a nearest neighbor graph with the adjacent matrix $W$ of the graph:\n$$ W_{ij} = \\begin{cases} \\exp\\left( -\\frac{\\|x_i - x_j\\|^2}{2 \\sigma^2} \\right), \u0026 \\text{if } \\|x_i - x_j\\|^2 \u003c \\tau \\\\ 0, \u0026 \\text{otherwise} \\end{cases} $$ where $x_i$ and $x_j$ are central points in $P$, $\\tau$ is a hyperparameter, and $\\sigma$ is the variance of the distance matrix.\n- Graph Filtering and Sampling # SUNDAE propose a band-limited graph filter that combined with a high-frequency filter and low-frequency filter. By doing this, they can catch both the detailed information and general information. Design of filters are Haar-like.\nThey also prune the abundant primitives according to the response magnitude of the high-pass filter.\n3. Neural Compensation # There is a decrease in rendering quality for large pruning ratio. To address this, they employ a neural compresation network to model the relationship between primitives in the 2D domain.\nThey render the 3D Gaussian primitives into neural images in a differentiable manner, using the differentiable 3D Gaussian renderer from 3DGS with feature rendering instead of RGB rendering. The center of each Gaussian primitive is projected using a standard point rendering method, and the covariance in the neural image space is calculated.\nThe neural image is then computed using the feature vectors of Gaussian primitives. A lightweight neural network (U-Net with skip connections) is used to compensate for the quality drop after spectral pruning.\nThe overall optimization process is based on the difference between the rendered images and the ground truth images from the dataset. The compensation network and the 3D Gaussian primitives are optimized simultaneously during training, using a loss function that combines L1 loss and D-SSIM loss.\n4. Continuous Pruning as a Strategy # In addition to the training-then-pruning strategy, a continuous pruning strategy is explored. Continuous pruning periodically removes a specific number or percentage of primitives during the training process. This aims to lower peak memory usage and allow training on GPUs with lower memory. However, it can result in less predictable final memory usage, as the reduction may vary across different scenes. Therefore, continuous pruning is considered an alternative strategy when needed.\nResults # Quantitative Results # Table 1. Quatitative evaluation of SUNDAE. SUNDAE demonstrates strong performance across various metrics, including PSNR, SSIM, FPS, and memory usage.\nCompared to existing methods on the MipNeRF360 dataset, SUNDAE achieves a balance between rendering quality and efficiency, maintaining high FPS rates while significantly reducing memory consumption. Even at low sampling rates, SUNDAE remains competitive with established approaches, showcasing the effectiveness of its spectral pruning and neural compensation techniques in managing Gaussian primitive relationships and retaining scene information. Overall, SUNDAE represents scenes more compactly while maintaining high quality rendering. Qualitative Results # Fig5. Qualitative results of SUNDAE. The qualitative results demonstrate that SUNDAE achieves comparable novel view synthesis quality with significantly lower memory consumption (1% or 10%).\nThe graph effectively captures primitive relationships, while the neural compensation head preserves rendering quality. Spectral pruning notably removes outliers near the camera, enhancing scene coherence. Ablation Study # Fig6. Ablations experiment on the ratio 𝛾 of the bandlimited filter of graph based pruning. Band-limited ratio of Graph-based pruning: The band-limited filter\u0026rsquo;s ratio, represented by 𝛾, significantly impacts rendering quality, with a 𝛾 value of 50% yielding the most favorable outcomes, emphasizing the advantage of spectral pruning in preserving important high-frequency details and low-frequency background (Fig. 4). Table 2. Ablations of neural compensation module size. Fig. 7. Visualization with and without neural compensation. The compensation performance of the network: Employing the neural compensation module enhances performance across all sampling rates evide(Table 2, Fig. 5), highlighting its compensatory capability in mitigating performance drops caused by spectral pruning and effectively modeling the relationship between primitives. Neural Compensation Module Size: Increasing the size of the neural compensation module does not necessarily enhance rendering quality (Table 2), aligning with findings from ADOP and indicating a balance between quality and memory usage. Conclusion # They propose SUNDAE, a novel approach to spectrally prune Gaussian fields with neural compensation, efficiently capturing the relationship between Gaussian primitives using graph signal processing and blending information to offset pruning-induced information loss. By leveraging spatial information among Gaussian primitives to construct a graph and spectrally pruning less significant ones, they employ a lightweight neural network to compensate for quality degradation post-pruning. Experimental findings demonstrate SUNDAE\u0026rsquo;s ability to maintain the efficiency of 3DGS while significantly reducing its size across various scenarios. "},{"id":3,"href":"/docs/spring24/03_/","title":"03","section":"Spring24","content":" Boosting Efficiency in Deep Learning: Introducing Unit Scaling for Low-Precision Training # Paper : Unit Scaling Out-of-the-Box Low-Precision Training Author : Charlie Blake, Douglas Orr, Carlo Luschi posted by Seongrok Moon, Changyoung Ju Introduction # The significant advances in deep learning over the past decade have largely relied on the development of algorithms that efficiently leverage available hardware. As the size of state-of-the-art models increases, hardware efficiency becomes crucial for reducing training costs, which have grown substantially in terms of money, time, and environmental impact. However, with the end of Moore\u0026rsquo;s Law and Dennard scaling, increased transistor density alone cannot provide a straightforward path to greater efficiency. The use of low-precision number formats is a promising alternative. These formats offer substantial gains in compute, memory, and bandwidth efficiency, making them valuable in the context of modern deep learning.\nBackground # Floating-Point Formats for Deep Learning # Traditionally, floating-point numbers are defined by the IEEE 754 standard, which specifies the number of exponent bits (E) and mantissa bits (M). Common floating-point formats used in machine learning include FP32, TF32, BFLOAT16, and FP16. Recently, two types of FP8 formats (E4 and E5) have been proposed.\nTable A.1. Common floating point formats for deep learning Advantages and Disadvantages of Low-Precision Training # Disadvantages: FP16 and BFLOAT16 offer different trade-offs. FP16 has higher precision, but BFLOAT16 has a wider range. FP8 formats reduce both range and precision. The use of low-precision formats can introduce quantization noise and other issues. Advantages: Using low-precision formats can significantly improve efficiency in terms of memory usage, bandwidth usage, compute performance, and cross-device communication costs. Figure 2. The signal to noise ratio (SNR) of samples from a normal distribution, quantised in FP16 and FP8, as a function of the distribution’s scale Techniques for Low-Precision Training # Mixed Precision: This technique uses multiple number formats with different bit-widths, placing most activations, weights, and gradients in FP16 without loss of accuracy.\nLoss Scaling: To overcome the limited range of FP16 and FP8, the loss can be multiplied by a scalar to increase the scale of gradients. This method requires empirically finding a suitable loss scale:\n\\[ \\text{scaled\\_loss} = \\text{loss} \\times \\text{scale\\_factor} \\] \\[ \\text{scaled\\_gradients} = \\text{gradients} \\times \\text{scale\\_factor} \\] Automatic Loss Scaling: This dynamically adjusts the loss scale during training, removing the need to sweep for an initial loss scale.\nPer-Tensor Scaling: This system locally rescales based on runtime statistics to address scaling difficulties in FP8 training.\nTable 1. A comparison of techniques for low precision training Analysis # Ideal Scaling # The ability to predict the scale of tensors at the start of training is crucial. We argue that unit variance ( \\(\\sigma = 1\\) ) is an optimal balance among various competing factors. This approach helps concentrate values within the representable range, reducing clipping errors during training.\nIn floating-point formats, values are represented as: \\[ \\text{value} = (-1)^{b_{\\text{sign}}} \\times 2^{\\text{exponent}} \\times \\left(1 \u0026#43; \\frac{b_{\\text{mantissa}}}{2^M}\\right) \\] where \\(b_{\\text{sign}}\\) , \\(b_{\\text{exponent}}\\) , and \\(b_{\\text{mantissa}}\\) represent the sign, exponent, and mantissa bits, respectively.\nFigure 1. Above: Unit scaling of an FFN layer. We multiply each tensor by a fixed scalar to achieve consistent scale, no longer requiring a loss scale to control the scale of gradients. Below: A histogram of exponent values at initialisation for the above FFN Predictable Scaling # If we can predict the scale of tensors in a deep learning model, we can effectively address clipping errors. At initialization, parameters are drawn from known distributions, allowing us to analytically or empirically derive the scale of each tensor.\nFor example, by considering the scaling factors for each operation in the neural network, we can perform scaled operations:\n\\[ y = \\alpha \\cdot f(x) \\] where \\(\\alpha\\) is the scaling factor and \\(f\\) represents the operation.\nUnit Scaling # Unit scaling is proposed to address the limitations of existing methods for managing scale in typical models. A model is considered unit-scaled if its activations, weights, and gradients have approximately unit variance at initialization. This is achieved by inserting scaling factors into the forward and backward passes. Unlike loss scaling, which requires an empirically determined hyperparameter or an adaptive algorithm, unit scaling determines these scales based on a set of rules for each operation, approximately preserving the variance of the inputs. This leads to global unit scaling throughout the model, ensuring tensor values are centered within the exponent range at initialization, providing headroom during training to avoid going out of range.\nA framework for scaling computational graphs # Computational Graphs\nRepresent model by the differentiable function \\(f_{model}(x_1,...,x_m)\\) Describe the structure of such a model using a directed acyclic graph (DAG) denoted \\(\\mathcal{G} =(\\mathcal{V}, \\mathcal{E}) \\) This kind of graph is commonly known as a computational graph, with vertices as nodes and their corresponding functions as ops. Forward and backward graphs\nWe refer to the computational graph corresponding to \\(f_{model}\\) as the forward graph In deep learning we typically apply reverse-mode automatic differentiation to the forward graph to create a second computational graph whose output nodes represent the partial derivatives of the model with respect to its inputs: \\( \\frac{\\partial f_{model}}{\\partial x_i}, \\forall i \\in[1 . . m] \\) . We call this the backward graph Scaled ops\nGiven an op \\(f\\left(x_1, \\ldots, x_k\\right)\\) , we define the scaled op \\( f^*\\left(x_1, \\ldots, x_k, \\alpha, \\beta_1, \\ldots, \\beta_k\\right) \\) with scaling factors \\( \\alpha, \\beta_1, \\ldots, \\beta_k \\in \\mathbb{R}^{\u0026#43;} \\) , such that \\( f^{*} \u0026amp; \\triangleq \\alpha \\cdot f(x_1, \\ldots, x_k)\\) \\( f_{\\text {grad }}^{*}\\left(x_1, \\ldots x_k, g\\right)_i \u0026amp; \\triangleq \\beta_i \\cdot f_{\\text {grad }}\\left(x_1, \\ldots x_k, g\\right)_i, \\forall i \\in[1 . . k] \\) Scaled computational graph\nA scaled computational graph is one where every op \\(f\\) in the forward graph is replaced by a scaled equivalent \\(f^{*}\\) , with the backward graph then generated to produce \\(f^{*}_{grad}\\) grad for each \\(f_{grad}\\) , using any choice of scaling factors. Constraint-scaled computational graphs\nA constraint-scaled computational graph is a scaled computational graph where we restrict the scaling factors of ops that consume non-cut-edge variables in the following way: for any edge \\(e \\notin \\mathcal{C}\\) , we require the op consuming the variable \\(x_e\\) to have scaling factors \\(\\alpha = \\beta_e f\\) . Proposition 5.1\nFor any scaled op, there is an equivalent unscaled op with the same training dynamics under a firstorder optimiser.\nTheorem 5.2\nA constraint-scaled computational graph itself represents a scaled op.\nA scaling strategy for unit variance # Unit scaled computational graphs\nInitially set aside any scale constraints, and calculate the scaling factors that give each op expected unit variance outputs (this process is covered below). Now resolve any scale constraints by taking each constrained group \\( {\\alpha, \\beta_1, \\ldots, \\beta_l } \\) and selecting the geometric mean \\( \\left(\\alpha, \\beta_1, \\ldots, \\beta_l \\right)^\\frac{1}{l\u0026#43;1} \\) Selecting scaling factors\nAssuming unit-scaled inputs to \\( y = f(x_i,\\ldots,x_k) \\) , derive the output scale \\( \\sigma_Y \\) and set the forward scaling factor \\( \\alpha = 1/\\sigma_Y \\) . Repeat this process for \\( x_i\u0026#39;=f_{grad}(\\ldots)_i, \\forall i \\in[1 . . k] \\) , to obtain the gradient scale \\( \\sigma_{x_i\u0026#39;} \\) i and set the backward scaling factor \\( \\beta_i = 1/\\sigma_{x_i\u0026#39;} \\) . Weighted addition # When tensors of different scales, such as those in residual layers, losses, and positional encodings, are added, simply adding them can adversely affect performance. To address this, we propose using weighted_add. In this approach, we can maintain unit scale while performing operations using a scaled identity function.\nRecipe # We now outline a high-level recipe for a unit-scaled model:\nInitialise non-bias parameters with unit variance. Calculate scaling factors for all scaled ops. Identify non-cut-edges, and constrain the ops consumingthem to have \\( \\alpha = \\beta \\) by taking the geometric mean. Replace adds with weighted adds. Example # Using the unit scaling recipe, we first build a scaled op, and then a full scaled layer. Consider a scaled projection op with learnable weights:\n\\( \\operatorname{matmul}^*(X,W) =\\alpha \\cdot X W \\) \\( \\operatorname{matmul}_{\\text {grad }}^*(X, W, G)_1 = \\beta_1 \\cdot G W^{\\top} \\) \\( \\operatorname{matmul}_{\\text {grad }}^*(X, W, G)_2 = \\beta_2 \\cdot X^{\\top} G \\) for input \\( X \\in \\mathbb{R}^{b \\times m} \\) , weight \\( W \\in \\mathbb{R}^{m \\times n} \\) , output \\( \\mathbb{R}^{b \\times n} \\) and incoming gradients \\( G \\in \\mathbb{R}^{b \\times n} \\) We show code for the above in Figure 3, which also gives a scaled layer for the Transformer FFN\nFig3. PyTorch examples Results # Character language modelling\nExperimental Setup: Train causal language models on WikiText-103 raw character language modeling, using cross-entropy loss during training and evaluating on bits per character (BPC). Below the product of these settings, we compare the performance of regular (baseline) and unit scaling in both FP32 and FP16.\nSequence layer type: Attention, RNN and Convolution Norm placement: PreNorm, PostNorm and NoNorm Residual scaling: default, fixed and running-mean Results\nFirst, these demonstrate the need for scaling when using FP16. This is due to gradient underflow, since loss scaling with a factor of 2048 resolves the issue. Second, they demonstrate that unit scaling, despite changing the training behaviour of the model beyond just numerics, matches or even slightly improves upon baseline performance in almost all cases. Finally, they show that no tuning is necessary when switching unit scaling to FP16. suggest that running-mean or fixed are reasonable choices when using unit scaling Fig4. Character language modelling, showing validation bits per character over a wide range of models Masked language modelling\nExperimental Setup\nTo evaluate the advantages of unit scaling, we assess BERTBASE and BERTLARGE models, which typically struggle with loss scaling. Results\nTable2. Downstream performance of regular and unit-scaled BERT models Related Work # Variance scaling analysis\nVariance scaling and residual networks, along with normalization variants, complement unit scaling, which considers both activation and gradient norms. The reparameterization implied by unit scaling, utilized in analyzing deep network training dynamics, applies scaling factors locally throughout the compute graph, akin to training hyperparameter scaling. FP8 inference\nFP8 training lacks hardware support, yet accelerated 8-bit inference is becoming more prevalent through integer quantization to INT8. While this process often leads to reduced accuracy, recent efforts aim to enhance efficient INT8 quantization. FP8 adoption allows accelerated inference in the same format as training, promising significant improvements in the simplicity and accuracy of 8-bit inference. Discussion # Compute overhead\nUnit scaling introduces minimal compute overhead by adding scaling operations that can be fused into preceding operations, resulting in negligible memory-access cost. While basic loss scaling operates similarly, automatic loss scaling may incur additional overhead due to occasional batch discards, particularly noticeable in FP8. Proposed automatic per-tensor scaling schemes may introduce overhead, depending on software and hardware characteristics, as they trade off accuracy for complexity. In contrast, unit scaling with fixed precomputed scaling factors offers a simpler alternative without such complexities. Broader impact\nWith the potential for unit scaling to effectively train larger models, concerns arise about issues such as toxicity, misinformation, privacy concerns, and environmental damage. To address these challenges, various methods have been proposed, including AI feedback, anti-experts, and baked-in safety models. Conclusion Unit scaling has demonstrated to address the complexities of low-precision training, providing a simpler and more granular solution, even enabling the training of BERTLARGE without loss scaling for the first time, even in FP8.\n"},{"id":4,"href":"/docs/spring24/04_/","title":"04","section":"Spring24","content":" Better \u0026amp; Faster Large Language Models via Multi-token Prediction # Posted by Jinoh Cho and Seonghyeon Park\nAuthors: Gloeckle et al. Institution : FAIR at Meta, CERMICS Ecole des Ponts ParisTech and LISN Universite Paris-Saclay Preliminaries # Language Modeling and Next-Token Prediction Task # Learning through a next-token prediction task has been a mainstream for language modeling. The goal of a next-token prediction task is to maximize the probability of the next token $x_{t+1}$, given the history of previous tokens $x_{t:1} = x_1, \\ldots, x_t$. This can be formulated as follow:\n$$ L_1 = - \\sum_{t} \\log P_{\\theta}(x_{t+1} \\mid x_{t:1}), $$\nwhere $P_{\\theta}$ represents large language model under training.\nCore Idea # Multi-Token Prediction Task # In this work, authors propose to learn language modeling from a multi-token prediction rather than a next-token prediction. At each position of the training corpus, the model is instructed to predict $n$ future tokens at once. Thus, the training objective is changed as follow:\n$$ L_n = - \\sum_{t} \\log P_{\\theta}(x_{t+n:t+1} \\mid x_{t:1}) = - \\sum_{t}\\sum_{i=1}^{n} \\log P_{\\theta}(x_{t+i} \\mid x_{t:1}). $$\nMemory-Efficient Implementation # Directly training language models by minimizing the multi-token prediction loss could result in high GPU memory usage, severly limiting the allowable batch-size. Thus, authors propose to carefully adapt the sequence of forward and backward operations for each prediction head rather than operating forward and backword operations simultaneusly for all heads. This could result in reducing peak GPU memory usage $O(nV+d)$ into $O(V+d)$. Here, the $n$ and $V$ denote the number of head and vocabulary size, respectively. Note that $d$ is the vector dimension of shared transformer trunk.\nFaster Inference with Self-Speculative Decoding # For speed up in inference time, authors utilize self-speculative decoding (Stern et al., 2018) scheme. Specifically, instead of iteratively predicting a next single token for the given token sequence, authors directly generate n-token using n independent output heads in a single step. This significantly speed up the decoding stage.\nResult # Learning global patterns with multi-byte prediction # To show using multi-token prediction loss helps to capture global pattern than using next-token prediction loss, they include experiment using extreme case of byte-levle tokenization. Notably, as shown in the table 1, multi-token prediction (8-byte prediction) models significantly solve more problem in the case of trained on small number of data.\nCoding Benchmarks # Pretrained model with multi-token prediction loss maintains an edge on that with next-token prediction loss. At the beginning, they pretrain the 7B parameter models with multi-token prediction loss or next-token prediction loss. (Use the pretrained model on MBPP, HumanEval and APPS) Then, they finetune the models with CodeContests dataset (Li et al., 2022) with multi-token head or next-token head.\nWhy does it work? # Conclusion # Discussion # "},{"id":5,"href":"/docs/spring24/05_/","title":"05","section":"Spring24","content":" Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting # Author: Liu F, Tang Y, Liu Z, Ni Y, Han K, Wang Y Written by Nayoung Kwon and Jiwoong Im\nIntroduction # The growing demand for rapid and efficient inference in large language models (LLMs) faces a significant bottleneck\nDecoding \\(K\\) tokens requires \\(K\\) sequential runs of the model. ⇒ LLM inference is slow.\nTo address this issue, speculative decoding has been introduced as a promising approach to accelerate LLM inference without altering the output quality. This method leverages two key observations about LLM inference:\nMany tokens can be predicted with minimal computational overhead. LLM inference is predominantly constrained by memory bandwidth rather than arithmetic computations. Speculative decoding reduce the need for frequent memory operations on their parameters by focusing computational efforts on validating pre-drafted tokens, thus enhancing inference efficiency. However, existing speculative decoding such as Medusa and Lookahead still face limitations, such as high inference latency and suboptimal token acceptance rates. This papaer proposed Kangaroo to address this challenge.\nBackgrounds # What is speculative decoding? # Speculative decoding is an apporach to accelerate LLM inference.\nDraft model: Additional model to accelerate inference (also known drafter)\nVerifier or target model: Original large LLM\nFig. 1 Contrast to autoregressive decoding and speculative decoding Left model: The target LLM generates K tokens in K forward steps, which is a \u0026ldquo;serial\u0026rdquo; process.\nRight model: The drafter generates tokens in parallel. Each generated token is then verified with a verification step.\nSpeculative decoding can be implemented through methods such as independent drafting and self-drafting.\nIndependent Drafting: This approach uses a small language model (LM) from the same series as the target LLM.\nRequires additional training and increases computational complexity by integrating separate target and drafting models. Self-Speculative Decoding: This method utilizes the target LLM itself.\nEmploys techniques such as Blockwise Decoding, Medusa, and early exiting to reduce computational burden. Computational efficiency can also be achieved through layer skipping. Kangaroo: Self-speculative decoding # Kangaroo refers to the self-speculative decoding method, utilizing a fixed shallow sub-network of the original (target) large LLM.\nFig. 2 Comparison of variouus self-drafting speculative docding methods In each decoding step, drafted tokens must be verified in parallel to ensure alignment with the target LLM, which determines the token acceptance rate. High token acceptance rates are crucial for the efficiency of this process. However, methods like Medusa have yet to achieve satisfactory token acceptance rates, as evidenced by performance metrics (see left graph). On the other hand, the Lookahead method achieves a high token acceptance rate but has a very low speedup ratio (see right graph). Addressing these trade-off, **Kangaroo** offers a solution by training a lightweight and efficient adapter module integrated with a fixed subnetwork of the target LLM, enhancing both the acceptance rate and overall speedup. Layer Early Exiting # The author has proposed a novel self-speculative decoding framework, named Kangaroo. Kangaroo utilizes double early exiting mechanisms, layer early exiting and draft early exiting. Layer early exiting suggests the equivalent self-draft small model exiting early from the fixed shallow layers of the large LLM and connecting to an adapter network to generate draft tokens. While this strategy is commonly used for self-speculative decoding frameworks, Kangaroo has further investigated suitable architectures of the adapter module and offered a low-cost approach to train a lightweight model. Draft early exiting uses early exiting at suitable points during the drafting phase to avoid unnecessary computational overhead on more challenging tokens.\nEvaluation Metrics # Speculative decoding is often evaluated using two primary metrics: walltime speedup ratio and compression rate. Given a speculative decoding algorithm, we assume that \\(N\\) tokens should be generated via the drafting model. As the drafting model predicts multiple tokens in each decoding step and multiple tokens can be accepted by the large model in a step, we record the number of accepted tokens per step as a list \\( S = \\[s_1, s_2, \\dots, s_{|S|}\\] \\) , where \\( \\sum_k s_k = N \\) and \\( |S| \\) denotes the number of steps. Then, the compression rate (CR) is defined as: \\[\\text{CR} = \\frac{1}{|S|} \\sum_k s_k.\\] However, once a draft token is rejected during the verification, all subsequent tokens sampled from the drafting model will be discarded. Therefore, CR does not accurately reflect the acceptance levels for tokens at varying distances, and the author has proposed a new evaluation metric named consistent token acceptance rate.\nThe consistent token acceptance rate \\( \\text{CTAR}(w) \\) is calculated as: \\[\\text{CTAR}(w) = \\frac{1}{|S|} \\sum_k \\mathbb{I} (s_k - w \u0026gt; 0),\\] where \\(\\mathbb{I}(\\cdot)\\) denotes an indicator function and \\( w \\) denotes a window size. CTAR can be interpreted as a rate of the number of steps to accept over \\( w \\) tokens.\nFigure 1 represents the empirical CTARs for \\(w = 1,2,\\dots,6 \\) of self-drafting speculative decoding frameworks including Kangaroo on the mathematical reasoning subtask of Spec-Bench [1].\n[1] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851, 2024.\nAdapter Network as Self-Drafting Model # We assume the target LLM has \\( L \\) layers and the self-draft model \\( \\mathcal{M}^s \\) consists of shallow sub-network \\( \\mathcal{M}^b[:l] \\) , which is first \\( l \\) layers of the target LLM \\(\\mathcal{M}^b\\) , and a adapter network \\( \\mathcal{A} \\) . The drafting model reuses the LM head of the target LLM, and the overall parameters of the target LLM, such as shallow sub-network, remaining layers of LLM, and LM head, are frozen during the training of the adapter network. In Kangaroo, the adapter \\( \\mathcal{A} \\) only encompasses one multi-head attention and two normalization layers. The author emphasizes the feed-forward network (FFN) of the transformer block is too heavy in parameters but redundant, which is presented in the ablation study of the adapter architecture in the Experiments Section.\nFigure 2 illustrates the framework of Kangaroo. The lightweight drafting model \\( \\mathcal{M}^s \\) , including shallow sub-network and adapter network, predicts the draft tokens autoregressively until draft early exiting occurs. The strategy of draft early exiting will be explained later in the Draft Early Exiting Section. Then, the hidden states, computed in shallow sub-network, are processed in the remaining layers of LLM to generate prediction results. The draft tokens and the original prediction results are now compared for verification, and\nDraft Early Exiting # Experiments # Discussion and Conclusion # Kangaroo with a double early-exit mechanism ensures both efficiency and high performance.\nSeveral advantages:\nLow-Cost Training: The shared KV cache and computation between the self-speculative draft model and the large LLM → only the adapter network requires additional deployment. Efficiency: Experiments on Spec-Bench demonstrate that Kangaroo achieves up to 1.7× speedup, outperforming existing methods with significantly fewer additional parameters (67M compared to 591M for Medusa). Flexibility: By focusing on reducing inference latency and optimizing token acceptance rates, Kangaroo ensures that performance remains robust across various tasks without incurring substantial overhead. Compare with others:\nKangaroo\u0026rsquo;s performance surpasses other speculative decoding methods, such as Medusa and Lookahead, particularly in terms of end-to-end speedup and token acceptance rates (see Fig.2 in introduction). The double early-exit mechanism plays a crucial role in maintaining this balance by efficiently handling easier tokens and exiting early when confidence is lower than predefined threshold, thus minimizing latency.\nLimitations and future work: # Enhanced Confidence Metrics: Although Kangaroo introduces a confidence-based mechanism, it retains the original issue of discarding generated tokens that do not meet the confidence threshold. Currently, confidence is measured only on the top-1 prediction. Future work could explore alternative metrics such as entropy or other measures to provide a more robust assessment of token validity. Alternative Networks for Verification: The use of an adapter network in Kangaroo shows promising results. However, experimenting with different network architectures could yield even better performance. Future research could investigate various types of networks to replace the adapter, potentially improving both the efficiency and accuracy of the speculative decoding process. Expanding Beyond Self-Speculative Decoding: While Kangaroo leverages the target LLM for self-speculative decoding, exploring independent drafting models from the same series might offer additional insights. Balancing the computational trade-offs between these approaches could lead to more optimized frameworks. Adaptive Early-Exit Mechanisms: The current implementation of early exits in Kangaroo could be refined by dynamically adjusting the confidence thresholds based on the context or specific tasks. This adaptation could further reduce unnecessary computations and improve the overall efficiency of the model. "},{"id":6,"href":"/docs/spring24/06_/","title":"06","section":"Spring24","content":" # "},{"id":7,"href":"/docs/spring24/07_/","title":"07","section":"Spring24","content":" EECE695E_2024_Spring_Blog: VeRA: Vector-based Random Matrix Adaptation [1] # EECE695E_2024_Spring_Blog for Efficient Machine Learning Class w/ Sejin Park and Kyumin Cho\nIntroduction to LoRA (Low Rank Adaptation) [2] family of PEFT (Parameter Efficient Finetuning) # Large language Models or LLMs consists of at least billions of parameters. This makes it extremely expensive to train and finetune. For example, the weights of GPT-3 175B can take up to 350GB when stored in FP16 precision (2 bytes per FP16 x 175B=350GB). When training such models additional information such as the optimizer states and gradients are needed. Assuming FP32 training with AdamW optimizer a single weight parameter of the model, it requires 4 bytes to store the weight itself in FP32, 8 bytes per parameter to for the optimizer AdamW (where two states, first moment and second moment in FP32, are maintained for each parameter), and 4 bytes per parameter to store the gradient in FP32. This adds up to 16 bytes of storage space needed for each model parameter required for training. [3] This means that a full finetune of a small model such as Llama-3 8B can take 128GB (16 bytes x 8B = 128GB) just to store the parameters. This calculation excludes the forward activations as well as the training batch data. This makes even training relatively small models impossible on a single GPU as datacenter-class GPUs such as A100 or H100 max out at 80GB for GPU and especially for consumber level GPUs such as RTX 4090 which only has 24GB.\nNot only does the weights needs to be stored on the GPU VRAM during VRAM, each finetune version of the model needs to store the entire modified copy. This means even if mass-storage devices like HDDs are used, it becomes prohibitively impossible to store multiple custom finetune version of the data itself.\nTherefore parameter efficient finetuning or PEFT methods have been developed that is able to finetune and specialize LLMs by only using small amount of parameters, this not only reduces the number of GPUs required to train the model itself, it cuts down on the permanent storage capacity required to store multiple versions of it. The most popular of these approaches is low rank adaptation or LoRA. As its name suggests, this technique uses low-rank matricess to represent large matrices in LLMs. The hidden dimension size in LLMs gets very large with size with GPT-3 175B having a hidden dimension (d) of 12,288. By multiplying two matrices with extremely low rank (as low as 1 or 2), it is possible to represent a large matrix. By encoding changes to the original weight in this large matrix new versions of the model can be stored with a very small memory footprint. In the case of GPT-3 175B, the authors of the paper reported reduction as large as 10,000x (from 350GB to 35MB), with rank size of 4 and when being only applied $W_Q$ and $W_V$ projection matrices. This low rank reduction of changes is based on the assumption that the change in weights during finetuning has a low intrinsic rank, based on previous studies [7] [8] that claim that learned over-parameterized models reside in the low intrinsic dimension.\nSince only the differences to the original model are tracked in training, original model parameters can be frozen and only the small low-rank matricess need to be trained. Gradients or optimizer states don\u0026rsquo;t are not required for the original model, only for the small low-rank matricess, so this greatly reduces the GPU VRAM requirement. Also, when servicing large variations of custom finetune models, only a single copy of the large model needs to be stored and each version only needs to store the small low-rank matricess that represents the difference between the original weights. This makes servicing large number of variations feasible and makes switching model versions easy as only the small LoRA weights need to be loaded and merged without loading the entire model itself.\nLet the pre-trained weight matrix be $(W_o \\in \\mathbb{R}^{d \\times k})$.\nThe modified weight matrix is given by: $[W_o + \\Delta W = W_o + BA,]$ where $(B \\in \\mathbb{R}^{d \\times r})$, $(A \\in \\mathbb{R}^{r \\times k})$, $( \\text{rank } r \\ll \\min(d, k))$, and $(\\Delta W = BA)$.\nThe original forward pass is: $[h = W_o x]$\nThe modified forward pass is: $[h = W_o x + \\Delta W x = W_o x + BAx]$\nThis can be shown in the following diagram. (Insert Diagram of LoRA here)\nIn LoRA, $W_o$ matrix usually corresponds to $W_Q$, $W_K$, $W_V$, or $W_O$, query, key, value, and output projection matrices of attention as opposed to Feed Forward Networks (FFN) matrices as hidden size of FFNs tend to be much larger then projection matrices of attentions.\nDuring training $B$ can be initialized as 0 so that $\\Delta W = B A$ is also 0 when training starts.\nWhen LoRA weights are deployed the original weights and the LoRA weights can be merged, $W = W_o + B A $, before inference proceeds as usual. The original weights can be obtained by subtracting the LoRA weights ($B A$).\nUnlike other PEFT methods such as adapter layer insertion, LoRA adds no additional latency after the weights are merged as the forward inference operation is exactly the same and no additional operation needs to be performed. This contributed to the popularity of LoRA as no changes to the inference code needs to be made and only weight merging operations before inference are needed which is relatively quick and easy to perform.\nHow VeRA works # Even with parameter efficient nature of LoRA it still requires a non-trivial amount of storage for each version. If a custom version was wanted for each vendor or consumer the storage requirement can easily add up. Even for a PEFT technique like LoRA, a finetune version of GPT-3 175B with a low rank of 4 applied to only query and value projections needed several dozen megabytes in storage. If a custom version was to be stored for each users, a million users would amount to dozens of terabytes of storage. This limits the scalability of LoRA for personalization and demands an even more PEFT technique than LoRA which is where VeRA comes in.\nVeRA tries to take advanatage of random matrices and projection to reduce the number of unique parameters needed for each finetune. The idea is to take a pair of randomly initialized matrices and attach a pair of scaling vectors that reparameterize it. The randomly initialized matrices remain frozen while the scaling vectors are trainable. If we use the same keys when generating the random matrices through a PRNG (pseudorandom number generator). We do not need to store the random matrices and only need to store the smaller scaling vectors. This greatly reduces the storage requirement of VeRA and allows larger ranks without drastically increasing the storage requirement.\nThis table taken from [1] shows the relative storage efficiency of VeRA compared to LoRA. Under the assumption that LoRA and VeRA are applied to the query and key projection layers.\nModel Rank LoRA - # Trainable Parameters LoRA - Required Bytes VeRA - # Trainable Parameters VeRA - Required Bytes ${RoBERTa}_{\\text{base}}$ 1 36.8K 144KB 18.4K 72KB ${RoBERTa}_{\\text{base}}$ 16 589.8K 2MB 18.8K 74KB ${RoBERTa}_{\\text{base}}$ 256 9437.1K 36MB 24.5K 96KB ${RoBERTa}_{\\text{large}}$ 1 98.3K 384KB 49.2K 192KB ${RoBERTa}_{\\text{large}}$ 16 1572.8K 6MB 49.5K 195KB ${RoBERTa}_{\\text{large}}$ 256 25165.8K 96MB 61.4K 240KB GPT-3 1 4.7M 18MB 2.4M 9.1MB GPT-3 16 75.5M 288MB 2.8M 10.5MB GPT-3 256 1207.9M 4.6GB 8.7M 33MB Performance # Extensions # DVoRA (DoRA + VeRA) # DoRA or () [4] is a\nFuture Avenue of Research # Behavior of VeRA compared to LoRA # In the paper LoRA Learns Less and Forgets Less [5] the authors claim that LoRA underpeforms full finetuning but, tends to retain the base model performance better on tasks outside of the finetune training data. The authors posits that this is due to the fact that full finetuning tends to fine higher rank weight perturbations compared to LoRA. Considering that VeRA has even fewer tunable parameters and rank of VeRA can be increased more freely compared to LoRA it seems that it would be worthwhile to explore the behavior of VeRA compared to LoRA. The original VeRA paper either used relatively older encoder-based models such as RoBERTa, relatively ocarse evaluations such as GLUE or ROGUE, or relatively simple instruction tuning dataset (Alpaca Cleaned). This paper focuses much more on relevant and challenging LLM tasks such as\nFew of things that can be compared is:\nHow performance of VeRA fares compared to LoRA and full finetuning on target domain task performance. Does VeRA exhibit the same regularization characteristic as LoRA by forgetting less of the source domain? Does sample-efficiency suffers compared to LoRA and full finetuning? NAS (Neural Architecture Search) to VeRA # Better initialization settings # The initalization scheme used in VeRA is relatively simple. The original VeRA paper does present some exploration and ablation studies of initialization schemes. The authors claim that using both $d$ and $b$ scaling vectors improve performance, using Kaiming uniform initialization for the performance is better, and initializing $d$ vector with $d_init$ set to $10^-1$ or $10^-7$ tends to outperform 1.0.\nBut, they are relatively limited and focus on relatively old model (RoBERTa) and coarse benchmarks such as RTE, MRPC, CoLA, and STS-B tasks. Additional experiments on more relevant LLM tasks such as instruction finetuning or continued pretraining could be more insightful as well as more diverse modalities(vision, sound, et cetera). For example, LoRAs have become a popular in diffusion models such as Stable Diffusion as a way of generating custom images. It would be meaningful to explore the behavior and the best settings for VeRA in these type of applications and tasks.\nAlso, the fact that the rank can be scaled freely in VeRA with not much overhead was underexplored in the original paper. By varying and expanding the rank size to be much greater than what is feasible with LoRA it seems possible that VeRA could have higher rank perturbations compared to LoRA possibly leading to different behaviors. Varying the rank and the initializations of VeRA and comparing the SVD decomposition of VeRA, LoRA, and full finetuning seems like an underexplored topic. How different configurations of VeRA can change the behavior of the weight perturbations or how it relates to performance could be important for exploring how the weight features changes with finetuning. For example, the LoRA Learns Less and Forgets Less paper claims that on full finetuning on code and math the model does not learn low-rank perturbations unlike the original assumptions behind LoRA. Considering that VeRA is able to expand to much higher rank, SVD analysis of VeRA when trained on complex tasks like code and math could yield interesting results.\nFuture universal random weights # The Platonic Representation Hypothesis [6]\u0026hellip;. If large fundamental models share a common representation, it is possible that there could be an ideal way to represent the randomized matrix basis on which VeRA operates well in. https://arxiv.org/abs/2405.07987\nReferences # [1]: D. J. Kopiczko, T. Blankevoort, and Y. M. Asano, “VERA: Vector-based Random Matrix Adaptation,” arXiv.org, Oct. 17, 2023. https://arxiv.org/abs/2310.11454\n[2]: E. J. Hu et al., “LORA: Low-Rank adaptation of Large Language Models,” arXiv.org, Jun. 17, 2021. https://arxiv.org/abs/2106.09685\n[3]: “Efficient training on a single GPU.” https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one#anatomy-of-models-memory\n[4]: S.-Y. Liu et al., “DORA: Weight-Decomposed Low-Rank Adaptation,” arXiv.org, Feb. 14, 2024. https://arxiv.org/abs/2402.09353\n[5]: D. Biderman et al., “LORA learns less and forgets less,” arXiv.org, May 15, 2024. https://arxiv.org/abs/2405.09673\n[6]: M. Huh, B. Cheung, T. Wang, and P. Isola, “The platonic representation hypothesis,” arXiv.org, May 13, 2024. https://arxiv.org/abs/2405.07987\n[7]: C. Li, H. Farkhoor, R. Liu, and J. Yosinski, “Measuring the intrinsic dimension of objective landscapes,” arXiv.org, Apr. 24, 2018. https://arxiv.org/abs/1804.08838\n[8]: A. Aghajanyan, L. Zettlemoyer, and S. Gupta, “Intrinsic dimensionality explains the effectiveness of language model Fine-Tuning,” arXiv.org, Dec. 22, 2020. https://arxiv.org/abs/2012.13255\n"},{"id":8,"href":"/docs/spring24/08_/","title":"08","section":"Spring24","content":" MIXTURE OF LORA EXPERTS # LoRA is a methodology for effective fine-tuning large-scale pretrained models. LoRA is characterized by its ease of applying tuned results to existing models. This property encouragees research into synthesizing multiple trained LoRAs to achieve enhanced performance across various tasks such as linear arithmetic composition and reference tuning-based composition. However, combining these trained LoRAs poses significant two challenges:\nLinear arithmetic composition can diminish the capabilities of the original pre-trained models and the unique characteristics of the individually trained LoRAs, potentially leading to suboptimal results.\nReference tuning-based composition is limited in adaptability and incurs substantial computational costs, as it necessitates retraining a large model.\nSo, we can ask following question:\n_How can multiple trained LoRAs be composed dynamically and efficiently, preserving all their individual characteristics, without the need for retraining?_ To address this question, Mixture of LoRA Experts (MoLE) presents a new method for achieving the optimal combination of LoRAs for specific tasks. MoLE considers indivisual LoRA as an expert and determines the weights applied to LoRAs at each layer through a gate function.\nWorkflow of Mixture of LoRA Experts (MoLE) Background # What is LoRA? # Low-Rank Adaptation (LoRA) is a parameter-efficient and effective approach for fine-tuning large-scale pretrained models.\nModels such as OPT, LLaMA, and CLIP demonstrate remarkable performance when fine-tuned for various downstream tasks. However, full fine-tuning of these massive models requires substantial computational resources. LoRA enables parameter-efficient fine-tuning by keeping the pretrained model\u0026rsquo;s weights frozen and adding trainable low-rank decomposition matrices.\nLoRA Methodology In the above figure, only the matrices A and B are trained, with dimensions (d x r) and (r x d) respectively. By setting r \u0026laquo; d, the number of parameters to be trained can be reduced. These trained matrices are then simply added to the existing pretrained weights, allowing tuning without affecting the inference speed of the original model.\nLoRAs Composistion # The common solution to further improve the performance of LoRA across various tasks is to compose multiple trained LoRAs. Research on LoRA composition can be broadly categorized into the following two methodologies.\nLinear arithmetic composition. It is a method of directly adding multiple LoRAs. This approach is simple and has been effective in the NLP and Vision-Language domain, but it can result in the loss of pre-trained model\u0026rsquo;s generative capabilities or the individual characteristics of each LoRA. \\[$$\\hat{\\mathbf{W}} = \\mathbf{W} \u0026#43; \\sum_{i=1}^{N} w_i \\cdot \\Delta \\mathbf{W}_i$$\\] Reference tuning-based composition tackles the above limitations of linear arithmetic method by introducing gradient fusion and controllable sampling, but is requires retaining when incorporating different LoRAs or creating new masks, which results non-trivial computational costs. (Left) Linear arithmetic composition. (Right) Reference tuning-based composition Mixture-of-Experts # MoE is an effective method that allows scaling up the number of parameters while maintaining the computational cost of the model.\nExperts FFN Layers: MoE layer is composed of N separate feed-forward networks as the experts. This concept involves dividing the FFN layer of traditional transformers into N experts. These experts can be thought of as being responsible for specific tokens.\nGating functions (Router): A function that determines the weights over the experts outputs. For the hidden representation h of input token, and the trainable embedding e of each a expert, the gate value a is obtained as follow:\n\\[\\alpha(E_i) = \\frac{\\exp(h \\cdot e_i)}{\\sum_{j=0}^{N} \\exp(h \\cdot e_j)}\\] The output is a weighted sum of the outputs from the top-k experts, determined by the gated values.\n\\[O = h \u0026#43; \\sum_{i=0}^{N} \\alpha(E_i) \\cdot E_i(h)\\] Illustration of a Swith Transformer block. Mixture of LoRA experts # Motivations # Direct linear arithmetic composition reduced the generative power of the model, while normalized linear arithmetic composition retained the generative power of the model but lost its LORA character. (Left) Result of linear arithmetic composision. (Right) Result of nomalized linear arithmetic composision. In the V\u0026L domain, directly composing multiple trained LoRAs into the original embedding caused significant parameter variations and meaningless output, while normalization compromised their original characteristics. In the NLP domain, composing four or more LoRAs within the FLAN-T5 model resulted in disordered output, and weight normalization across five datasets decreased the performance, suggesting adverse effects on the intrinsic qualities of the trained LoRAs. 2. Each layer of the trained LoRA represented a unique characteristic, which cumulatively defined the overall properties of the LoRA. (Right: Observed that different layers of LoRA encode distinct features, such as dog coat color and facial features., left: When evaluated on a subset of datasets, there were significant differences in performance across the different layers of LoRA.) So, The conjecture is that adjusting the characteristics by varying the layer-specific weights according to the desired domain objective will result in a more effective composition of trained LORAs.\nMethod # See related formulas Symbols input $x \\in \\mathbb{R} ^ {L \\times d}$ L: sequence length d: dim of $x$ Multi attention layer : $$\\mathcal{f}_{Attn} (\\centerdot)$$ Feed forward neural network layer: $$\\mathcal{f}_{FFN} (\\centerdot)$$ LN: layer normalization Trained LORAs $$\\Omega = \\left\\{ \\Delta \\Theta \\right\\}^N_{i=0}$$ learnable gating function $$\\mathcal{G} (\\centerdot)$$ The weight of the $i^{th}$ trained LorA $$\\mathcal{G}_i (\\centerdot)$$ Concatenation operation: $$\\oplus$$ Learnable parameter $e \\in \\mathbb{R} ^ {N^2 \\times L \\times d}$ Learnable temperature scalar $\\tau$ Freezing part $$x^\\prime_{\\theta} = x + \\mathcal{f}_{Attn} (LN(x)|\\theta)$$ $$\\mathbf{F}_\\theta (x) = x^\\prime_{\\theta} + \\mathcal{f}_{Attn} (LN(x^\\prime_{\\theta})|\\theta)$$ LoRA part $$x^\\prime_{\\Delta \\Theta_i} = x + \\mathcal{f}_{Attn} (LN(x)|\\Delta \\Theta_i)$$ The output of each LoRA $$\\mathbf{E} _{\\Delta \\Theta_i} (x) = x^\\prime_{\\Delta \\Theta_i} + \\mathcal{f}_{FFN} (LN(x^\\prime_{\\Delta \\Theta_i})|\\Delta \\Theta_i)$$ The output of all LoRA $$\\mathbf{E}_\\Omega (x) = Normalization(\\mathbf{E}_{\\Delta \\Theta_0} (x) \\oplus \\ldots \\oplus \\mathbf{E}_{\\Delta \\Theta_{N-1}} (x)) \\in \\mathbb{R} ^ {N \\times L \\times d}$$ Flatten and dot product operation $$\\epsilon = Flatten(\\mathbf{E}_\\Omega (x))^T \\centerdot e, \\epsilon \\in \\mathbb{R} ^ N$$ Gate value for each LoRA $$\\mathcal{G} (\\epsilon_i) = \\frac {exp(^{\\epsilon_i} /_ \\tau)} {\\displaystyle\\sum_{j=1}^N {exp(^{\\epsilon_j} /_ \\tau)}} $$ Final output of the gating function $${\\tilde{\\mathbf{E}}_\\Omega (x)} = \\displaystyle\\sum_{i=0}^N {\\mathcal{G} (\\epsilon_i) \\centerdot \\mathbf{E} _{\\Delta \\Theta_i} (x)} , {\\tilde{\\mathbf{E}}_\\Omega (x)} \\in \\mathbb{R} ^ {L \\times d} $$ Final output of Transformer block $$\\mathcal{O}(x) = {\\mathbf{F}_\\theta (x)} + {\\tilde{\\mathbf{E}}_\\Omega(x)} $$ Training # The final loss function used in MoLE is as follows:\nAlpha is a coefficient for weight balancing. Gating Balacing Loss\nAs shown in Figure 5 (a), the average entropy of the distribution probabilities from the gating functions gradually decreases as training progresses. In Figure 5 (b), we can see a gating probability of 64% for LoRA β among the three LoRAs, indicating that the gating function tends to converge to a state where it assigns large weights to well-performing LoRAs in the early stages. This can result in a significantly larger impact from a few specific LoRAs compared to others, potentially leading to biased outcomes. To avoid this, the author created a gating balancing loss. The gating balancing loss helps prevent bias by ensuring that the loss value decreases as the model becomes less biased. See related Symbols M: The num of blocks where gating functions are placed N: num of LoRAs Domain-specific Loss In V\u0026amp;L, Using a loss in CLIP(Radford et al,20221b) In NLP, Using a loss in FLAN-T5(Chung et al,2022)\nResults # On V\u0026amp;L Domain Setup) Base generator: DeamBooth(Ruiz et al., 2023) (built on Stable Diffusion V2.1) LoRA: combination of three separately trained LoRAs Image resolution: 512x512 learning rate: 1e-5 DDPM sampler (Ho et al., 2020) with 50 steps in each case Train 400 iterations for each required composition with batch size 2 and α as 0.5 Metrics) Image alignment: Evaluate the visual similarity of generated images with individual composed concepts in the CLIP image feature space. Text alignment: Evaluate the text-image similarity of generated images with given text prompts in the CLIP feature space. For each composition, calculated the average scores among 200 generated images per prompt using 5 text prompts. Compared Baselines) Normalized linear arithmetic composition SVDiff (Han et al., 2023) Results) It demonstrates better performance compared to other models and shows outstanding results in other tasks as well. When viewing the generated images, it is evident that all specified subjects are accurately represented and maintained. On NLP Domain Setup) Base Model: Flan-T5 (Chung et al., 2022) LoRA: Several LoRAs based on FLAN datasets learning rate: 1e-5 Train 800 iterations for each required composition with batch size 12 and α as 0.5. Compared Baselines) LoRAhub PEMs Results) It can be observed that MoLE demonstrates better performance in most tasks. Analyisis and Limitations # "},{"id":9,"href":"/docs/spring24/09_/","title":"09","section":"Spring24","content":" MobileNetV4 - Universal Models for the Mobile Ecosystem # Posted by JoonSeok Kim and DongGyu Kim\nQin, Danfeng and Leichner, Chas and Delakis, Manolis and Fornoni, Marco and Luo, Shixin and Yang, Fan and Wang, Weijun and Banbury, Colby and Ye, Chengxi and Akin, Berkin and others arXiv preprint arXiv:2404.1051 Main Contributions # MobileNetV4 targets designing neural networks for mobile devices. Since the mobile platform can only offer limited compuation ability and DRAM utilization, software engineers are trying to design small and efficient neural networks. To use AI at the industry level, the inference latency must also be small. Main objectives of designing inference models for mobile devices are\nAcceptable test performance on widely-used datasets such as ImageNet-1k Low inference latency for utilization in mobile devices Minimization of the number of parameters for low memory utilization on mobile platforms Minimization in the number of MACs for high enegy efficiency This paper mainly focuses on lowering inference latency while maintining the test accuracy up to SOTA mobile neural net performance. Since it targets mobile platforms, it analyzes performance of various mobile hardwares, and designs a neural network to fit the harwares maximum performance. The designing process was done by the NAS technique, where the intantiation of UIB blocks were set as the search space. The main contributions of this work can be states as follows.\nUniversal Inverted Bottleneck (UIB) seach block - Unifies the Inverted Bottleneck (IB), ConvNext, Feed Forward Netowork (FFN), and Extra Depthwise variant Mobile MQA - Attention block tailored for mobile accelerators NAS technique to improve performance Achieves Pareto optimal acorss various devices such as CPUs, DSPs, GPUs, and TPUs Novel distillation technique to boost accuracy. Achieves 87% accuracy on ImageNet-1k and 39x smaller model size Preliminaries - Inverted Residual Blocks and Linear Bottlenecks # Fig. 0 (a) Original Residual Block Previously, the residual bottleneck block was propsoed, which consists of the 1x1 pointwise convolution in the first layer, a depthwise convolution in the second layer, and the final pointwise convolution layer, where its output is residually connected with the module\u0026rsquo;s input. The first layer acts as a projection layer to generate the narrow, and parameter-efficient convolution layer for depthwise convolution. This part acts as the bottleneck layer. The output of this layer is expanded again in the pointwise convolution. Here, the module forms a wide-narrow-widw approach considering the number of channels.\nFig. 0 (b) Inverted Residual Blocks However, in MobileNetV2, the authors use the inverted residual blocks with linear bottlenecks. In opposed to the original residual block where the first pointwise convolution acts as the projection layer and the final pointwise convolution acts as the expansion layer, the inverted residual block applied this in the opposite way to create the narrow-widw-narrow block. This assumes that the low-dimensional feature data is stored in the \u0026ldquo;narrow\u0026rdquo; layers, and these need to be expanded and extracted through the depthwise convolution. The required information in the narrow layers are passed onto the deeper layers through residual connections. Also, one has to take note that the final pointwise convolution layer in each inverted residual blocks do not have activations, in order to compensate the performnace degradation due to squeezing the layers where the skip connections are linked.\nPreliminaries - Roofline Model and Hardware Efficiency # Algorithm running on hardware is composed of two parts - memory access and computation. The computation time is determined by the computation requirement and hardware performance.\n\\[ \\text{runtime\\_computation} = \\frac{\\text{Number of Operations}}{\\text{FLOPS}} \\] Algorithm runtime can be limited by the memory access bottleneck or communication overhead\n\\[ \\text{runtime\\_communication} = \\frac{\\text{Number of IO Bytes}}{\\text{Bandwidth}} \\] Hardware performance is determined by the upper bound of the computation time or memory access latency\n\\[ \\text{performance} = \\max(\\text{runtime\\_computation}, \\text{runtime\\_communication}) \\] Below Fig. 1(a) and Fig. 1(b) illustrates the roofline model and its characteristics\nFig. 1 (a) Roofline Model Fig. 1 (b) Roofline Model with Ceiling Hardware-Independent Pareto Efficiency # Fig. 2. Ridge Points and Latency/Accuracy Tradeoffs Fig. 3. Op Cost vs. Ridge Point This research focuses on efficiency on various hardware targets such as DSPs, CPUs, and GPUs. To find whether the hardware is limited by its memory bottlenecked or compute bottlenecked, the Roofline Model of that hardware must be investigated. It is defined by the harware's peak computational throughput and its peak memory bandwidth. The optima of that hardware can be found in its ridge point, which is defined by the hardware's ratio of Peak MACs to Peak memory bandwidth. The algorithm's accuracy and latency are swept by the ridge point on various hardware on Fig. 2, and Fig. 3. The roodline model of MobileNetV4 achieves highest Pareto-optimal performance compared to other MobileNet models. MobileNetV4 is designed to achieve Pareto optimal and hence balances MAC operations and memory bandwidth. The initial layers are designed with high MAC intensity, so as to improve model capacity and downstream accuracy. The end layers use identically-sized FC layers to maximize accuracy. These two initial and end layers are balances so that MobileNetV4 should not see slowdowns at any hardware.\nUniversal Inverted Bottlenecks (UIB) # Fig. 4. Universal Inverted Bottleneck (UIB) blocks The main advantage of UIB is its adaptability and flexibility, that mitigates seach complexity. Optional Depthwise (DW) convolution blocks are inserted before the expansion layer, and between the expansion and projection layer. In the NAS procedure, common components such as the pointwise expansion and projection are shared and DWs are added as search options. UIB has four possible instantiations as follows.\nInverted Bottleneck (IB) : Spatial mixing on the expanded features activations, and provides higher model capacity ConvNext : Cheaper spatial mixing before the expansion with larger kernel size ExtraDW : Inexpensive increase of the network depth and the receptive field. Combined benefits of ConvNext and IB FFN : Stack of two 1x1 pointwise convolutions. Accelerator-friendly operation Mobile MQA # Table 1. Efficiency Gains by MQA This paper considers the Operational Intensity (OI), which is the ratio of arithmetic operations to memory access, to enhance efficiency of vision models on mobile accelerators. Here, Multi-Query Attention (MQA) is proposed instead od Multi-Head Self Attention (MHSA), which is simplified by utilization of shared keys and values across all heads. This sharing of keys and values reduces memory access hence improving OI, especially when the batch size is small. Large language models does not have significant accuracy drop in this MQA case. Table 1 shows that by adding MHSA and MQA, the performace accuracy has increased whereas the inference latency for MQA is approximately x39 lower than that of MHSA. Hence, MQA can accelerate better in the mobile environment, with negligible performance degradation.\nThe Spatial Reduction Attention (SRA) is applied, hence incorporating asymmetric spatial down-sampling, to downscale keys and values, and not queries. In hybrid models, there is a certain correlation between spatially adjacent tokens, hence necessitating spatial mixing convolution filters.\nRefined NAS for Enhanced Architectures # As shown above, the insitantiation of UIB blocks are in the neural architecture search process. TuNAS was adopted for the paper\u0026rsquo;s search strategy. The paper uses a two-stage search operation, the coarse-grained search and fine-grained serach to address the variance in parameter counts between UIB\u0026rsquo;s depthwise layers and other search options. The course-grained search process involves determining optimal filter sizes with fixed parameters. The fine-grained stage searches for the UIB\u0026rsquo;s layer configuration.\nResults - Comparisons with Other Works # Table 5. Classification results on ImageNet-1k Table 6. Object Detection results on the COCO validation set Results on the classification performance on ImageNet-1k dataset show that the MobileNetV4 achieves the highest performance and smallest latency compareed to other models on various mobile platforms such as CPUs and DSPs of mobile phones. While other models have closely competitive latency with the MobileNetV4 model, their latency is much higher.\nThe effectiveness of MobileNetV4 as backbone networks are tested on the COCO object detection experiment. The number of MACs were set to be similiar, and the Retina framework was used as the object detector. As the same as classification, MobileNetV4 achieves highest performance compared to other mobile-target modles, with the lowest CPU latency. Hence, the ability of MobileNetV4 for mobile devices can be shown.\nConclusion # This paper proposes the MobileNet V4 series, a universal high-efficiency model that operates efficiently across a wide range of mobile environments. By introducing a new Universal Inverted Bottleneck and Mobile MQA layer and applying an enhanced NAS recipe, MobileNet V4 achieves near Pareto-optimal performance on various hardware, including mobile CPUs, GPUs, DSPs, and dedicated accelerators. Additionally, using the latest distillation techniques, it demonstrates cutting-edge performance in mobile computer vision by achieving 87% ImageNet-1K accuracy with a latency of 3.8ms on the Pixel 8 EdgeTPU. The paper also presents a theoretical framework and analysis for understanding the model\u0026rsquo;s universality across heterogeneous devices, providing guidance for future design. Perspectives, Discussions and Future Research Directions # MobileNetV4 was designed with a focus on optimizing hardware performance, particularly for mobile devices such as mobile CPUs, GPUs, and DSPs. It leveraged Neural Architecture Search (NAS) to design its Unit Inverted Bottleneck (UIB) blocks and employed distillation techniques to enhance performance. Additionally, MobileNetV4 used MQA to incorporate transformer-like operations, further boosting its performance. The authors aimed to integrate as many state-of-the-art techniques as possible to optimize the neural network\u0026rsquo;s performance in mobile environments. Given its recent development, MobileNetV4 is likely the state-of-the-art convolution-based neural network for mobile platforms.\nThe novelty of this work appears to be the neural architecture search involving the UIB, which includes the expansion and projection blocks with inverted residuals previously introduced in MobileNetV2. Since the performance is validated across various mobile platforms, MobileNetV4 presents an attractive solution for industry applications in mobile systems. However, the paper seems to combine existing methods, techniques, and modules into one comprehensive experiment to produce the optimal MobileNet model. The optimization was conducted logically and practically, but it would have been helpful if the authors had provided an analysis of the finalized MobileNetV4, explaining why NAS designed the UIB in that specific way and how features are extracted at each layer.\nIt is worth noting that many academic labs are currently designing mobile-level transformers like MobileViT or FastViT. This paper shows that these transformer-based models have similar performance to MobileNetV4 but with significantly higher inference latency. Although the parameter counts and the number of MAC operations are comparable, the latency difference raises questions about the practicality of developing mobile-level transformers for vision tasks. Despite incorporating MQA blocks, it appears beneficial to use bottleneck modules as the primary feature extractors.\nBeyond MobileNetV4, many CNNs are integrating multi-head attention layers, while transformers are incorporating convolutions. Convolutions capture local feature relationships, whereas attention modules provide global features in computer vision. Combining these two characteristics enhances neural network performance across the board. In the future, it would be practical to design NPUs and domain-specific accelerators that enable fast and efficient computation for both convolutions and attention mechanisms simultaneously.\nSimilar Works - MobileFormer # Fig. 5. MobileFormer Network A similiar work that utilizes the MobileNet and trasnformer modules was introduced in CVPR 2022, the Mobile-Former. This structure capitalizes on MobileNet\u0026rsquo;s strength in local processing and the transformer\u0026rsquo;s capability for global interaction. The bridge facilitates bidirectional fusion of local and global features. Unlike recent vision transformer approaches, the transformer in Mobile-Former uses very few tokens (typically 6 or fewer), which are randomly initialized to learn global priors, thereby minimizing computational cost. Additionally, the proposed lightweight cross-attention mechanism used to model the bridge enhances both computational efficiency and representation power.\nTable. 7. Performance comparison of MobileFormer with other works Mobile-Former demonstrates superior performance compared to MobileNetV3 in the low FLOP regime, ranging from 25M to 500M FLOPs on ImageNet classification. For example, Mobile-Former achieves 77.9% top-1 accuracy at 294M FLOPs, surpassing MobileNetV3 by 1.3% while reducing computations by 17%. In object detection tasks, Mobile-Former outperforms MobileNetV3 by 8.6 AP within the RetinaNet framework. Furthermore, when applied to the DETR model, replacing its backbone, encoder, and decoder with Mobile-Former results in a detector that not only surpasses DETR by 1.1 AP but also reduces computational cost by 52% and parameter count by 36%.\nReferences # [1] MobileNetV4 Implementation: [Link.](https://github.com/jiaowoguanren0615/MobileNetV4/tree/main)\n[2] Qin, Danfeng, et al. \u0026ldquo;MobileNetV4-Universal Models for the Mobile Ecosystem.\u0026rdquo; arXiv preprint arXiv:2404.10518 (2024).\n[3] Sandler, Mark, et al. \u0026ldquo;Mobilenetv2: Inverted residuals and linear bottlenecks.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n[4] Chen, Yinpeng, et al. \u0026ldquo;Mobile-former: Bridging mobilenet and transformer.\u0026rdquo; Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n"},{"id":10,"href":"/docs/spring24/10_/","title":"10","section":"Spring24","content":" Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length # Authors: Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou\nReviewer: Hyunho Kook\n1. Introduction # Recently, Large Language Models (LLMs) have been gaining popularity. The impressive performance and versatility demonstrated by large models above a certain level have started to be utilized in various fields. However, as the size of the models grows, the size of the data that the models are expected to process is also increasing. Examples of this include processing currently open issues by inputting a GitHub repository or translating a large volume of books without losing context. In addition, the ability to maintain context and carry on a conversation for an extended period within a single chat is also sometimes required. The transformer, which is the foundation model of modern LLMs, exhibits vulnerabilities in this regard. Firstly, since it uses KV cache, memory usage increases rapidly as the sequence length grows, and it has a computational complexity proportional to the square of the sequence length.\nTo address this problem, the authors propose a method that inherits and advances MEGA (exponential moving average with gated attention), the predecessor of this paper. The overall contributions are as follows:\nCEMA (Extending Multi-dimensional Damped EMA to Complex Domain), an extension of Exponential Moving Average (EMA) to the complex domain, is proposed. Timestep Normalization, an extension of Group Norm to the timestep domain, is proposed as an alternative to Layer Norm. Normalized Attention, which performs normalization during attention computation, is proposed. 2-hop Residual Connection, which composes residual connections in 2-hop units, is proposed. By employing these methods, the authors have created a transformer architecture that is linear with respect to context length. They have also addressed the issues encountered in the previous research, MEGA, which were (i) low performance and (ii) the need for different architecture structures for each data type or task.\n2. MEGA (exponential Moving avErage with Gated Attention) # Before diving into the main contributions of our work, it\u0026rsquo;s essential to grasp the concepts behind MEGA (Moving Average with Gating Attention), a technique that serves as a stepping stone towards more efficient transformer models. MEGA leverages the moving average with damping to enhance the attention mechanism. Let\u0026rsquo;s break it down step by step.\n2.a Moving Average with Damping # At the core of MEGA lies the moving average with damping, which can be expressed by the following equations:\n$$ \\begin{aligned} u_t^{(j)} \u0026amp;= \\beta_j x_{t,j} \\ h_t^{(j)} \u0026amp;= \\alpha_j \\odot u_t^{(j)} + (1 - \\alpha_j \\odot \\delta_j) \\odot h_{t-1}^{(j)} \\ y_{t,j} \u0026amp;= \\eta_j^T h_t^{(j)} \\end{aligned} $$\nThis exponential moving average (EMA) serves as the foundation for the subsequent attention mechanism.\n2.b Generating Query and Key # The EMA is then fed into the attention mechanism to generate the Query and Key matrices after applying the SiLU activation function. First, we calculate the shared representation $Z$ using the EMA of the input $X$:\n$$ \\begin{aligned} X\u0026rsquo; \u0026amp;= EMA(X) \\in \\mathbb{R}^{n \\times d} \\ Z \u0026amp;= \\phi_{silu}(X\u0026rsquo;W_z + b_z) \\in \\mathbb{R}^{n \\times z} \\end{aligned} $$\nUsing the shared representation $Z$, we can now compute the queries and keys. It\u0026rsquo;s important to note that the original input $X$ is used to calculate the values:\n$$ \\begin{aligned} Q \u0026amp;= \\kappa_q \\odot Z + \\mu_q \\in \\mathbb{R}^{n \\times z}\\ K \u0026amp;= \\kappa_k \\odot Z + \\mu_k \\in \\mathbb{R}^{n \\times z}\\ V \u0026amp;= \\phi_{silu}(XW_v + b_v) \\in \\mathbb{R}^{n \\times v} \\end{aligned} $$\n2.c Attention Output Calculation # After obtaining the Query, Key, and Value matrices, we can calculate the output of the attention mechanism using the standard formula:\n$$ O = f\\left(\\frac{QK^T}{\\tau(X)}\\right)V \\in \\mathbb{R}^{n \\times v} $$\nHere, $f$ is typically the softmax function, and $\\tau$ is a temperature function based on the input $X$.\n2.d Gating Mechanisms # MEGA employs gating mechanisms, similar to the Gated Linear Unit (GLU), to generate the final output $Y$ and the internal activation $\\hat{H}$. This is achieved through the following equations:\n$$ \\begin{aligned} \\gamma \u0026amp;= \\phi_{silu}(X\u0026rsquo;W_\\gamma + b_\\gamma) \\in \\mathbb{R}^{n \\times v} \\ \\varphi \u0026amp;= \\phi_{sigmoid}(X\u0026rsquo;W_\\varphi + b_\\varphi) \\in \\mathbb{R}^{n \\times d} \\ \\hat{H} \u0026amp;= \\phi_{silu}(X\u0026rsquo;W_h + (\\gamma \\odot O)U_h + b_h) \\in \\mathbb{R}^{n \\times d} \\ Y \u0026amp;= \\varphi \\odot \\hat{H} + (1 - \\varphi) \\odot X \\in \\mathbb{R}^{n \\times d} \\end{aligned} $$\n2.e Chunk-wise Attention for Linear Time Complexity # To reduce the time complexity to a linear function of O(nc), MEGA employs chunk-wise attention, as illustrated in the figure above. By maintaining an internal accumulation of information and processing the input in chunks, MEGA enables transformers to achieve comparable performance on long context datasets.\n2.f Limitations and Future Directions # Despite its benefits, MEGA still faces two primary challenges:\nPerformance falls short compared to typical transformers with full attention. Different architectures are required for different tasks. Moreover, the effectiveness of MEGA in large-scale networks remains to be demonstrated conclusively.\nIn the following sections, we will explore how our proposed approach addresses these limitations and pushes the boundaries of efficient transformer models.\n3. Methods # Now, this paper, Megalodon, proposes four different methods that makes a network have even better performance than the baseline, while it can be applied for general cases.\n3.a CEMA (Extending Multi-dimensional Damped EMA to Complex Domain) # To improve EMA (Exponential Moving Average) capability when working over the complex number system, the authors propose CEMA (Complex Exponential Moving Average), which extends the idea of EMA to the complex plane. The key equations for CEMA are:\n$$ \\begin{aligned} h_t^{(j)} \u0026amp;= \\alpha_j(\\cos \\theta_j + i \\sin \\theta_j) \\odot u_t^{(j)} + (1 - \\alpha_j \\odot \\delta_j)(\\cos \\theta_j + i \\sin \\theta_j) \\odot h_{t-1}^{(j)} \\ y_{t,j} \u0026amp;= \\mathcal{Re}(\\eta_j^T h_t^{(j)}) \\end{aligned} $$\nHere, $\\alpha$ and $\\delta$ are real-valued parameters, just like in the original EMA. However, $\\eta$ is a complex-valued parameter in CEMA. The $\\theta_j$ values are learnable parameters that are used to uniformly distribute the $h$ arguments over the period of $2\\pi$. This is achieved by parameterizing $\\theta_j$ as:\n$$ \\theta_{j,k} = \\frac{2\\pi k}{h} \\omega_j, \\quad \\forall k \\in {1, 2, \\dots, h} $$\nwhere $\\omega$ is a learnable parameter that determines the base angles.\nThe introduction of complex numbers in CEMA allows it to capture positional features of internal embeddings without changing the magnitude of $h$. Instead, it periodically changes the angle in the complex domain. This makes CEMA a powerful tool for modeling long sequences and improving the capability of EMA in complex-valued systems.\n3.b Timestep Normalization # Transformer-based models typically use Layer Normalization (LayerNorm) instead of Batch Normalization (BatchNorm) due to the significant variation in input batch sizes across different timesteps. While this approach has led to great success in large language models, the authors argue that LayerNorm cannot effectively capture and reduce the internal covariance along the temporal domain. To address this issue, they propose using a GroupNorm-like method. However, applying GroupNorm directly is challenging because it cannot access future information during auto-regressive inference.\nTo overcome this obstacle, the authors introduce Timestep Normalization, a superset of GroupNorm. This technique divides the total timesteps into $k$ groups and applies group normalization to each group independently. By doing so, Timestep Normalization can reduce the covariance shift along the temporal domain while still being compatible with the causal nature of auto-regressive models.\n3.c Normalized Attention # During the calculation of attentions, the results can have a hugh instability along the temporal domain because the value of CEMA keeps changing. Therfore, the authors propose to use normalizaion on $Z$ before calculating the queries and keys. In this case, we do not have to scale the $QK^{T}$ since the values are already normalized while improving the stability of the attention mechanism.\n$$ \\begin{aligned} X\u0026rsquo; \u0026amp;= \\mathcal{CEMA}(X) \u0026amp;\u0026amp; \\in \\mathbb{R}^{n \\times d} \\ Z \u0026amp;= X\u0026rsquo;W_z + b_z, \\quad Z\u0026rsquo; = \\frac{Z}{|Z|} \u0026amp;\u0026amp; \\in \\mathbb{R}^{n \\times z} \\ Q \u0026amp;= \\kappa_q \\odot Z\u0026rsquo; + \\mu_q \u0026amp;\u0026amp; \\in \\mathbb{R}^{n \\times z} \\ K \u0026amp;= \\kappa_k \\odot Z\u0026rsquo; + \\mu_k \u0026amp;\u0026amp; \\in \\mathbb{R}^{n \\times z} \\ O \u0026amp;= f_{\\text{softmax}}\\left(QK^T\\right)V \u0026amp;\u0026amp; \\in \\mathbb{R}^{n \\times v} \\end{aligned} $$\n3.d 2-hop Residual Connection # Finnally, the last contribution of this paper is the use of the 2-hop residual connection. The authors point out that pre-normalizaion can achieve more stable training in deep learning, this can further lead instability when scaling up the model size. Therefore, to reduce the instability, this paper uses 2-hop residual connections which propagate the input to the end of the layer without modifying it.\n3.e Summary of Methods # The main contribution of this paper lies in the application of CEMA (Complex Exponential Moving Average), which effectively captures the positional information of internal embeddings ($H$) by incorporating a circular system in the complex domain. By using periodic functions like sine and cosine, CEMA can encode the sequential nature of the input data, allowing the model to better understand and utilize the temporal dependencies.\nHowever, the introduction of complex numbers in CEMA doubles the dimensionality of the variables, which can potentially lead to significant instability during the training process. To address this issue, the authors propose three additional methods: Timestep Normalization, Normalized Attention, and 2-hop Residual Connection. These techniques work in conjunction with CEMA to stabilize the training process and ensure the model\u0026rsquo;s robustness.\nTimestep Normalization is employed to reduce the covariance shift along the temporal domain while maintaining compatibility with the causal nature of auto-regressive models. By normalizing the activations within each timestep group, this method helps to mitigate the impact of the increased dimensionality introduced by CEMA.\nNormalized Attention is used to stabilize the attention mechanism by properly scaling the attention weights. This prevents extreme values that could otherwise destabilize the training process, ensuring that the model can learn effectively from the input data.\n2-hop Residual Connection provides a more direct path for gradient flow, allowing the model to propagate information more efficiently during the backward pass. This helps to alleviate the vanishing gradient problem and facilitates faster convergence of the model.\nThese three methods work in harmony with CEMA to counterbalance the potential instability caused by the increased dimensionality. By carefully integrating these techniques, the authors aim to create a robust and effective model that can leverage the benefits of CEMA while maintaining stable training dynamics.\nIn essence, while CEMA is the primary contribution of this paper, the additional methods proposed by the authors play a crucial role in ensuring the model\u0026rsquo;s stability and performance. The combination of CEMA with Timestep Normalization, Normalized Attention, and 2-hop Residual Connection demonstrates a well-rounded approach to tackling the challenges associated with modeling sequential data in the complex domain.\n4. Experiments # In most cases, the authors try to compare the proposed idea with the LLAMA2-7B model. They use MEGALODON-7B model and use the same amount of training tokens to pre-train it. They try to show that MEGALODON can not only achieve the higher performance on long context datasets, but also show superior skills on short context problems than a naive transformer architecture.\n4.a Short-Context Evaluation # The authors first show the loss during training and the performance on short context datasets during inference. The proposed method can achieve training loss between LLAMA-7b and LLAMA-13b with 7b parameters. Furthermore, if you look at the table 1, then during inference the MEGALODON-7B truly show the performance between LLAMA-7b and LLAMA-13b. This proves that MEGALODON can ahieve better performance than the original model unlike the previous work, MEGA.\n4.b Long-Context Evaluation # To verify the performance on long context datasets, the authors provide two results. The figure 5 describes that if the context length increases, then the MEGALODON can truly output lower PPL answers. Then, the table 2 shows the performance on a long context benchmark, Scrolls. Although the LLAMA2 can show slightly better performance when it is finetuned for long context datasets with 500B tokens, the MEGALODON can show comparable accuracies without applying any fine-tuning techniques.\n4.c Instuction Finetuning # The authors also prove that MEGALODON can have a generalization power by testing the pre-trained model on MT-Bench. They do not utilized further techniques such as RLHF during fine-tuning for a specific instruction. They show a comparable or sometimes competitive performance.\n4.d Medium-scale Benchmarks # Finnally, the authors prove that the MEGALODON can show competitive performance compared to the previous long-context-agnostic models on general benchmarks. Inside the table 4 and 5, the MEGALODON achieves highest performance on ImageNet classifcation task and medium-scale benchmakr (PG-19).\n4.e Summary of Experiments # In the experiment part, the authors try to prove that the MEGALODON can have the features that have been proven for the previous foundation models, while showing superior performance on long context benchmarks. This implies that the MEGALODON is not a sophisticately tuned model for long contexts.\n5. Comparison with related works # There have been several similar related studies:\nEfficient Attention: FlashAttention optimized the GPU computation of the attention, showing advantages in speed without changing the existing mechanism. Additionally, there have been attempts to increase the context length by converting the attention mechanism to a linear one or compressing the KV cache.\nStructured State Space Model: A notable study in this area is Mamba, which added mechanisms like Selective Scan to a State Space model with linear time complexity, enabling it to process long context.\nHowever, these studies have limitations. Even if we accept that Flash Attention did not change the attention mechanism itself, Linear Attention, KV cache compression, and State Space Models have shown significantly lower performance on general benchmarks, although they may perform better than standard Transformers in long contexts.\n6. Discussion # In my opinion, there are a few potential limitations that are not extensively discussed in the paper:\nReliance on CEMA for Out-of-Chunk Context: The self-attention mechanism in MEGALODON is applied within each chunk. For data that falls completely outside the chunk boundaries, the model relies solely on CEMA for processing. This limitation could potentially hinder the model\u0026rsquo;s ability to handle long-range dependencies that span across multiple chunks.\nComplexity of the Architecture: Compared to the traditional Transformer layer, the MEGALODON architecture is considerably more complex. It requires the computation of EMA, including the complex domain, for each token. Additionally, several normalization and attention components have been introduced, such as Timestep Normalization, which further increases the complexity of the model compared to the previous works.\nLimited Exploration of Downstream Tasks: While the paper demonstrates the effectiveness of MEGALODON on long-context question answering tasks from the Scrolls dataset, the range of downstream tasks explored is relatively narrow. Evaluating the model\u0026rsquo;s performance on a broader set of tasks, such as summarization, dialogue generation, and composition, would provide a more comprehensive assessment of its capabilities and potential limitations.\nDespite these limitations, MEGALODON presents a promising direction for efficient long-context modeling. In my opinion, this kind of efficent and linear processing of memory can be a breakthrough for long-context LLMs.\n"},{"id":11,"href":"/docs/spring24/11_/","title":"11","section":"Spring24","content":" Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention # *Team: Younghyun Cho, Sangjun Lee\nLarge Language Models with Inifinite Input Sequence # Currently, Large Language Models (LLM) are based on Transformer architecture [1], which utilize interactions over the segments of an input sequence. However, this architecture has a limitation, requiring huge computations and memory in proportion to the length of input sequences. Thereby, current LLM are struggled to infinite input sequence tasks like summarizing books. To overcome this problem, google researchers suggests combining transformer architecture with a compressive memory, which stores previous informations in a constant size. They dubbed this method as Infini-attention.\nDetailed Method # Background: Scaled Dot-product Attention # The multi-head scaled dot-product attention (a.k.a. self-attention or MHA) is the main component in transformer architectures’ block.\nTo calculate the attention state \\( A_{dot} ∈ \\mathbb R^{N×d_{value}} \\) of a single head in the MHA module with an input seqeunce $X ∈ \\mathbb R^{N×d_{model}}$, three components, key, query and value are computed as\n\\(K = XW_K, V = XW_V \\ \\text{and} \\ Q = XW_Q, \\) where $W_K ∈ \\mathbb R^{d_{model} ×d_{key }}$, $W_V ∈ \\mathbb R^{d_{model} ×d_{value}}$ and $W_Q ∈ \\mathbb R^{d_{model} ×d_{key}}$ are trainable projection matrices. Then, we can get the attention state as\n$$ A_{dot} = \\text{softmax} \\Bigl( \\dfrac {QK^\\top} {\\sqrt {d_{model}}} \\Bigr) V. $$\nWe could calculate the MHA By parallely computing $H$ number of attention states over an input sequence and then concatenating them.\nInfini-attention # Figure 1: Infini-attention has an additional compressive memory with linear attention for processing infinitely long contexts. $\\{KV\\}_{s−1}$ and $\\{KV\\}_s$ are attention key and values for current and previous input segments, respectively and $Q_s$ the attention queries. PE denotes position embeddings. As shown Figure 1, Infini-attention computes both local and global context states and combine them for its output. Similar to multi-head attention (MHA), it maintains $H$ number of parallel compressive memory per attention layer ($H$ is the number of attention heads) in addition to the dot-product attention.\nCompressive Memory # The researchers suggest three implementation to properly maintain the compressive memory inspired from previous neural network memory [11, 12, 13].\nMemory Retrieval # To fetch information from the memory, Infini-attetnion simply reuse the query ($Q$) in current state and combine with the memory. Specifically, the attention state from the memory $M_{s−1} ∈ \\mathbb R^{d_{key} ×d_{value }}$, $A_{mem} ∈ \\mathbb R^{N×d_{value}}$, is computed as follows with query $Q ∈ \\mathbb R^{N×d_{key}}$:\n$$ A_{mem} = \\dfrac {σ(Q)M_{s−1}}{σ(Q)z_{s−1}}. $$\n$\\sigma$ is a nonlinear activation function, and $z_{s−1} ∈ \\mathbb R^{d_{key}}$ is a normalization term. The researchers use element-wise ELU+1 and sum over all keys for each described before.\nMemory Update # After the retrieval of the memory, we should update the memory and normalization part with the current key and value as follows:\n$$ M_s ← M_{s−1} + σ(K)^\\top V \\ \\text{and} \\ z_s ← z_{s−1} + \\sum^N_{t=1}σ(K_t). $$\nAfter the update, the next input segment $S+1$ uses the updated memory $M_s$ and normalization term $z_s$ recursively. Also, $σ(K)^\\top V$ is refered to associative binding operator [3].\nAlso, the authors combines the delta rule [2] into Infini-attention. The delta rule takes the difference between the value of new segment ($V$) and the stored value in memory as the associative binding terms instead of simply using $V$ (which is similar as the advantage function in reinforcement learning).\n$$ M_s ← M_{s−1} + σ(K)^\\top (V − \\dfrac {σ(K)M_{s−1}} {σ(K)z_{s−1}}). $$\nThe authors call this method as $Linear+Delta$ and the former method as $Linear$.\nLong-term Context Injection # It is important to have a balance in the local attention $A_{dot}$ and the global context $A_{mem}$. The researchers add a scalar $\\beta$ which is the gating component of the weighted sum over the above attention states:\n$$ A = sigmoid(β) ⊙ A_{mem} + (1 − sigmoid(β)) ⊙ A_{dot}. $$\nFinally, to get the MHA output of an attention layer $O ∈ \\mathbb R^{N×d_{model }}$, we concatenate the $H$ parallel attention state and then project them to the output dimension:\n$$ O = [A^1; . . . A^H ]W_O $$\nwhere $W_O ∈ \\mathbb R^{H×d_{value} ×d_{model}}$ is the projection weights.\nComparsion with Other Transformers with Context Memory # Table 1: Transformer models with segment-level memory are compared. For each model, the memory size and effective context length are defined in terms of their model parameters ($N$: input segment length, $S$: the number of segments, $l$: the number of layers, $H$: the number of attention heads, $c$: Compressive Transformer memory size, $r$: compression ratio, $p$: the number of soft-prompt summary vectors and $m$: summary vector accumulation steps). Table 1 shows the analysis of transformer models combining with segment-level memory.\nTransformer-XL [4] uses KV components from the privious segment with current components over each layer. Thus the context window of Transformer-XL is enlarged from $N$ to $N \\times L$, and it requires $(d_{key} + d_{value}) × H × N × l$ memory foot prints. Figure from Transformer-XL [4]. Illustration of the vanilla model with a segment length 4. Compressive Transformer [5] append additional cache to Transformer-XL that saves the past activations. It broaden the Transformer-XL’s context window by $c × r × l$. It keeps a fine-grained memory of past activations, which are then compressed into coarser compressed memories. The below model has three layers, a sequence length $n_s = 3$, memory size $n_m = 6$, compressed memory size $n_{cm} = 6$. The highlighted memories are compacted, with a compression function $f_c$ per layer, to a single compressed memory — instead of being discarded at the next sequence. In this example, the rate of compression $c = 3$. Figure from Compressive Transformer [5]. Memorizing Transformers [6] trys to gather the every KV components as the global context for the input segment. To reduce the overhead of storing every KV compoents, Memorizing Transformers adapts the context-weaving only on the last layer. The context window could explore entire input sequence $N \\times S$ using KNN retriever. Figure from Memorizing Transformers [6]. Memorizing Transformers extend Transformers with access to (key, value) pairs of previously seen subsequences. RMT [7] and AutoCompressors [8, 9] utilized extra vectors that interact with current segment and then is delivered to next token recursively (which is similar in hidden vector in Recurrent Neural Networks (RNN)). However, the google researchers argue that the size of the additional memory vectors is the main factor of the efficiency of the method, which means that the performance and the memory footprint is aligned each other. Figure from Recurrent Memory Transformer [7]. Memory is added as tokens to the input sequence and memory output is passed to the next segment. During training gradients flow from the current segment through memory to the previous segment. Figure from AutoCompressors [8]. AutoCompressors process long documents by recursively generating summary vectors which are passed as soft prompts to all subsequent segments. Figure 2. Infini-Transformer (top) has an entire context history whereas Transformer-XL (bottom) discards old contexts since it caches the KV states for the last segment only. Compare to the above context-based transformer models, Infini-Transformer could catch the entire context $N\\times S$ with the fixed memory size $d_{key} × d_{value} + d_{key}$ that only stores $M_s$ and $z_s$ over the every attention heads and layers.\nExperiments # Infini attention was tested on three main benchmarks such as long-context language modeling, passkey retrieval and book summarization.\nLong-context Language Modeling # Table 2: Long-context language modeling results are compared in terms of average token-level perplexity. Comp. dentoes compression ratio. Infini-Transformer outperforms memorizing transformers with memory length of 65K and achieves 114x compression ratio. The authors trained and evaluated small Infini-Transformer models on PG19 [5] and Arxiv-math [6] benchmarks. They noted that the model with Infini Attention outperformed the baseline model. Additionally, extending the training sequence length further improved the perplexity score, a metric indicating language model performance, where lower scores signify better performance.\nFigure 3. Gating score visualization. Figure 3 illlustrates the gating value ($sigmoid(\\beta)$) of each heads and layers of the pretrained Infini-Transformer. The speciallized head means that the gating scores are close to 0 or 1 which only pass the local attention outputs or context attention output from the memory. The mixer head, of which the gating scores is near 0.5, combines the both information.\n1M passkey retrieval benchmark # Table 3: Infini-Transformers solved the passkey task with up to 1M context length when fine-tuned on 5K legnth inputs. We report token-level retrieval accuracy for passkeys hidden in a different pat (start/middle/end) of long inputs with lengths 32K to 1M The pass-key task is a task that hides a random number in a long context and asks it back in the model output. Below is the input format of the passkey task.\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. (repeat x times) The pass key is 9054. Remember it. 9054 is the pass key. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. (repeat y times) What is the pass key? The pass key is\nWhile previous work [14] showed that the 8B LLaMA model can solve tasks up to 32K in length when fine-tuned with the same 32K-long inputs using Position Interpolation, Infini-Transformer takes this problem further, fine-tuning with only 5K-long inputs and testing it on a 1M-long region. They reported both zero-shot accuracy and finetuning accuracy. Table 3 shows that Inifni-Transformer solved the passkey test perfectly from 32K to 1M after FT.\n500K length book summarization (BookSum) # Table 4: 500K length book summarization (BookSum) results. The BART, PRIMERA and Unlimiformer results are from Bertsch et al. (2024). Figure 4: Infini-Transformers obtain better Rouge overall scores with more book text probived as input. The researchers scaled up their approach by continuously pre-training an 8B LLM model with an 8K input length over 30K steps. They then fine-tuned this model for the book summarization task, setting the input length to 32K for fine-tuning and 500K for evaluation. They used a generation temperature of 0.5, a top-p of 0.95, and a decoding step of 1024 to generate book summaries. Their model outperforms previous best results and sets a new state-of-the-art on BookSum by processing the full text of books. Figure 4 shows the overall Rouge score for the validation split of the BookSum data, indicating a clear trend: the more text provided from the book, the better the summary performance for Infini-Transformers.\nConclusion # This work presents a novel attention, Infini-Attention, which is a close integration of compressive memory module into the vanilla dot-product attention layer. It builds both masked local attention and long-term linear attention into a single transformer block. It helps handle infinitely long processes with limited memory and computation resources. As long-context LLMs are increasingly important today, having such an effective memory system shows the potential for powerful reasoning, planning, continuous adaptation and capabilities not previously seen in LLMs.\nDiscussion # Since Infini-Attention compresses and stores information, it is questionable whether it can produce inconsistent or confusing output if it conflicts with the knowledge of the base model. It use the name Infini-Attention due to its incremental updates, but the authors only test it on 1M tokens. As mentioned earlier, it is doubtful that it can perform on truly infinite data with minimal information loss. We can use memory-based not only for language tasks but for the other tasks. For example, In transformer models for videos [15], they compute over spatio-temporaly combined 3D input (multiple frames) with transformer model, but this requires huge computation overhead. Instead, we could only use a transformer models with 2D input that only takes one frame with the compressive memory that stores global context extracted from the previous frames. Reference # [1] “Attention Is All You Need.”, Vaswani et al.\n[2] “Metalearned neural memory.”, Munkhdalai et al.\n[3] “Tensor product variable binding and the representation of symbolic structures in connectionist systems.”, Smolensky.\n[4] “Transformer-xl: Attentive language models beyond a fixed-length context.”, Dai et al.\n[5] “Compressive transformers for long-range sequence modelling.”, Rae et al.\n[6] “Memorizing transformers.”, Wu et al.\n[7] “Recurrent Memory Transformer.” Bulatov et al.\n[8] “Adapting Language Models to Compress Contexts.”, Chvalier et al.\n[9] “In-context Autoencoder for Context Compression in a Large Language Model.”, Ge et al.\n[10] “Leave No Context Behind: Efficient Infinite Context Transformer with Infini-attention.”, Munkhdalai et al.\n[11] “Metalearned neural memory.”, Munkhdalai et al.\n[12] “Learning associative inference using fast weight memory.”, Schlag.\n[13] “Transformers are rnns: Fast autoregressive transformers with linear attention.”, Katharopoulos et al.\n[14] “Extending context window of large language models via positional interpolation.” Chen et al.\n[15] “ViViT: A Video Vision Transformer.”, Arnab et al.\n"},{"id":12,"href":"/docs/spring24/12_/","title":"12","section":"Spring24","content":" Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies # Posted by: Harit Keawmuang, Junkyeong Park\nAuthors: Zichao Li (University of California, Santa Cruz), Cihang Xie (University of California, Santa Cruz), Ekin Dogus Cubuk (Google Deepmind)\nIn recent years, there has been a growing interest in image-and-language representation learning, which aims to capture the complex interactions between visual and textual information. The Contrastive Language-Image Pre-Training (CLIP) framework has emerged as a leading approach in this field, utilizing large-scale text and image data to create a unified representation space. CLIP has achieved remarkable performance across various tasks and has demonstrated robust generalization to out-of-distribution data. While prior studies on scaling CLIP have focused on scenarios with substantial computational resources, this paper investigates the performance of CLIP under resource constraints, specifically examining the effects of data size, architecture, and training strategies.\nThe study explores the impact of different training data sizes, showing that smaller, high-quality datasets can outperform larger, lower-quality ones. This is critical for practical applications where data quality and computational limits are significant considerations. The research also compares various architectures, highlighting that larger vision transformers (ViTs) do not always guarantee better performance and that CNNs may be more effective when data is limited. Additionally, the paper evaluates different training strategies, including SLIP, FLIP, CLIP, and CLIP+Data Augmentation, revealing that data augmentation can enhance performance without significant computational costs. These findings provide valuable insights for efficiently training and deploying CLIP models, making advanced image-and-language learning more accessible and affordable.\nWhat is CLIP? # CLIP effectively merges the capabilities of natural language processing (NLP) and computer vision. By learning from images and their textual descriptions, CLIP unifies text and image understanding, allowing it to perform various tasks without task-specific training.\nRelated Works # In the recent years, NLP has advanced significantly in pre-training language models on large text datasets. Simultaneously, computer vision has improved by pre-training convolutional neural networks (CNNs) on extensive image datasets. The CLIP model combines these approaches by jointly pre-training on images and text using a contrastive loss function, creating a shared embedding space for both.\nRecent efforts have focused on improving scalability and efficiency of CLIP model. For example, FLIP was introduced to minimize the computation by masking image patches, enabling larger batch sizes without sacrificing performance. Most research has focused on large-scale training with significant computational resources, utilizing ViT large models on extensive datasets. However, less attention has been given to optimizing CLIP for smaller training budgets.\nMethod # Training Pipeline, Dataset, and Hyperparameters # In this paper, they adopted the identical training approach as CLIP, which employs a contrastive loss to simultaneously train the vision and text encoders from scratch. This loss function encourages the encoders to map related image-text pairs to similar feature representations in a shared embedding space by minimizing the distance between positive pairs and maximizing the distance between negative pairs. Key aspects of the training include:\nMinimal data augmentation: Images resized to 224x224 and pixel values normalized to the range of -1 to 1. Optimizer: AdafactorShazeer \u0026amp; Stern with β1 = 0.9 and β2 = 0.999. Batch size: 16k Learning rate: Initial rate of 0.001 with a cosine learning scheduler and weight decay of 0.0001. Evaluation Matrices # Zero-shot transfer evaluation: Assesses the model\u0026rsquo;s ability to generalize to new tasks without fine-tuning. Linear probe evaluations: Freezes the vision encoder and optimizes the fully connected layer\u0026rsquo;s learning rate. Retrieval performance on MSCOCO captions: Ranks text captions based on cosine similarity with image embeddings, reporting Recall@1 for image-to-text retrieval and average results for text-to-image retrieval. Data # Data Quantity # To evaluate the effect of data quantity on CLIP\u0026rsquo;s performance, they conducted experiments with datasets of different sizes: 10M, 25M, 100M, 200M, and 400M. Using ViT-B/32 as the vision encoder, models were trained for 2 to 32 epochs.\nFigure 1. Data Quantity: Zero-Shot performances with the same dataset size across varied training epochs Results showed that for smaller datasets (e.g., 25M), increasing epochs did not significantly improve ImageNet performance. In contrast, larger datasets (e.g., 400M) benefited from more epochs. Additionally, zero-shot performance on ImageNet variants followed a similar pattern: larger datasets and longer training improved performance. However, the correlation between performance on ImageNet and its variants was inconsistent, with some datasets showing improved results in specific variants but not others.\nFigure 2. Data Quantity: Few-Shot Performances on ImageNet They also observed that the few-shot performance also showed a similar trend to the zero-shot performance.\nFigure 3. Data Quantity: Retrieval Performances on MSCOCO In Retrieval Performances, a slightly different trend emerged. Specifically, they found that there was little to no improvement in both image retrieval and text retrieval performance when the number of epochs exceeded eight.\nData Quality # They also examined the impact of data quality by creating subsets of the 3.4B dataset based on image-text similarity, selecting the top 20%, 40%, 60%, and 80% highest-quality data.\nFigure 4. Data Quality: Zero-Shot Performances on ImageNet. (a) trained for one epoch. (b) trained for the same number of sampled data. Models trained on these subsets for a single epoch demonstrated that higher quality data subsets yielded superior zero-shot performance on ImageNet. Specifically, the Top40% subset outperformed the entire dataset despite fewer iterations. When comparing datasets with an equal number of samples, the Top40% dataset achieved the best performance, highlighting the importance of data quality in training CLIP models.\nFigure 5. Data Quality: Few-Shot Performances on ImageNet. (a) and (b) one epoch. (c) and (d) the same number of sampled data. Additionally, when the number of sample data points is the same, higher quality datasets have superior 5-shot and 10-shot performance.\nFigure 6. Data Quality: Retrieval Performances on MSCOCO. (a) and (b) one epoch. (c) and (d) the same number of sampled data. When it comes to search performance, the top 80% datasets in particular show the most impressive retrieval performance.\nVariants of Vision Transformers # This study examines how the performance of various CLIP models, differentiated by the size of their vision encoders, is influenced by dataset size and the number of sampled data points. They used different vision encoders (ViT-Ti/16, S/16, B/32, B/16, L/16) while keeping text transformers fixed at vit-base. They sampled ten subsets from the full dataset, ranging from 10M to 3.4B samples, maintaining consistent data distribution and quality. Models were trained for one epoch to assess the effect of data quantity, ensuring fair comparison by training all subsets for the same number of iterations.\nFigure 7. Various ViTs: Zero-Shot performances with various numbers of sample data Zero-shot performance on ImageNet revealed that larger vision encoders (e.g., ViT-L/16) did not consistently outperform smaller ones when the sample size was under 100M. As data size increased, larger encoders showed better performance.\nFigure 8. Various ViTs: Zero-Shot performances with the same number of sampled data: 3.4B As the dataset size grows, the performance difference between larger ViTs and their smaller counterparts becomes more pronounced. Additiallay, accuracy trends across various datasets (ImageNet-R, ImageNet-Sketch, ImageNet-V2, ObjectNet) were nearly linear, except for ImageNet-A, which had a non-linear improvement, highlighting its challenging nature. (appendix)\nFigure 9. Various ViTs: Linear probing performances with various sizes of vision encoders with the same number of sampled data: 3.4B Linear probing results indicated that for smaller datasets, ViT-L/16 underperformed compared to smaller models, but excelled with more data. Larger ViTs demonstrated better robustness on out-of-distribution datasets.\nFigure 10. Various ViTs: Retrieval Performances on MSCOCO Retrieval tasks showed ViT-L/16 performed poorly with less than 100M samples but improved with more data, aligning with zero-shot trends and benefiting more from larger datasets compared to smaller models.\nComparison of Network Architectures # To effectively choose the best network architectures, they performed a comparison among the various architectures. Previous studies have explored various vision encoders for CLIP, such as ResNet, MLP-Mixer, and ViT, but some architectures like Swin-Transformer and ConvNext haven\u0026rsquo;t been investigated. Here, they compared CNN and vision transformer architectures with similar computational costs, including ViT-B/32, ResNet-50, ConvNext-T, Swin-T, and Mixer-B/32. In Zero-shot, when considering limited data samples, ResNet-50 performs better initially, but ViT-B/32 achieves superior performance with more samples due to its stronger ability to capture global information (see Figure 11(a)). In linear probing, MLP-Mixer outperforms others with fewer samples, but ViT-B/32 excels with larger datasets. ViT and MLP-Mixer show better robustness, likely due to their lower inductive bias, leading to improved generalization (Figure 11(b)). For retrieval tasks, ResNet-50 is better with smaller sample sizes, but ViT-B/32 surpasses it as sample sizes increase. Mixer-B/32 performs poorly in retrieval tasks, making ViT the preferred choice for CLIP\u0026rsquo;s vision encoder across various tasks.\nFigure 11. Performances of the various network architectures Training Strategies # In this section, the various training strategies for CLIP are explored, including SLIP, FLIP, and a proposed method from this paper called CLIP+Data Augmentation. SLIP enhances the vision encoder through self-supervised learning but is computationally expensive compared to the original CLIP. FLIP masks patches in training images to reduce computation. However, CLIP+Data Augmentation aimed to enhance CLIP\u0026rsquo;s vision encoder while mitigating the computational demands associated with previous self-supervised learning approaches. By applying data augmentation directly to input images, they offered a cost-effective alternative, validated across four subsets with 30 epochs of training using techniques like crop\u0026amp;flip, RandAugment, and Stacked RandAugment. The results in Figure 12 demonstrated consistent performance improvements of all three methods over raw CLIP, with no additional computational burden incurred, even enabling comparable performance to larger datasets, exemplified by the Stacked RA model trained on a dataset half the size achieving similar results.\nFigure 12. Comparison between various data augmentation for CLIP Their experiments on the ImageNet dataset show that SLIP outperforms CLIP and FLIP when training samples are under one billion, indicating the benefit of self-supervised learning for limited data. However, as sample size increases, CLIP and FLIP surpass SLIP, suggesting that enhancing vision encoders isn\u0026rsquo;t necessary for large datasets. Additionally, SLIP is twice as computationally expensive as CLIP and performs worst in zero-shot tasks when costs are equal. Data augmentation, particularly CLIP + Data Aug, improves performance and generalization on ImageNet and its variants without extra computational costs, especially for larger datasets and multiple epochs of training as presented in Figure 13.\nFigure 13. Zero-shot performance with the various training strategies In the linear probing evaluation, vision encoders trained with CLIP + Data Aug consistently outperformed the other strategies, particularly on OOD datasets. CLIP and CLIP + Data Aug also showed better robustness than SLIP with similar ImageNet accuracy. Combining CLIP with data augmentation offers a more effective feature extractor, balancing performance, and computation cost. The training results on linear probing performance are shown in Figure 14.\nFigure 14. Linear probing performance with the various training strategies In retrieval tasks, SLIP consistently outperformed CLIP, CLIP + Data Aug, and FLIP on both image and text retrieval across all dataset sizes. Unlike its zero-shot performance, SLIP showed the best results for retrieval tasks as presented in Figure 15, suggesting it is a superior strategy for these tasks despite being less effective for classification.\nFigure 15. Retrieval performances with the various training strategies Conclusion # This study examines how data size, network architecture, and training methods affect CLIP\u0026rsquo;s performance. Their experiments highlight the critical roles of data quantity and quality. They also demonstrate that data augmentation can improve CLIP\u0026rsquo;s performance with minimal additional computational cost. Furthermore, they investigate various network architectures and training strategies, finding that some outperform others depending on the computational budget, emphasizing the need for careful selection. From our perspective, the balance between computational efficiency and model accuracy is crucial, and exploring adaptive methods could yield significant benefits. Future research could focus on integrating transfer learning with CLIP to enhance domain-specific performance and investigating AutoML techniques for optimal architecture and strategy selection.\n"}]