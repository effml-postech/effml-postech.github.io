[{"id":0,"href":"/docs/spring24/00_taco_example/","title":"00 Taco Example","section":"Spring24","content":" Example : Content # This paper propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss. By doing so, they avoid decoding based on text-guided generative models\u0026mdash;known for high generative diversity\u0026mdash;and effectively utilize the semantic information of text at a global level.\nExample : Using KaTeX for math equation # KaTeX shortcode let you render math typesetting in markdown document. See KaTeX\nHere is some inline example: \\(\\pi(x)\\) , rendered in the same line. And below is display example, having display: block \\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi\\] Text continues here!!!\n"},{"id":1,"href":"/docs/spring24/01_/","title":"01","section":"Spring24","content":" Is Bigger Edit Batch Size Always Better? - An Empirical Study on Model Editing with Llama-3 # Authors: Junsang Yoon, Akshat Gupta, Gopala Anumanchipalli\nPosted by Jin Hyun, Gyuhyun Jung\nBackground # What is model editing? # Fig 1. Concept of model editing. The rapidly evolving field of artificial intelligence faces the challenge of keeping large language models (LLMs) up-to-date with new information, as traditional retraining methods are time-consuming and resource-intensive. As shown in figure, an alternative is model editing proposed in (Sinitsin et al., 2020). It enables data-efficient alterations to the behavior of models.\nFig 2. Example of model editing in case of MEMIT. Model editing modifies stored facts within a model and corrects inaccuracies without retraining. Techniques such as ROME (Rank-One Model Editing) (Meng et al., 2022a), MEMIT (Mass Editing Memory in Transformer) (Meng et al., 2022b), and EMMET (Equality-constrained Mass Model Editing algorithm for Transformers) (Gupta et al., 2024), known as \u0026ldquo;locate-and-edit\u0026rdquo; algorithms, have emerged to optimize the preservation-memorization (PM) objective. These methods directly modify specific areas of the model and are applicable to any transformer-based LLMs, offering a more efficient way to update models without retraining.\nHow model editing works? # For a relation \\((s,r,o)\\) expressed as a tuple in the form of (subject, relation, object). In model editing, we aim to update the memory of the existing model with new facts by learning about a new object \\((s,r,o^*)\\) . Model editing directly reform the weight by objective function, called the preservation-memorization objective. This objective consists of two parts, a preservation term and a memorization term. Below equation shows how ROME works with preservation term and memorization term.\n\\( \\argmin_{\\hat{W}} \\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\quad \\text{s.t.} \\quad \\hat{W} k_e = v_e \\\\Preservation\\_term=\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\\\ Memorization\\_term=\\hat{W} k_e = v_e \\) Where W represents the weights of the feedforward layer we want to edit, k is a key-vector representative of a fact, \\(v_e\\) is the desired output, and \\(K_0 =[k_1^0 |k_2^0 |\\cdots| k_0^N]\\) is a matrix consisting of facts we want to preserve. Above equation is optimized by follwing gradient.\n\\(\\hat{W} = W_0 \u0026#43; \\Delta \\quad \\text{where} \\\\ \\Delta = (v_e - W_0 k_e) \\frac{k_e^T C_0^{-1}}{k_e^T C_0^{-1} k_e} \\) For MEMIT model editing. it optimizes same objectives with ROME, but performance memorization using a least-square constraint, which allows for a closed-form solution. It has similar form with ROME method, but it multiplies \\(\\lambda\\) term, which is hyperparameter, to preservation term. Also, it combines memorization term for minimize target\n\\(\\argmin_{\\hat{W}} \\lambda\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \u0026#43; \\left\\| \\hat{W} K_E - V_E \\right\\|\\\\Preservation\\_term=\\lambda\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\\\ Memorization\\_term=\\hat{W} K_E - V_E \\) \\(V_E\\) is stacked matrix of \\(v_e\\) vectors, and fact is represented by a pair of vectors denoted as key ( \\(k_e\\) ) and value ( \\(v_e\\) ). This objective has similar solution of ROME, followed by below equations.\n\\(\\hat{W} = W_0 \u0026#43; \\Delta \\quad \\text{where} \\\\ \\Delta = (V_E - W_0 K_R)K^T_E (\\lambda C_0 \u0026#43; K_E^T K_E^T)^{-1} \\) In EMMET, it shows model editing is possible with batched facts. It is possible by allowing memorization happens using an equality-constraint. EMMET objective and gradient solution is followed by below equations.\n\\(\\argmin_{\\hat{W}} \\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\|\\quad \\text{s.t.} \\hat{W} k_i^e = v_i^e \\quad \\forall i \\in [1, 2, \\cdots, E] \\\\Preservation\\_term=\\left\\| \\hat{W} K_0 - W_0 K_0 \\right\\| \\\\ Memorization\\_term=\\hat{W} k_i^e = v_i^e \\quad \\forall i \\in [1, 2, \\cdots, E] \\\\ \\hat{W} = W_0 \u0026#43; \\Delta \\quad \\text{where} \\\\ \\Delta = (V_E - W_0 K_R)(K_E^T C_0^{-1}K_E)^{-1}K_E^TC_0^{-1} \\) How model editing performance is estimated? # Model performance is estimated with 4 main scores, and these scores are bsed on how model editing works with expressions of correct facts in \\((s,r,o^{c})\\) and false facts in \\((s,r,o^{*})\\) .\nEfficacy Score (ES) # ES measures if the new fact, which we want to edit, is successfully edited to model. It is measured by percentage where \\(\\mathbb{P}[o^*] \u0026gt; \\mathbb{P}[o^{c}]\\) , which means the portion of correct edition result from predictions.\nParaphrase Score (PS) # PS measures model\u0026rsquo;s ability to generalize following an edit. It is measured by where P(new fact) \u0026gt; P(old fact) under paraphrases of the query prompt.\nNeighborhood Score (NS) # NS represents the specificity of model editing. To measure NS, we collect a set of nearby subjects \\(s_n\\) for which \\((s_n,r,o^{c})\\) holds true. Then we test \\(\\mathbb{P}[o^*] \u0026gt; \\mathbb{P}[o^{c}]\\) , reporting the success fraction asn NS.\nComposite Score (S) # S represents the overall performance. It combines aspect of edit success, generalization, and specificity. It is calculated as the harmonic mean of Edit Success (ES), Paraphrase Score (PS), and Neighborhood Score (NS). It provies overall efficacy of model edits.\nExperiments \u0026amp; Results # What is the Optimal Layer for Model Editing? # Investigating the effectiveness of hidden states in LLMS for recalling facts using causal tracing showed that subject’s last token within the feed-forward networks at intermediate layer plays a significant role. (Meng et al., 2022b)\nMotivation : Later work showed that layers deemed important during causal tracing did not always translate to model editing performance. Therefore, this work focused on finding the optimal layer for model editing layer empirically.\nSteps for finding optimal layer\nMake 1000 non-sequential edits from the CounterFact (Meng et al., 2022a) dataset at each layer of the Llama-3 model. Calculate various model metrics(ES, PS, NS, S) to evaluate their impact. The layer that achieves the highest score is selected as the most suitable for targeted interventions. Fig 3. Post-edit performance of various metrics for Llama3-8b model using MEMIT and ROME on various layers. Eqch layer is edited with 1000 facts, one at a time and non-sequentially. Fig 4. Post-edit performance of various metrics on Llama2-7b for MEMIT on various layers. Evaluation results showed that layer 1 for Llama-3 outperformed on numerous metrics. Furthermore this trend was also shown in previous version, Llama-2, as seen in Figure 6. Here, MEMIT and ROME have very similar performance for model editing across layer of a model.\n→ Why? : Both algorithms optimize for the same objective with difference in the memorization constraints. This shows that memorization constraints plays minor effect on editing performance.\nOptimal way of Scaling Up model editing? # After finding the optimal layer, scaling of model editing on the same model can happen in two ways : batch editing \u0026amp; sequential-batched editing.\n1. Batch Editing :\nA large number(batch size) of knowledge edits are performed on the model with the same update. This work stick to editing a single layer of the model.\nExperimental settings\nTargeting layer1 in Llama-3 with batch size 16, 64, 256, 1024, and 4096 for Batched editing. Evaluation Results of Batch Editing\nFig 5. Various metric results (PS, NS, ES, S) after a batch edit (16, 64, 256, 1024, 4096) on MEMIT and EMMET respectively. For both MEMIT \u0026amp; EMMET editing, metrics are seen to consistently fall with larger batches, with NS being the most pronounced to fall. ES is most resilient metric to edits. PS, only metric to do so, seen to increase dramatically between batch sizes of 16 and 64. The similar trend between two editing techniques reflect the similarity in their optimization objectives.\n2. Sequential-batched Editing :\nSequential Editing is an alternate way to scale up model editing where facts are added sequentially to a model.\nThis work proposes optimal way to scale model editing that strikes a balance between Batch Editing \u0026amp; Sequential Editing.\nSequential-batched editing sequentially edit many batch of facts at a time. And the experiment was conducted going from batch size of 1 up to 4096. (1, 64, 256, 1024, 4096)\nFig 6. Single layer sequential editing performance for various batch sizes on MEMIT and EMMET respectively. Experimental results according to figures above showed that larger batch sizes are actually worse for model performance than sequential edits with smaller batches. In contrast, larger batch sizes seem to be better for metrics in NS : while batch edits are less successful in general, it is better in preserving locality of edits. This results were concluded to optimal batch size of 1024 for both MEMIT and EMMET. Increasing batch-size beyond that lead to larger model degradation and better editing results can be achieved by sequential-batched editing with smaller batch sizes.\nConclusion # This work examines several model editing techniques in the context of the newly released Llama-3 model and there are some conclusion as follows:\nEarlier layers may be more optimal intervention points. Model editing techniques that share same optimization objectives shows similar trends in layer and editing. Smaller, frequent sequential batch size edits have a superior performance. Batch size of 1024 for MEMIT and EMMET is optimal batchsize with sequential-batched editing. The authors argue that the current trend of pushing towards bigger edit batch sizes for scaling model editing may have limitations. Instead, they propose that future research should focus on methods that combine both batched and sequential editing to optimize performance while minimizing model degradation. Also, future work was proposed for experiments on multi-layer intervention for edits, as well as experiments against other popular models and algorithms, including methods that are hyper-network based.\nDiscussions, and research direction proposal from post writers # The paper empirically analyzes the performance of model editing based on batch size. It would be more beneficial for model editing research if the theoretical reasons behind the overall metrics decreasing as batch size increases are elucidated, rather than just empirically.\nWhile the work presents a hybrid format combining sequential editing and batch editing, it lacks in-depth analysis of the strengths and weaknesses of both approaches. Additionally, it is important to ensure that the individual characteristics of techniques such as ROME, MEMIT, and EMMET are appropriately integrated into editing optimization.\nAnalyzing the reasons behind the improvement in performance when layers are edited later in the network (NS) and the improvement when batch size is increased (PS) could help in identifying the optimal point for multi-layer editing\nIt seems necessary to investigate how many layers should be edited in multi-layer editing to achieve effective results beyond single-layer editing.\nReferences # Implementation code: Link.\nYunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang. 2023. Editing large language models: Problems, methods, and opportunities. arXiv preprint arXiv:2305.13172.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, Artem Babenko. 2020. Editable neural networks. arXiv preprint arXiv:2004.00345.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022a. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359–17372.\nKevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. 2022b. Massediting memory in a transformer. arXiv preprint arXiv:2210.07229.\nAkshat Gupta, Dev Sajnani, and Gopala Anumanchipalli. 2024. A unified framework for model editin. arXiv preprint arXiv:2403.14236.\n"},{"id":2,"href":"/docs/spring24/02_/","title":"02","section":"Spring24","content":" Spectrally Pruned Gaussian Fields with Neural Compensation (SUNDAE) # *Authors: Yang, Runyi, et al\n*Team: Donggeon Lee, Chiho Yoon\nSummary # 3D representation, which is the basis for many VR/AR and robotics applications, has long been an area of interest in computer vision and graphics. With the advent of neural radiation fields (NeRFs) Link, several methods have emerged to improve the quality and efficiency of NeRFs.\nConventional 3D Gaussian Splatting (3DGS) # Fig. 1. Comparison of 3D Gaussian Splatting to previous NeRF technologies. One of the recent hot topics in the NeRF field, 3D Gaussian Splatting (3DGS), demonstrates high quality and particularly fast, near real-time rendering speeds (about 100FPS).\nPros: Superior rendering speed and quality Cons: High memory consumption Proposed SUNDAE # Fig. 2. Comparison of 3D gaussian splatting and proposed SUNDAE It constructs a memory-efficient Gaussian field using spectral pruning and neural compensation. It considers the relationship between primitives, reducing memory usage while maintaining rendering quality. It significantly reduces memory consumption while preserving high rendering quality. Code: https://runyiyang.github.io/projects/SUNDAE/. Introduction # Fig. 3. Conceptual illustration of vanilla 3DGS, SUNDAE spectral pruning technique, and neural compensation. 3D Gaussian Splatting (3DGS) Link # Recently, 3DGS has been proposed as a novel 3D scene representation, utilizing a set of 3D positions, opacity, anisotropic covariance, and spherical harmonic (SH) coefficients to represent a 3D scene (left panel of Fig. 2). 3DGS demonstrates notable advantages in rendering speed, rendering quality, and training time. But it requires a large storage.\nSpectral graph pruning # Gaussian fields utilize a collection of Gaussian primitives as the representation of the scene. As these primitives are irregularly distributed in 3D space, they propose a graph-based data structure, rather than regular structures like grids, to capture the relationship between these primitives (middle panel of Fig. 2).\nNeural compensation # To address an inevitable decrease in rendering quality, they employ a neural compensation head to compensate for this quality loss (right panel of Fig. 2).\nContributions # A newly proposed primitive pruning framework for Gaussian fields based upon the spectrum of primitive graphs. A novel feature splatting and mixing module to compensate for the performance drop caused by the pruning. State-of-the-art results, in terms of both quality and speed, on various benchmarks with low memory footprint. Methods # We have 4 steps for the method\n3D Gaussian Splatting Warm Up Spectral Graph Pruning Neural Compensation Continuous Pruning as a Strategy The overall framework is in Fig.\nFig. 4. The overall framework of SUNDAE with pipeline and graph-based pruning. Let’s see how each step works.\n1. 3D Gaussian Splatting Warm up # SUNDAE initialize Gaussian centers of the vanilla 3D Gaussian Splating as the first step for generating a dense representation using Gaussian primitives. Then, an effective densification strategy is used to increase the primitives.\n- Gaussian Primitive Initialization # The point cloud $P_c$ is the first input for representing the 3D scene using Gaussian primitives $P$. Then they turn 3D coordinates $x \\in P_c$ into Gaussian primitives $p \\in P$ by the following equation:\n$p(x)=exp(-1/2(x)^T\\Sigma^-1(x))$ where the $\\Sigma$ is defined as 3D covariance matrix. They also use an ellipsoid configuration. The decomposition of $\\Sigma$ is achieved with scaling matrix $S$ and a rotation matrix $R$, as expressed in the equation:\n$\\Sigma=RSS^TR^T$ - Gaussian Primitive Densification # They optimize all parameters of Gaussian primitives and integrate a densification strategy to improve representation power during the training process.\n2. Spectral Graph Pruning # After warm-up, a dense representation incurs significant storage consumption. To efficiently prune redundant primitives, they used the graph signal processing theory and construct a graph based on Gaussian primitives.\n- Graph Signal Processing Preliminaries # Graph shift: Graph shift represents the connections between nodes in a weighted graph, typically represented by a weighted adjacency matrix. It quantitatively describes the relationships between nodes using the weights of edges.\nGraph signal: Graph signal is a mapping assigning values to each node in a graph, utilized to model interactions between nodes.\nGraph Fourier Transform: Graph Fourier Transform is the process of expanding a graph signal using the eigenbasis of the graph shift, enabling analysis of the structure and interactions within the graph.\n- Graph Construction # Given a set of Gaussian Primitives %P% , they construct a nearest neighbor graph with the adjacent matrix $W$ of the graph:\n$$ W_{ij} = \\begin{cases} \\exp\\left( -\\frac{\\|x_i - x_j\\|^2}{2 \\sigma^2} \\right), \u0026 \\text{if } \\|x_i - x_j\\|^2 \u003c \\tau \\\\ 0, \u0026 \\text{otherwise} \\end{cases} $$ where $x_i$ and $x_j$ are central points in $P$, $\\tau$ is a hyperparameter, and $\\sigma$ is the variance of the distance matrix.\n- Graph Filtering and Sampling # SUNDAE propose a band-limited graph filter that combined with a high-frequency filter and low-frequency filter. By doing this, they can catch both the detailed information and general information. Design of filters are Haar-like.\nThey also prune the abundant primitives according to the response magnitude of the high-pass filter.\n3. Neural Compensation # There is a decrease in rendering quality for large pruning ratio. To address this, they employ a neural compresation network to model the relationship between primitives in the 2D domain.\nThey render the 3D Gaussian primitives into neural images in a differentiable manner, using the differentiable 3D Gaussian renderer from 3DGS with feature rendering instead of RGB rendering. The center of each Gaussian primitive is projected using a standard point rendering method, and the covariance in the neural image space is calculated.\nThe neural image is then computed using the feature vectors of Gaussian primitives. A lightweight neural network (U-Net with skip connections) is used to compensate for the quality drop after spectral pruning.\nThe overall optimization process is based on the difference between the rendered images and the ground truth images from the dataset. The compensation network and the 3D Gaussian primitives are optimized simultaneously during training, using a loss function that combines L1 loss and D-SSIM loss.\n4. Continuous Pruning as a Strategy # In addition to the training-then-pruning strategy, a continuous pruning strategy is explored. Continuous pruning periodically removes a specific number or percentage of primitives during the training process. This aims to lower peak memory usage and allow training on GPUs with lower memory. However, it can result in less predictable final memory usage, as the reduction may vary across different scenes. Therefore, continuous pruning is considered an alternative strategy when needed.\nResults # Quantitative Results # Table 1. Quatitative evaluation of SUNDAE. SUNDAE demonstrates strong performance across various metrics, including PSNR, SSIM, FPS, and memory usage.\nCompared to existing methods on the MipNeRF360 dataset, SUNDAE achieves a balance between rendering quality and efficiency, maintaining high FPS rates while significantly reducing memory consumption. Even at low sampling rates, SUNDAE remains competitive with established approaches, showcasing the effectiveness of its spectral pruning and neural compensation techniques in managing Gaussian primitive relationships and retaining scene information. Overall, SUNDAE represents scenes more compactly while maintaining high quality rendering. Qualitative Results # Fig5. Qualitative results of SUNDAE. The qualitative results demonstrate that SUNDAE achieves comparable novel view synthesis quality with significantly lower memory consumption (1% or 10%).\nThe graph effectively captures primitive relationships, while the neural compensation head preserves rendering quality. Spectral pruning notably removes outliers near the camera, enhancing scene coherence. Ablation Study # Fig6. Ablations experiment on the ratio 𝛾 of the bandlimited filter of graph based pruning. Band-limited ratio of Graph-based pruning: The band-limited filter\u0026rsquo;s ratio, represented by 𝛾, significantly impacts rendering quality, with a 𝛾 value of 50% yielding the most favorable outcomes, emphasizing the advantage of spectral pruning in preserving important high-frequency details and low-frequency background (Fig. 4). Table 2. Ablations of neural compensation module size. Fig. 7. Visualization with and without neural compensation. The compensation performance of the network: Employing the neural compensation module enhances performance across all sampling rates evide(Table 2, Fig. 5), highlighting its compensatory capability in mitigating performance drops caused by spectral pruning and effectively modeling the relationship between primitives. Neural Compensation Module Size: Increasing the size of the neural compensation module does not necessarily enhance rendering quality (Table 2), aligning with findings from ADOP and indicating a balance between quality and memory usage. Conclusion # They propose SUNDAE, a novel approach to spectrally prune Gaussian fields with neural compensation, efficiently capturing the relationship between Gaussian primitives using graph signal processing and blending information to offset pruning-induced information loss. By leveraging spatial information among Gaussian primitives to construct a graph and spectrally pruning less significant ones, they employ a lightweight neural network to compensate for quality degradation post-pruning. Experimental findings demonstrate SUNDAE\u0026rsquo;s ability to maintain the efficiency of 3DGS while significantly reducing its size across various scenarios. "},{"id":3,"href":"/docs/spring24/03_/","title":"03","section":"Spring24","content":" Introduction # The significant advances in deep learning over the past decade have largely relied on the development of algorithms that efficiently leverage available hardware. As the size of state-of-the-art models increases, hardware efficiency becomes crucial for reducing training costs, which have grown substantially in terms of money, time, and environmental impact. However, with the end of Moore\u0026rsquo;s Law and Dennard scaling, increased transistor density alone cannot provide a straightforward path to greater efficiency. The use of low-precision number formats is a promising alternative. These formats offer substantial gains in compute, memory, and bandwidth efficiency, making them valuable in the context of modern deep learning.\nBackground # Floating-Point Formats for Deep Learning # Traditionally, floating-point numbers are defined by the IEEE 754 standard, which specifies the number of exponent bits (E) and mantissa bits (M). Common floating-point formats used in machine learning include FP32, TF32, BFLOAT16, and FP16. Recently, two types of FP8 formats (E4 and E5) have been proposed.\nTable A.1. Common floating point formats for deep learning Advantages and Disadvantages of Low-Precision Training # Disadvantages: FP16 and BFLOAT16 offer different trade-offs. FP16 has higher precision, but BFLOAT16 has a wider range. FP8 formats reduce both range and precision. The use of low-precision formats can introduce quantization noise and other issues. Advantages: Using low-precision formats can significantly improve efficiency in terms of memory usage, bandwidth usage, compute performance, and cross-device communication costs. Figure 2. The signal to noise ratio (SNR) of samples from a normal distribution, quantised in FP16 and FP8, as a function of the distribution’s scale Techniques for Low-Precision Training # Mixed Precision: This technique uses multiple number formats with different bit-widths, placing most activations, weights, and gradients in FP16 without loss of accuracy.\nLoss Scaling: To overcome the limited range of FP16 and FP8, the loss can be multiplied by a scalar to increase the scale of gradients. This method requires empirically finding a suitable loss scale:\n\\[ \\text{scaled\\_loss} = \\text{loss} \\times \\text{scale\\_factor} \\] \\[ \\text{scaled\\_gradients} = \\text{gradients} \\times \\text{scale\\_factor} \\] Automatic Loss Scaling: This dynamically adjusts the loss scale during training, removing the need to sweep for an initial loss scale.\nPer-Tensor Scaling: This system locally rescales based on runtime statistics to address scaling difficulties in FP8 training.\nTable 1. A comparison of techniques for low precision training Analysis # Ideal Scaling # The ability to predict the scale of tensors at the start of training is crucial. We argue that unit variance ( \\(\\sigma = 1\\) ) is an optimal balance among various competing factors. This approach helps concentrate values within the representable range, reducing clipping errors during training.\nIn floating-point formats, values are represented as: \\[ \\text{value} = (-1)^{b_{\\text{sign}}} \\times 2^{\\text{exponent}} \\times \\left(1 \u0026#43; \\frac{b_{\\text{mantissa}}}{2^M}\\right) \\] where \\(b_{\\text{sign}}\\) , \\(b_{\\text{exponent}}\\) , and \\(b_{\\text{mantissa}}\\) represent the sign, exponent, and mantissa bits, respectively.\nFigure 1. Above: Unit scaling of an FFN layer. We multiply each tensor by a fixed scalar to achieve consistent scale, no longer requiring a loss scale to control the scale of gradients. Below: A histogram of exponent values at initialisation for the above FFN Predictable Scaling # If we can predict the scale of tensors in a deep learning model, we can effectively address clipping errors. At initialization, parameters are drawn from known distributions, allowing us to analytically or empirically derive the scale of each tensor.\nFor example, by considering the scaling factors for each operation in the neural network, we can perform scaled operations:\n\\[ y = \\alpha \\cdot f(x) \\] where \\(\\alpha\\) is the scaling factor and \\(f\\) represents the operation.\nUnit Scaling # Unit scaling is proposed to address the limitations of existing methods for managing scale in typical models. A model is considered unit-scaled if its activations, weights, and gradients have approximately unit variance at initialization. This is achieved by inserting scaling factors into the forward and backward passes. Unlike loss scaling, which requires an empirically determined hyperparameter or an adaptive algorithm, unit scaling determines these scales based on a set of rules for each operation, approximately preserving the variance of the inputs. This leads to global unit scaling throughout the model, ensuring tensor values are centered within the exponent range at initialization, providing headroom during training to avoid going out of range.\nA framework for scaling computational graphs # Computational Graphs\nRepresent model by the differentiable function \\(f_{model}(x_1,...,x_m)\\) Describe the structure of such a model using a directed acyclic graph (DAG) denoted \\(\\mathcal{G} =(\\mathcal{V}, \\mathcal{E}) \\) This kind of graph is commonly known as a computational graph, with vertices as nodes and their corresponding functions as ops. Forward and backward graphs\nWe refer to the computational graph corresponding to \\(f_{model}\\) as the forward graph In deep learning we typically apply reverse-mode automatic differentiation to the forward graph to create a second computational graph whose output nodes represent the partial derivatives of the model with respect to its inputs: \\( \\frac{\\partial f_{model}}{\\partial x_i}, \\forall i \\in[1 . . m] \\) . We call this the backward graph Scaled ops\nGiven an op \\(f\\left(x_1, \\ldots, x_k\\right)\\) , we define the scaled op \\( f^*\\left(x_1, \\ldots, x_k, \\alpha, \\beta_1, \\ldots, \\beta_k\\right) \\) with scaling factors \\( \\alpha, \\beta_1, \\ldots, \\beta_k \\in \\mathbb{R}^{\u0026#43;} \\) , such that \\(f^* \u0026amp; \\triangleq \\alpha \\cdot f\\left(x_1, \\ldots, x_k\\right)\\) \\( f_{\\text {grad }}^*\\left(x_1, \\ldots x_k, g\\right)_i \u0026amp; \\triangleq \\beta_i \\cdot f_{\\text {grad }}\\left(x_1, \\ldots x_k, g\\right)_i, \\forall i \\in[1 . . k] \\) Scaled computational graph\nA scaled computational graph is one where every op \\(f\\) in the forward graph is replaced by a scaled equivalent \\(f^{*}\\) , with the backward graph then generated to produce \\(f^{*}_{grad}\\) grad for each \\(f_{grad}\\) , using any choice of scaling factors. Constraint-scaled computational graphs\nA constraint-scaled computational graph is a scaled computational graph where we restrict the scaling factors of ops that consume non-cut-edge variables in the following way: for any edge \\(e \\notin \\mathcal{C}\\) , we require the op consuming the variable \\(x_e\\) to have scaling factors \\(\\alpha = \\beta_e f\\) . Proposition 5.1\nFor any scaled op, there is an equivalent unscaled op with the same training dynamics under a firstorder optimiser.\nTheorem 5.2\nA constraint-scaled computational graph itself represents a scaled op.\nA scaling strategy for unit variance # Unit scaled computational graphs\nInitially set aside any scale constraints, and calculate the scaling factors that give each op expected unit variance outputs (this process is covered below). Now resolve any scale constraints by taking each constrained group \\( {\\alpha, \\beta_1, \\ldots, \\beta_l } \\) and selecting the geometric mean \\( \\left(\\alpha, \\beta_1, \\ldots, \\beta_l \\right)^\\frac{1}{l\u0026#43;1} \\) Selecting scaling factors\nAssuming unit-scaled inputs to \\( y = f(x_i,\\ldots,x_k) \\) , derive the output scale \\( \\sigma_Y \\) and set the forward scaling factor \\( \\alpha = 1/\\sigma_Y \\) . Repeat this process for \\( x_i\u0026#39;=f_{grad}(\\ldots)_i, \\forall i \\in[1 . . k] \\) , to obtain the gradient scale \\( \\sigma_{x_i\u0026#39;} \\) i and set the backward scaling factor \\( \\beta_i = 1/\\sigma_{x_i\u0026#39;} \\) . Weighted addition # When tensors of different scales, such as those in residual layers, losses, and positional encodings, are added, simply adding them can adversely affect performance. To address this, we propose using weighted_add. In this approach, we can maintain unit scale while performing operations using a scaled identity function.\nRecipe # We now outline a high-level recipe for a unit-scaled model:\nInitialise non-bias parameters with unit variance. Calculate scaling factors for all scaled ops. Identify non-cut-edges, and constrain the ops consumingthem to have \\( \\alpha = \\beta \\) by taking the geometric mean. Replace adds with weighted adds. Example # Using the unit scaling recipe, we first build a scaled op, and then a full scaled layer. Consider a scaled projection op with learnable weights:\n\\( \\operatorname{matmul}^*(X,W) =\\alpha \\cdot X W \\) \\( \\operatorname{matmul}_{\\text {grad }}^*(X, W, G)_1 = \\beta_1 \\cdot G W^{\\top} \\) \\( \\operatorname{matmul}_{\\text {grad }}^*(X, W, G)_2 = \\beta_2 \\cdot X^{\\top} G \\) for input \\( X \\in \\mathbb{R}^{b \\times m} \\) , weight \\( W \\in \\mathbb{R}^{m \\times n} \\) , output \\( \\mathbb{R}^{b \\times n} \\) and incoming gradients \\( G \\in \\mathbb{R}^{b \\times n} \\) We show code for the above in Figure 3, which also gives a scaled layer for the Transformer FFN\nFig3. PyTorch examples Results # Character language modelling\nExperimental Setup: Train causal language models on WikiText-103 raw character language modeling, using cross-entropy loss during training and evaluating on bits per character (BPC). Below the product of these settings, we compare the performance of regular (baseline) and unit scaling in both FP32 and FP16.\nSequence layer type: Attention, RNN and Convolution Norm placement: PreNorm, PostNorm and NoNorm Residual scaling: default, fixed and running-mean Results\nFirst, these demonstrate the need for scaling when using FP16. This is due to gradient underflow, since loss scaling with a factor of 2048 resolves the issue. Second, they demonstrate that unit scaling, despite changing the training behaviour of the model beyond just numerics, matches or even slightly improves upon baseline performance in almost all cases. Finally, they show that no tuning is necessary when switching unit scaling to FP16. suggest that running-mean or fixed are reasonable choices when using unit scaling Fig4. Character language modelling, showing validation bits per character over a wide range of models Masked language modelling\nExperimental Setup\nTo evaluate the advantages of unit scaling, we assess BERTBASE and BERTLARGE models, which typically struggle with loss scaling. Results\nTable2. Downstream performance of regular and unit-scaled BERT models Related Work # Variance scaling analysis\nVariance scaling and residual networks, along with normalization variants, complement unit scaling, which considers both activation and gradient norms. The reparameterization implied by unit scaling, utilized in analyzing deep network training dynamics, applies scaling factors locally throughout the compute graph, akin to training hyperparameter scaling. FP8 inference\nFP8 training lacks hardware support, yet accelerated 8-bit inference is becoming more prevalent through integer quantization to INT8. While this process often leads to reduced accuracy, recent efforts aim to enhance efficient INT8 quantization. FP8 adoption allows accelerated inference in the same format as training, promising significant improvements in the simplicity and accuracy of 8-bit inference. Discussion # Compute overhead\nUnit scaling introduces minimal compute overhead by adding scaling operations that can be fused into preceding operations, resulting in negligible memory-access cost. While basic loss scaling operates similarly, automatic loss scaling may incur additional overhead due to occasional batch discards, particularly noticeable in FP8. Proposed automatic per-tensor scaling schemes may introduce overhead, depending on software and hardware characteristics, as they trade off accuracy for complexity. In contrast, unit scaling with fixed precomputed scaling factors offers a simpler alternative without such complexities. Broader impact\nWith the potential for unit scaling to effectively train larger models, concerns arise about issues such as toxicity, misinformation, privacy concerns, and environmental damage. To address these challenges, various methods have been proposed, including AI feedback, anti-experts, and baked-in safety models. Conclusion Unit scaling has demonstrated to address the complexities of low-precision training, providing a simpler and more granular solution, even enabling the training of BERTLARGE without loss scaling for the first time, even in FP8.\n"},{"id":4,"href":"/docs/spring24/04_/","title":"04","section":"Spring24","content":" Better \u0026amp; Faster Large Language Models via Multi-token Prediction # Posted by Jinoh Cho and Seonghyeon Park\nAuthors: Gloeckle et al. Institution : FAIR at Meta, CERMICS Ecole des Ponts ParisTech and LISN Universite Paris-Saclay Preliminaries # Language Modeling and Next-Token Prediction Task # Learning through a next-token prediction task has been a mainstream for language modeling. The goal of a next-token prediction task is to maximize the probability of the next token $x_{t+1}$, given the history of previous tokens $x_{t:1} = x_1, \\ldots, x_t$. This can be formulated as follow:\n$$ L_1 = - \\sum_{t} \\log P_{\\theta}(x_{t+1} \\mid x_{t:1}), $$\nwhere $P_{\\theta}$ represents large language model under training.\nCore Idea # Multi-Token Prediction Task # In this work, authors propose to learn language modeling from a multi-token prediction rather than a next-token prediction. At each position of the training corpus, the model is instructed to predict $n$ future tokens at once. Thus, the training objective is changed as follow:\n$$ L_n = - \\sum_{t} \\log P_{\\theta}(x_{t+n:t+1} \\mid x_{t:1}) = - \\sum_{t}\\sum_{i=1}^{n} \\log P_{\\theta}(x_{t+i} \\mid x_{t:1}). $$\nMemory-Efficient Implementation # Directly training language models by minimizing the multi-token prediction loss could result in high GPU memory usage, severly limiting the allowable batch-size. Thus, authors propose to carefully adapt the sequence of forward and backward operations for each prediction head rather than operating forward and backword operations simultaneusly for all heads. This could result in reducing peak GPU memory usage $O(nV+d)$ into $O(V+d)$. Here, the $n$ and $V$ denote the number of head and vocabulary size, respectively. Note that $d$ is the vector dimension of shared transformer trunk.\nFaster Inference with Self-Speculative Decoding # For speed up in inference time, authors utilize self-speculative decoding (Stern et al., 2018) scheme. Specifically, instead of iteratively predicting a next single token for the given token sequence, authors directly generate n-token using n independent output heads in a single step. This significantly speed up the decoding stage.\nResult # Learning global patterns with multi-byte prediction # To show using multi-token prediction loss helps to capture global pattern than using next-token prediction loss, they include experiment using extreme case of byte-levle tokenization. Notably, as shown in the table 1, multi-token prediction (8-byte prediction) models significantly solve more problem in the case of trained on small number of data.\nCoding Benchmarks # Pretrained model with multi-token prediction loss maintains an edge on that with next-token prediction loss. At the beginning, they pretrain the 7B parameter models with multi-token prediction loss or next-token prediction loss. (Use the pretrained model on MBPP, HumanEval and APPS) Then, they finetune the models with CodeContests dataset (Li et al., 2022) with multi-token head or next-token head.\nWhy does it work? # Conclusion # Discussion # "},{"id":5,"href":"/docs/spring24/05_/","title":"05","section":"Spring24","content":" Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting # Author: Liu F, Tang Y, Liu Z, Ni Y, Han K, Wang Y Written by Nayoung Kwon and Jiwoong Im\nIntroduction # The growing demand for rapid and efficient inference in large language models (LLMs) faces a significant bottleneck\nDecoding K tokens requires K sequential runs of the model. ⇒ LLM inference is slow\nTo address this issue, speculative decoding has been introduced as a promising approach to accelerate LLM inference without altering the output quality. This method leverages two key observations about LLM inference:\nMany tokens can be predicted with minimal computational overhead. LLM inference is predominantly constrained by memory bandwidth rather than arithmetic computations. However, existing speculative decoding still face limitations, such as high inference latency and suboptimal token acceptance rates. This papaer proposed Kangaroo to address this challenge.\nBackgrounds # #What is speculative decoding? Speculative decoding is an apporach to accelerate LLM inference. *Draft model: Additional model to accelerate inference (also known drafter) *Verifier or target model: Original large LLM\nLayer Early Exiting # The author has proposed a novel self-speculative decoding framework, named Kangaroo. Kangaroo utilizes double early exiting mechanisms, layer early exiting and draft early exiting. Layer early exiting suggests the equivalent self-draft small model exiting early from the fixed shallow layers of the large LLM and connecting to an adapter network to generate draft tokens. While this strategy is common for self-speculative decoding frameworks, Kangaroo has further investigated suitable architectures of the adapter module and offered a low-cost approach to train a lightweight model. Draft early exiting uses early exiting at suitable points during the drafting phase to avoid unnecessary computational overhead on more challenging tokens.\nEvaluation Metrics # Speculative decoding is often evaluated using two primary metrics: walltime speedup ratio and compression rate. Given a speculative decoding algorithm, we assume that \\(N\\) tokens should be generated via the drafting model. As the drafting model predicts multiple tokens in each decoding step and multiple tokens can be accepted by the large model in a step, we record the number of accepted tokens per step as a list \\( S = \\[s_1,\\, s_2,\\, \\dots,\\, s_{|S|}\\] \\) , where \\( \\sum_k s_k = N \\) and \\( |S| \\) denotes the number of steps. Then, the compression rate (CR) is defined as: \\[\\text{CR} = \\frac{1}{|S|} \\sum_k s_k.\\] However, once a draft token is rejected during the verification, all subsequent tokens sampled from the drafting model will be discarded. Therefore, CR does not accurately reflect the acceptance levels for tokens at varying distances, and the author has proposed a new evaluation metric named consistent token acceptance rate.\nThe consistent token acceptance rate \\( \\text{CTAR}(w) \\) is calculated as: \\[\\text{CTAR}(w) = \\frac{1}{|S|} \\sum_k \\\\] Draft Early Exiting # Experiments # Discussion and Conclusion # "},{"id":6,"href":"/docs/spring24/06_/","title":"06","section":"Spring24","content":" # "},{"id":7,"href":"/docs/spring24/07_/","title":"07","section":"Spring24","content":" EECE695E_2024_Spring_Blog: VeRA: Vector-based Random Matrix Adaptation [1] # EECE695E_2024_Spring_Blog for Efficient Machine Learning Class w/ Sejin Park and Kyumin Cho\nIntroduction to LoRA (Low Rank Adaptation) [2] family of PEFT (Parameter Efficient Finetuning) # Large language Models or LLMs consists of at least billions of parameters. This makes it extremely expensive to train and finetune. For example, the weights of GPT-3 175B can take up to 350GB when stored in FP16 precision (2 bytes per FP16 x 175B=350GB). When training such models additional information such as the optimizer states and gradients are needed. Assuming FP32 training with AdamW optimizer a single weight parameter of the model, it requires 4 bytes to store the weight itself in FP32, 8 bytes per parameter to for the optimizer AdamW (where two states, first moment and second moment in FP32, are maintained for each parameter), and 4 bytes per parameter to store the gradient in FP32. This adds up to 16 bytes of storage space needed for each model parameter required for training. [3] This means that a full finetune of a small model such as Llama-3 8B can take 128GB (16 bytes x 8B = 128GB) just to store the parameters. This calculation excludes the forward activations as well as the training batch data. This makes even training relatively small models impossible on a single GPU as datacenter-class GPUs such as A100 or H100 max out at 80GB for GPU and especially for consumber level GPUs such as RTX 4090 which only has 24GB.\nNot only does the weights needs to be stored on the GPU VRAM during VRAM, each finetune version of the model needs to store the entire modified copy. This means even if mass-storage devices like HDDs are used, it becomes prohibitively impossible to store multiple custom finetune version of the data itself.\nTherefore parameter efficient finetuning or PEFT methods have been developed that is able to finetune and specialize LLMs by only using small amount of parameters, this not only reduces the number of GPUs required to train the model itself, it cuts down on the permanent storage capacity required to store multiple versions of it. The most popular of these approaches is low rank adaptation or LoRA. As its name suggests, this technique uses low-rank matricess to represent large matrices in LLMs. The hidden dimension size in LLMs gets very large with size with GPT-3 175B having a hidden dimension (d) of 12,288. By multiplying two matrices with extremely low rank (as low as 1 or 2), it is possible to represent a large matrix. By encoding changes to the original weight in this large matrix new versions of the model can be stored with a very small memory footprint. In the case of GPT-3 175B, the authors of the paper reported reduction as large as 10,000x (from 350GB to 35MB), with rank size of 4 and when being only applied $W_Q$ and $W_V$ projection matrices. This low rank reduction of changes is based on the assumption that the change in weights during finetuning has a low intrinsic rank, based on previous studies [7] [8] that claim that learned over-parameterized models reside in the low intrinsic dimension.\nSince only the differences to the original model are tracked in training, original model parameters can be frozen and only the small low-rank matricess need to be trained. Gradients or optimizer states don\u0026rsquo;t are not required for the original model, only for the small low-rank matricess, so this greatly reduces the GPU VRAM requirement. Also, when servicing large variations of custom finetune models, only a single copy of the large model needs to be stored and each version only needs to store the small low-rank matricess that represents the difference between the original weights. This makes servicing large number of variations feasible and makes switching model versions easy as only the small LoRA weights need to be loaded and merged without loading the entire model itself.\nLet the pre-trained weight matrix be $(W_o \\in \\mathbb{R}^{d \\times k})$.\nThe modified weight matrix is given by: $[W_o + \\Delta W = W_o + BA,]$ where $(B \\in \\mathbb{R}^{d \\times r})$, $(A \\in \\mathbb{R}^{r \\times k})$, $( \\text{rank } r \\ll \\min(d, k))$, and $(\\Delta W = BA)$.\nThe original forward pass is: $[h = W_o x]$\nThe modified forward pass is: $[h = W_o x + \\Delta W x = W_o x + BAx]$\nThis can be shown in the following diagram. (Insert Diagram of LoRA here)\nIn LoRA, $W_o$ matrix usually corresponds to $W_Q$, $W_K$, $W_V$, or $W_O$, query, key, value, and output projection matrices of attention as opposed to Feed Forward Networks (FFN) matrices as hidden size of FFNs tend to be much larger then projection matrices of attentions.\nDuring training $B$ can be initialized as 0 so that $\\Delta W = B A$ is also 0 when training starts.\nWhen LoRA weights are deployed the original weights and the LoRA weights can be merged, $W = W_o + B A $, before inference proceeds as usual. The original weights can be obtained by subtracting the LoRA weights ($B A$).\nUnlike other PEFT methods such as adapter layer insertion, LoRA adds no additional latency after the weights are merged as the forward inference operation is exactly the same and no additional operation needs to be performed. This contributed to the popularity of LoRA as no changes to the inference code needs to be made and only weight merging operations before inference are needed which is relatively quick and easy to perform.\nHow VeRA works # Even with parameter efficient nature of LoRA it still requires a non-trivial amount of storage for each version. If a custom version was wanted for each vendor or consumer the storage requirement can easily add up. Even for a PEFT technique like LoRA, a finetune version of GPT-3 175B with a low rank of 4 applied to only query and value projections needed several dozen megabytes in storage. If a custom version was to be stored for each users, a million users would amount to dozens of terabytes of storage. This limits the scalability of LoRA for personalization and demands an even more PEFT technique than LoRA which is where VeRA comes in.\nVeRA tries to take advanatage of random matrices and projection to reduce the number of unique parameters needed for each finetune. The idea is to take a pair of randomly initialized matrices and attach a pair of scaling vectors that reparameterize it. The randomly initialized matrices remain frozen while the scaling vectors are trainable. If we use the same keys when generating the random matrices through a PRNG (pseudorandom number generator). We do not need to store the random matrices and only need to store the smaller scaling vectors. This greatly reduces the storage requirement of VeRA and allows larger ranks without drastically increasing the storage requirement.\nThis table taken from [1] shows the relative storage efficiency of VeRA compared to LoRA. Under the assumption that LoRA and VeRA are applied to the query and key projection layers.\nModel Rank LoRA - # Trainable Parameters LoRA - Required Bytes VeRA - # Trainable Parameters VeRA - Required Bytes ${RoBERTa}_{\\text{base}}$ 1 36.8K 144KB 18.4K 72KB ${RoBERTa}_{\\text{base}}$ 16 589.8K 2MB 18.8K 74KB ${RoBERTa}_{\\text{base}}$ 256 9437.1K 36MB 24.5K 96KB ${RoBERTa}_{\\text{large}}$ 1 98.3K 384KB 49.2K 192KB ${RoBERTa}_{\\text{large}}$ 16 1572.8K 6MB 49.5K 195KB ${RoBERTa}_{\\text{large}}$ 256 25165.8K 96MB 61.4K 240KB GPT-3 1 4.7M 18MB 2.4M 9.1MB GPT-3 16 75.5M 288MB 2.8M 10.5MB GPT-3 256 1207.9M 4.6GB 8.7M 33MB Performance # Extensions # DVoRA (DoRA + VeRA) # DoRA or () [4] is a\nFuture Avenue of Research # Behavior of VeRA compared to LoRA # In the paper LoRA Learns Less and Forgets Less [5] the authors claim that LoRA underpeforms full finetuning but, tends to retain the base model performance better on tasks outside of the finetune training data. The authors posits that this is due to the fact that full finetuning tends to fine higher rank weight perturbations compared to LoRA. Considering that VeRA has even fewer tunable parameters and rank of VeRA can be increased more freely compared to LoRA it seems that it would be worthwhile to explore the behavior of VeRA compared to LoRA. The original VeRA paper either used relatively older encoder-based models such as RoBERTa, relatively ocarse evaluations such as GLUE or ROGUE, or relatively simple instruction tuning dataset (Alpaca Cleaned). This paper focuses much more on relevant and challenging LLM tasks such as\nFew of things that can be compared is:\nHow performance of VeRA fares compared to LoRA and full finetuning on target domain task performance. Does VeRA exhibit the same regularization characteristic as LoRA by forgetting less of the source domain? Does sample-efficiency suffers compared to LoRA and full finetuning? NAS (Neural Architecture Search) to VeRA # Better initialization settings # The initalization scheme used in VeRA is relatively simple. The original VeRA paper does present some exploration and ablation studies of initialization schemes. The authors claim that using both $d$ and $b$ scaling vectors improve performance, using Kaiming uniform initialization for the performance is better, and initializing $d$ vector with $d_init$ set to $10^-1$ or $10^-7$ tends to outperform 1.0.\nBut, they are relatively limited and focus on relatively old model (RoBERTa) and coarse benchmarks such as RTE, MRPC, CoLA, and STS-B tasks. Additional experiments on more relevant LLM tasks such as instruction finetuning or continued pretraining could be more insightful as well as more diverse modalities(vision, sound, et cetera). For example, LoRAs have become a popular in diffusion models such as Stable Diffusion as a way of generating custom images. It would be meaningful to explore the behavior and the best settings for VeRA in these type of applications and tasks.\nAlso, the fact that the rank can be scaled freely in VeRA with not much overhead was underexplored in the original paper. By varying and expanding the rank size to be much greater than what is feasible with LoRA it seems possible that VeRA could have higher rank perturbations compared to LoRA possibly leading to different behaviors. Varying the rank and the initializations of VeRA and comparing the SVD decomposition of VeRA, LoRA, and full finetuning seems like an underexplored topic. How different configurations of VeRA can change the behavior of the weight perturbations or how it relates to performance could be important for exploring how the weight features changes with finetuning. For example, the LoRA Learns Less and Forgets Less paper claims that on full finetuning on code and math the model does not learn low-rank perturbations unlike the original assumptions behind LoRA. Considering that VeRA is able to expand to much higher rank, SVD analysis of VeRA when trained on complex tasks like code and math could yield interesting results.\nFuture universal random weights # The Platonic Representation Hypothesis [6]\u0026hellip;. If large fundamental models share a common representation, it is possible that there could be an ideal way to represent the randomized matrix basis on which VeRA operates well in. https://arxiv.org/abs/2405.07987\nReferences # [1]: D. J. Kopiczko, T. Blankevoort, and Y. M. Asano, “VERA: Vector-based Random Matrix Adaptation,” arXiv.org, Oct. 17, 2023. https://arxiv.org/abs/2310.11454\n[2]: E. J. Hu et al., “LORA: Low-Rank adaptation of Large Language Models,” arXiv.org, Jun. 17, 2021. https://arxiv.org/abs/2106.09685\n[3]: “Efficient training on a single GPU.” https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one#anatomy-of-models-memory\n[4]: S.-Y. Liu et al., “DORA: Weight-Decomposed Low-Rank Adaptation,” arXiv.org, Feb. 14, 2024. https://arxiv.org/abs/2402.09353\n[5]: D. Biderman et al., “LORA learns less and forgets less,” arXiv.org, May 15, 2024. https://arxiv.org/abs/2405.09673\n[6]: M. Huh, B. Cheung, T. Wang, and P. Isola, “The platonic representation hypothesis,” arXiv.org, May 13, 2024. https://arxiv.org/abs/2405.07987\n[7]: C. Li, H. Farkhoor, R. Liu, and J. Yosinski, “Measuring the intrinsic dimension of objective landscapes,” arXiv.org, Apr. 24, 2018. https://arxiv.org/abs/1804.08838\n[8]: A. Aghajanyan, L. Zettlemoyer, and S. Gupta, “Intrinsic dimensionality explains the effectiveness of language model Fine-Tuning,” arXiv.org, Dec. 22, 2020. https://arxiv.org/abs/2012.13255\n"},{"id":8,"href":"/docs/spring24/08_/","title":"08","section":"Spring24","content":" MIXTURE OF LORA EXPERTS # LoRA is a methodology for effective fine-tuning large-scale pretrained models.\nWorkflow of MoLE Background # What is LoRA? # LoRA is a methodology for effective fine-tuning large-scale pretrained models.\nModels such as OPT, LLaMA, and CLIP demonstrate remarkable performance when fine-tuned for various downstream tasks. However, full fine-tuning of these massive models requires substantial computational resources. LoRA enables parameter-efficient fine-tuning by keeping the pretrained model\u0026rsquo;s weights frozen and adding trainable low-rank decomposition matrices.\nLoRA Methodology In the above figure, only the matrices A and B are trained, with dimensions (d x r) and (r x d) respectively. By setting r \u0026laquo; d, the number of parameters to be trained can be reduced. These trained matrices are then added to the existing pretrained weights, allowing tuning without affecting the inference speed of the original model.\nLoRAs Composistion # The common solution to further improve the performance of LoRA is to compose multiple trained LoRAs. Research on LoRA composition can be broadly categorized into the following two methodologies.\nLinear arithmetic composition. It is a method of directly adding multiple LoRAs. This approach is simple and has been effective in the NLP and Vision-Language domain, but it can result in the loss of pre-trained model\u0026rsquo;s generative capabilities or the individual characteristics of each LoRA.\n\\[$$\\hat{\\mathbf{W}} = \\mathbf{W} \u0026#43; \\sum_{i=1}^{N} w_i \\cdot \\Delta \\mathbf{W}_i$$\\] Reference tuning-based composition tackles the above limitations of linear arithmetic method by introducing gradient fusion and controllable sampling, but is requires retaining when incorporating different LoRAs or creating new masks, which results non-trivial computational costs.\n(Left) Linear arithmetic composition. (Right) Reference tuning-based composition Mixture-of-Experts # MoE is an effective method that allows scaling up the number of parameters while maintaining the computational cost of the model.\nExperts FFN Layers: MoE layer is composed of N separate feed-forward networks as the experts. This concept involves dividing the FFN layer of traditional transformers into N experts. These experts can be thought of as being responsible for specific tokens.\nGating functions (Router): A function that determines the weights over the experts outputs. For the hidden representation h of input token, and the trainable embedding e of each a expert, the gate value a is obtained as follow:\n\\[\\alpha(E_i) = \\frac{\\exp(h \\cdot e_i)}{\\sum_{j=0}^{N} \\exp(h \\cdot e_j)}\\] The output is a weighted sum of the outputs from the top-k experts, determined by the gated values.\n\\[O = h \u0026#43; \\sum_{i=0}^{N} \\alpha(E_i) \\cdot E_i(h)\\] Illustration of a Swith Transformer block. Mixture of LoRA experts # Motivations # Direct linear arithmetic composition reduced the generative power of the model, while normalized linear arithmetic composition retained the generative power of the model but lost its LORA character. In the V\u0026L domain, directly composing multiple trained LoRAs into the original embedding caused significant parameter variations and meaningless output, while normalization compromised their original characteristics. In the NLP domain, composing four or more LoRAs within the FLAN-T5 model resulted in disordered output, and weight normalization across five datasets decreased the performance, suggesting adverse effects on the intrinsic qualities of the trained LoRAs. 2. Each layer of the trained LoRA represented a unique characteristic, which cumulatively defined the overall properties of the LoRA. (Right: Observed that different layers of LoRA encode distinct features, such as dog coat color and facial features., left: When evaluated on a subset of datasets, there were significant differences in performance across the different layers of LoRA.) So, The conjecture is that adjusting the characteristics by varying the layer-specific weights according to the desired domain objective will result in a more effective composition of trained LORAs.\nMethod # See related formulas Symbols input $x \\in \\mathbb{R} ^ {L \\times d}$ L: sequence length d: dim of $x$ Multi attention layer : $$\\mathcal{f}_{Attn} (\\centerdot)$$ Feed forward neural network layer: $$\\mathcal{f}_{FFN} (\\centerdot)$$ LN: layer normalization Trained LORAs $$\\Omega = \\left\\{ \\Delta \\Theta \\right\\}^N_{i=0}$$ learnable gating function $$\\mathcal{G} (\\centerdot)$$ The weight of the $i^{th}$ trained LorA $$\\mathcal{G}_i (\\centerdot)$$ Concatenation operation: $$\\oplus$$ Learnable parameter $e \\in \\mathbb{R} ^ {N^2 \\times L \\times d}$ Learnable temperature scalar $\\tau$ Freezing part $$x^\\prime_{\\theta} = x + \\mathcal{f}_{Attn} (LN(x)|\\theta)$$ $$\\mathbf{F}_\\theta (x) = x^\\prime_{\\theta} + \\mathcal{f}_{Attn} (LN(x^\\prime_{\\theta})|\\theta)$$ LoRA part $$x^\\prime_{\\Delta \\Theta_i} = x + \\mathcal{f}_{Attn} (LN(x)|\\Delta \\Theta_i)$$ The output of each LoRA $$\\mathbf{E} _{\\Delta \\Theta_i} (x) = x^\\prime_{\\Delta \\Theta_i} + \\mathcal{f}_{FFN} (LN(x^\\prime_{\\Delta \\Theta_i})|\\Delta \\Theta_i)$$ The output of all LoRA $$\\mathbf{E}_\\Omega (x) = Normalization(\\mathbf{E}_{\\Delta \\Theta_0} (x) \\oplus \\ldots \\oplus \\mathbf{E}_{\\Delta \\Theta_{N-1}} (x)) \\in \\mathbb{R} ^ {N \\times L \\times d}$$ Flatten and dot product operation $$\\epsilon = Flatten(\\mathbf{E}_\\Omega (x))^T \\centerdot e, \\epsilon \\in \\mathbb{R} ^ N$$ Gate value for each LoRA $$\\mathcal{G} (\\epsilon_i) = \\frac {exp(^{\\epsilon_i} /_ \\tau)} {\\displaystyle\\sum_{j=1}^N {exp(^{\\epsilon_j} /_ \\tau)}} $$ Final output of the gating function $${\\tilde{\\mathbf{E}}_\\Omega (x)} = \\displaystyle\\sum_{i=0}^N {\\mathcal{G} (\\epsilon_i) \\centerdot \\mathbf{E} _{\\Delta \\Theta_i} (x)} , {\\tilde{\\mathbf{E}}_\\Omega (x)} \\in \\mathbb{R} ^ {L \\times d} $$ Final output of Transformer block $$\\mathcal{O}(x) = {\\mathbf{F}_\\theta (x)} + {\\tilde{\\mathbf{E}}_\\Omega(x)} $$ Training # The final loss function used in MoLE is as follows:\nAlpha is a coefficient for weight balancing. Gating Balacing Loss\nAs shown in Figure 5 (a), the average entropy of the distribution probabilities from the gating functions gradually decreases as training progresses. In Figure 5 (b), we can see a gating probability of 64% for LoRA β among the three LoRAs, indicating that the gating function tends to converge to a state where it assigns large weights to well-performing LoRAs in the early stages. This can result in a significantly larger impact from a few specific LoRAs compared to others, potentially leading to biased outcomes. To avoid this, the author created a gating balancing loss. The gating balancing loss helps prevent bias by ensuring that the loss value decreases as the model becomes less biased. See related Symbols M: The num of blocks where gating functions are placed N: num of LoRAs Domain-specific Loss In V\u0026amp;L, Using a loss in CLIP(Radford et al,20221b) In NLP, Using a loss in FLAN-T5(Chung et al,2022)\nResults # On V\u0026amp;L Domain Setup) Base generator: DeamBooth(Ruiz et al., 2023) (built on Stable Diffusion V2.1) LoRA: combination of three separately trained LoRAs Image resolution: 512x512 learning rate: 1e-5 DDPM sampler (Ho et al., 2020) with 50 steps in each case Train 400 iterations for each required composition with batch size 2 and α as 0.5 Metrics) Image alignment: Evaluate the visual similarity of generated images with individual composed concepts in the CLIP image feature space. Text alignment: Evaluate the text-image similarity of generated images with given text prompts in the CLIP feature space. For each composition, calculated the average scores among 200 generated images per prompt using 5 text prompts. Compared Baselines) Normalized linear arithmetic composition SVDiff (Han et al., 2023) Results) It demonstrates better performance compared to other models and shows outstanding results in other tasks as well. When viewing the generated images, it is evident that all specified subjects are accurately represented and maintained. On NLP Domain Setup) Base Model: Flan-T5 (Chung et al., 2022) LoRA: Several LoRAs based on FLAN datasets learning rate: 1e-5 Train 800 iterations for each required composition with batch size 12 and α as 0.5. Compared Baselines) LoRAhub PEMs Results) It can be observed that MoLE demonstrates better performance in most tasks. Analyisis and Limitations # "},{"id":9,"href":"/docs/spring24/09_/","title":"09","section":"Spring24","content":" MobileNetV4 - Universal Models for the Mobile Ecosystem # Posted by JoonSeok Kim and DongGyu Kim\nQin, Danfeng and Leichner, Chas and Delakis, Manolis and Fornoni, Marco and Luo, Shixin and Yang, Fan and Wang, Weijun and Banbury, Colby and Ye, Chengxi and Akin, Berkin and others arXiv preprint arXiv:2404.1051 Main Contributions # MobileNetV4 targets designing neural networks for mobile devices. Since the mobile platform can only offer limited compuation ability and DRAM utilization, software engineers are trying to design small and efficient neural networks. To use AI at the industry level, the inference latency must also be small. Main objectives of designing inference models for mobile devices are\nAcceptable test performance on widely-used datasets such as ImageNet-1k Low inference latency for utilization in mobile devices Minimization of the number of parameters for low memory utilization on mobile platforms Minimization in the number of MACs for high enegy efficiency This paper mainly focuses on lowering inference latency while maintining the test accuracy up to SOTA mobile neural net performance. Since it targets mobile platforms, it analyzes performance of various mobile hardwares, and designs a neural network to fit the harwares maximum performance. The designing process was done by the NAS technique, where the intantiation of UIB blocks were set as the search space. The main contributions of this work can be states as follows.\nUniversal Inverted Bottleneck (UIB) seach block - Unifies the Inverted Bottleneck (IB), ConvNext, Feed Forward Netowork (FFN), and Extra Depthwise variant Mobile MQA - Attention block tailored for mobile accelerators NAS technique to improve performance Achieves Pareto optimal acorss various devices such as CPUs, DSPs, GPUs, and TPUs Novel distillation technique to boost accuracy. Achieves 87% accuracy on ImageNet-1k and 39x smaller model size Preliminaries - Inverted Residual Blocks and Linear Bottlenecks # Fig. 0 (a) Original Residual Block Previously, the residual bottleneck block was propsoed, which consists of the 1x1 pointwise convolution in the first layer, a depthwise convolution in the second layer, and the final pointwise convolution layer, where its output is residually connected with the module\u0026rsquo;s input. The first layer acts as a projection layer to generate the narrow, and parameter-efficient convolution layer for depthwise convolution. This part acts as the bottleneck layer. The output of this layer is expanded again in the pointwise convolution. Here, the module forms a wide-narrow-widw approach considering the number of channels.\nFig. 0 (b) Inverted Residual Blocks However, in MobileNetV2, the authors use the inverted residual blocks with linear bottlenecks. In opposed to the original residual block where the first pointwise convolution acts as the projection layer and the final pointwise convolution acts as the expansion layer, the inverted residual block applied this in the opposite way to create the narrow-widw-narrow block. This assumes that the low-dimensional feature data is stored in the \u0026ldquo;narrow\u0026rdquo; layers, and these need to be expanded and extracted through the depthwise convolution. The required information in the narrow layers are passed onto the deeper layers through residual connections. Also, one has to take note that the final pointwise convolution layer in each inverted residual blocks do not have activations, in order to compensate the performnace degradation due to squeezing the layers where the skip connections are linked.\nPreliminaries - Roofline Model and Hardware Efficiency # Algorithm running on hardware is composed of two parts - memory access and computation. The computation time is determined by the computation requirement and hardware performance.\n$\\text{runtime\\_computation} = \\frac{\\text{Number of Operations}}{\\text{FLOPS}}$ Algorithm runtime can be limited by the memory access bottleneck or communication overhead\n$\\text{runtime\\_communication} = \\frac{\\text{Number of IO Bytes}}{\\text{Bandwidth}}$ Hardware performance is determined by the upper bound of the computation time or memory access latency\n$\\text{performance} = \\max(\\text{runtime\\_computation}, \\text{runtime\\_communication})$ Below Fig. 1(a) and Fig. 1(b) illustrates the roofline model and its characteristics\nFig. 1 (a) Roofline Model Fig. 1 (b) Roofline Model with Ceiling Hardware-Independent Pareto Efficiency # Fig. 2. Ridge Points and Latency/Accuracy Tradeoffs Fig. 3. Op Cost vs. Ridge Point This research focuses on efficiency on various hardware targets such as DSPs, CPUs, and GPUs. To find whether the hardware is limited by its memory bottlenecked or compute bottlenecked, the Roofline Model of that hardware must be investigated. It is defined by the harware's peak computational throughput and its peak memory bandwidth. The optima of that hardware can be found in its ridge point, which is defined by the hardware's ratio of Peak MACs to Peak memory bandwidth. The algorithm's accuracy and latency are swept by the ridge point on various hardware on Fig. 2, and Fig. 3. The roodline model of MobileNetV4 achieves highest Pareto-optimal performance compared to other MobileNet models. MobileNetV4 is designed to achieve Pareto optimal and hence balances MAC operations and memory bandwidth. The initial layers are designed with high MAC intensity, so as to improve model capacity and downstream accuracy. The end layers use identically-sized FC layers to maximize accuracy. These two initial and end layers are balances so that MobileNetV4 should not see slowdowns at any hardware.\nUniversal Inverted Bottlenecks (UIB) # Fig. 4. Universal Inverted Bottleneck (UIB) blocks The main advantage of UIB is its adaptability and flexibility, that mitigates seach complexity. Optional Depthwise (DW) convolution blocks are inserted before the expansion layer, and between the expansion and projection layer. In the NAS procedure, common components such as the pointwise expansion and projection are shared and DWs are added as search options. UIB has four possible instantiations as follows.\nInverted Bottleneck (IB) : Spatial mixing on the expanded features activations, and provides higher model capacity ConvNext : Cheaper spatial mixing before the expansion with larger kernel size ExtraDW : Inexpensive increase of the network depth and the receptive field. Combined benefits of ConvNext and IB FFN : Stack of two 1x1 pointwise convolutions. Accelerator-friendly operation Mobile MQA # Table 1. Efficiency Gains by MQA This paper considers the Operational Intensity (OI), which is the ratio of arithmetic operations to memory access, to enhance efficiency of vision models on mobile accelerators. Here, Multi-Query Attention (MQA) is proposed instead od Multi-Head Self Attention (MHSA), which is simplified by utilization of shared keys and values across all heads. This sharing of keys and values reduces memory access hence improving OI, especially when the batch size is small. Large language models does not have significant accuracy drop in this MQA case. Table 1 shows that by adding MHSA and MQA, the performace accuracy has increased whereas the inference latency for MQA is approximately x39 lower than that of MHSA. Hence, MQA can accelerate better in the mobile environment, with negligible performance degradation.\nThe Spatial Reduction Attention (SRA) is applied, hence incorporating asymmetric spatial down-sampling, to downscale keys and values, and not queries. In hybrid models, there is a certain correlation between spatially adjacent tokens, hence necessitating spatial mixing convolution filters.\nRefined NAS for Enhanced Architectures # As shown above, the insitantiation of UIB blocks are in the neural architecture search process. TuNAS was adopted for the paper\u0026rsquo;s search strategy. The paper uses a two-stage search operation, the coarse-grained search and fine-grained serach to address the variance in parameter counts between UIB\u0026rsquo;s depthwise layers and other search options. The course-grained search process involves determining optimal filter sizes with fixed parameters. The fine-grained stage searches for the UIB\u0026rsquo;s layer configuration.\nResults - Comparisons with Other Works # Table 5. Classification results on ImageNet-1k Table 6. Object Detection results on the COCO validation set Results on the classification performance on ImageNet-1k dataset show that the MobileNetV4 achieves the highest performance and smallest latency compareed to other models on various mobile platforms such as CPUs and DSPs of mobile phones. While other models have closely competitive latency with the MobileNetV4 model, their latency is much higher.\nThe effectiveness of MobileNetV4 as backbone networks are tested on the COCO object detection experiment. The number of MACs were set to be similiar, and the Retina framework was used as the object detector. As the same as classification, MobileNetV4 achieves highest performance compared to other mobile-target modles, with the lowest CPU latency. Hence, the ability of MobileNetV4 for mobile devices can be shown.\nConclusion # This paper proposes the MobileNet V4 series, a universal high-efficiency model that operates efficiently across a wide range of mobile environments. By introducing a new Universal Inverted Bottleneck and Mobile MQA layer and applying an enhanced NAS recipe, MobileNet V4 achieves near Pareto-optimal performance on various hardware, including mobile CPUs, GPUs, DSPs, and dedicated accelerators. Additionally, using the latest distillation techniques, it demonstrates cutting-edge performance in mobile computer vision by achieving 87% ImageNet-1K accuracy with a latency of 3.8ms on the Pixel 8 EdgeTPU. The paper also presents a theoretical framework and analysis for understanding the model\u0026rsquo;s universality across heterogeneous devices, providing guidance for future design. Perspectives, Discussions and Future Research Directions # MobileNetV4 was designed with a focus on optimizing hardware performance, particularly for mobile devices such as mobile CPUs, GPUs, and DSPs. It leveraged Neural Architecture Search (NAS) to design its Unit Inverted Bottleneck (UIB) blocks and employed distillation techniques to enhance performance. Additionally, MobileNetV4 used MQA to incorporate transformer-like operations, further boosting its performance. The authors aimed to integrate as many state-of-the-art techniques as possible to optimize the neural network\u0026rsquo;s performance in mobile environments. Given its recent development, MobileNetV4 is likely the state-of-the-art convolution-based neural network for mobile platforms.\nThe novelty of this work appears to be the neural architecture search involving the UIB, which includes the expansion and projection blocks with inverted residuals previously introduced in MobileNetV2. Since the performance is validated across various mobile platforms, MobileNetV4 presents an attractive solution for industry applications in mobile systems. However, the paper seems to combine existing methods, techniques, and modules into one comprehensive experiment to produce the optimal MobileNet model. The optimization was conducted logically and practically, but it would have been helpful if the authors had provided an analysis of the finalized MobileNetV4, explaining why NAS designed the UIB in that specific way and how features are extracted at each layer.\nIt is worth noting that many academic labs are currently designing mobile-level transformers like MobileViT or FastViT. This paper shows that these transformer-based models have similar performance to MobileNetV4 but with significantly higher inference latency. Although the parameter counts and the number of MAC operations are comparable, the latency difference raises questions about the practicality of developing mobile-level transformers for vision tasks. Despite incorporating MQA blocks, it appears beneficial to use bottleneck modules as the primary feature extractors.\nBeyond MobileNetV4, many CNNs are integrating multi-head attention layers, while transformers are incorporating convolutions. Convolutions capture local feature relationships, whereas attention modules provide global features in computer vision. Combining these two characteristics enhances neural network performance across the board. In the future, it would be practical to design NPUs and domain-specific accelerators that enable fast and efficient computation for both convolutions and attention mechanisms simultaneously.\nSimilar Works - MobileFormer # Fig. 5. MobileFormer Network A similiar work that utilizes the MobileNet and trasnformer modules was introduced in CVPR 2022, the Mobile-Former. This structure capitalizes on MobileNet\u0026rsquo;s strength in local processing and the transformer\u0026rsquo;s capability for global interaction. The bridge facilitates bidirectional fusion of local and global features. Unlike recent vision transformer approaches, the transformer in Mobile-Former uses very few tokens (typically 6 or fewer), which are randomly initialized to learn global priors, thereby minimizing computational cost. Additionally, the proposed lightweight cross-attention mechanism used to model the bridge enhances both computational efficiency and representation power.\nTable. 7. Performance comparison of MobileFormer with other works Mobile-Former demonstrates superior performance compared to MobileNetV3 in the low FLOP regime, ranging from 25M to 500M FLOPs on ImageNet classification. For example, Mobile-Former achieves 77.9% top-1 accuracy at 294M FLOPs, surpassing MobileNetV3 by 1.3% while reducing computations by 17%. In object detection tasks, Mobile-Former outperforms MobileNetV3 by 8.6 AP within the RetinaNet framework. Furthermore, when applied to the DETR model, replacing its backbone, encoder, and decoder with Mobile-Former results in a detector that not only surpasses DETR by 1.1 AP but also reduces computational cost by 52% and parameter count by 36%.\nReferences # [1] MobileNetV4 Implementation: [Link.](https://github.com/jiaowoguanren0615/MobileNetV4/tree/main)\n[2] Qin, Danfeng, et al. \u0026ldquo;MobileNetV4-Universal Models for the Mobile Ecosystem.\u0026rdquo; arXiv preprint arXiv:2404.10518 (2024).\n[3] Sandler, Mark, et al. \u0026ldquo;Mobilenetv2: Inverted residuals and linear bottlenecks.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n[4] Chen, Yinpeng, et al. \u0026ldquo;Mobile-former: Bridging mobilenet and transformer.\u0026rdquo; Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n"},{"id":10,"href":"/docs/spring24/10_/","title":"10","section":"Spring24","content":" Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length # Authors: Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou\nReviewer: Hyunho Kook\n1. Introduction # Recently, Large Language Models (LLMs) have been gaining popularity. The impressive performance and versatility demonstrated by large models above a certain level have started to be utilized in various fields. However, as the size of the models grows, the size of the data that the models are expected to process is also increasing. Examples of this include processing currently open issues by inputting a GitHub repository or translating a large volume of books without losing context. In addition, the ability to maintain context and carry on a conversation for an extended period within a single chat is also sometimes required. The transformer, which is the foundation model of modern LLMs, exhibits vulnerabilities in this regard. Firstly, since it uses KV cache, memory usage increases rapidly as the sequence length grows, and it has a computational complexity proportional to the square of the sequence length.\nTo address this problem, the authors propose a method that inherits and advances MEGA (exponential moving average with gated attention), the predecessor of this paper. The overall contributions are as follows:\nCEMA (Extending Multi-dimensional Damped EMA to Complex Domain), an extension of Exponential Moving Average (EMA) to the complex domain, is proposed. Timestep Normalization, an extension of Group Norm to the timestep domain, is proposed as an alternative to Layer Norm. Normalized Attention, which performs normalization during attention computation, is proposed. 2-hop Residual Connection, which composes residual connections in 2-hop units, is proposed. By employing these methods, the authors have created a transformer architecture that is linear with respect to context length. They have also addressed the issues encountered in the previous research, MEGA, which were (i) low performance and (ii) the need for different architecture structures for each data type or task.\n2. MEGA (exponential Moving avErage with Gated Attention) # 3. Methods # CEMA (Extending Multi-dimensional Damped EMA to Complex Domain) # Timestep Normalization # Normalized Attention # 2-hop Residual Connection # 4. Experiments # 5. Comparison with related works # There have been several similar related studies:\nEfficient Attention: FlashAttention optimized the GPU computation of the attention, showing advantages in speed without changing the existing mechanism. Additionally, there have been attempts to increase the context length by converting the attention mechanism to a linear one or compressing the KV cache.\nStructured State Space Model: A notable study in this area is Mamba, which added mechanisms like Selective Scan to a State Space model with linear time complexity, enabling it to process long context.\nHowever, these studies have limitations. Even if we accept that Flash Attention did not change the attention mechanism itself, Linear Attention, KV cache compression, and State Space Models have shown significantly lower performance on general benchmarks, although they may perform better than standard Transformers in long contexts.\n6. Discussion # In my opinion, there are a few potential limitations that are not extensively discussed in the paper:\nReliance on CEMA for Out-of-Chunk Context: The self-attention mechanism in MEGALODON is applied within each chunk. For data that falls completely outside the chunk boundaries, the model relies solely on CEMA for processing. This limitation could potentially hinder the model\u0026rsquo;s ability to handle long-range dependencies that span across multiple chunks.\nComplexity of the Architecture: Compared to the traditional Transformer layer, the MEGALODON architecture is considerably more complex. It requires the computation of EMA, including the complex domain, for each token. Additionally, several normalization and attention components have been introduced, such as Timestep Normalization, which further increases the complexity of the model compared to the previous works.\nLimited Exploration of Downstream Tasks: While the paper demonstrates the effectiveness of MEGALODON on long-context question answering tasks from the Scrolls dataset, the range of downstream tasks explored is relatively narrow. Evaluating the model\u0026rsquo;s performance on a broader set of tasks, such as summarization, dialogue generation, and composition, would provide a more comprehensive assessment of its capabilities and potential limitations.\nDespite these limitations, MEGALODON presents a promising direction for efficient long-context modeling. In my opinion, this kind of efficent and linear processing of memory can be a breakthrough for long-context LLMs.\n"},{"id":11,"href":"/docs/spring24/11_/","title":"11","section":"Spring24","content":" # "},{"id":12,"href":"/docs/spring24/12_/","title":"12","section":"Spring24","content":" Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies # Posted by: Harit Keawmuang, Junkyeong Park\nAuthors: Zichao Li (University of California, Santa Cruz), Cihang Xie (University of California, Santa Cruz), Ekin Dogus Cubuk (Google Deepmind)\nIn recent years, there has been a growing interest in image-and-language representation learning, which aims to capture the complex interactions between visual and textual information. The Contrastive Language-Image Pre-Training (CLIP) framework has emerged as a leading approach in this field, utilizing large-scale text and image data to create a unified representation space. CLIP has achieved remarkable performance across various tasks and has demonstrated robust generalization to out-of-distribution data. While prior studies on scaling CLIP have focused on scenarios with substantial computational resources, this paper investigates the performance of CLIP under resource constraints, specifically examining the effects of data size, architecture, and training strategies.\nThe study explores the impact of different training data sizes, showing that smaller, high-quality datasets can outperform larger, lower-quality ones. This is critical for practical applications where data quality and computational limits are significant considerations. The research also compares various architectures, highlighting that larger vision transformers (ViTs) do not always guarantee better performance and that CNNs may be more effective when data is limited. Additionally, the paper evaluates different training strategies, including SLIP, FLIP, CLIP, and CLIP+Data Augmentation, revealing that data augmentation can enhance performance without significant computational costs. These findings provide valuable insights for efficiently training and deploying CLIP models, making advanced image-and-language learning more accessible and affordable.\nWhat is CLIP? # CLIP effectively merges the capabilities of natural language processing (NLP) and computer vision. By learning from images and their textual descriptions, CLIP unifies text and image understanding, allowing it to perform various tasks without task-specific training.\nRelated Works # In the recent years, NLP has advanced significantly in pre-training language models on large text datasets. Simultaneously, computer vision has improved by pre-training convolutional neural networks (CNNs) on extensive image datasets. The CLIP model combines these approaches by jointly pre-training on images and text using a contrastive loss function, creating a shared embedding space for both.\nRecent efforts have focused on improving scalability and efficiency of CLIP model. For example, FLIP was introduced to minimize the computation by masking image patches, enabling larger batch sizes without sacrificing performance. Most research has focused on large-scale training with significant computational resources, utilizing ViT large models on extensive datasets. However, less attention has been given to optimizing CLIP for smaller training budgets.\nMethod # Training Pipeline, Dataset, and Hyperparameters # In this paper, they adopted the identical training approach as CLIP, which employs a contrastive loss to simultaneously train the vision and text encoders from scratch. This loss function encourages the encoders to map related image-text pairs to similar feature representations in a shared embedding space by minimizing the distance between positive pairs and maximizing the distance between negative pairs. Key aspects of the training include:\nMinimal data augmentation: Images resized to 224x224 and pixel values normalized to the range of -1 to 1. Optimizer: AdafactorShazeer \u0026amp; Stern with β1 = 0.9 and β2 = 0.999. Batch size: 16k Learning rate: Initial rate of 0.001 with a cosine learning scheduler and weight decay of 0.0001. Evaluation Matrices # Zero-shot transfer evaluation: Assesses the model\u0026rsquo;s ability to generalize to new tasks without fine-tuning. Linear probe evaluations: Freezes the vision encoder and optimizes the fully connected layer\u0026rsquo;s learning rate. Retrieval performance on MSCOCO captions: Ranks text captions based on cosine similarity with image embeddings, reporting Recall@1 for image-to-text retrieval and average results for text-to-image retrieval. Data # Data Quantity # To evaluate the effect of data quantity on CLIP\u0026rsquo;s performance, they conducted experiments with datasets of different sizes: 10M, 25M, 100M, 200M, and 400M. Using ViT-B/32 as the vision encoder, models were trained for 2 to 32 epochs.\nFigure 1. Data Quantity: Zero-Shot performances with the same dataset size across varied training epochs Results showed that for smaller datasets (e.g., 25M), increasing epochs did not significantly improve ImageNet performance. In contrast, larger datasets (e.g., 400M) benefited from more epochs. Additionally, zero-shot performance on ImageNet variants followed a similar pattern: larger datasets and longer training improved performance. However, the correlation between performance on ImageNet and its variants was inconsistent, with some datasets showing improved results in specific variants but not others.\nFigure 2. Data Quantity: Few-Shot Performances on ImageNet They also observed that the few-shot performance also showed a similar trend to the zero-shot performance.\nFigure 3. Data Quantity: Retrieval Performances on MSCOCO In Retrieval Performances, a slightly different trend emerged. Specifically, they found that there was little to no improvement in both image retrieval and text retrieval performance when the number of epochs exceeded eight.\nData Quality # They also examined the impact of data quality by creating subsets of the 3.4B dataset based on image-text similarity, selecting the top 20%, 40%, 60%, and 80% highest-quality data.\nFigure 4. Data Quality: Zero-Shot Performances on ImageNet. (a) trained for one epoch. (b) trained for the same number of sampled data. Models trained on these subsets for a single epoch demonstrated that higher quality data subsets yielded superior zero-shot performance on ImageNet. Specifically, the Top40% subset outperformed the entire dataset despite fewer iterations. When comparing datasets with an equal number of samples, the Top40% dataset achieved the best performance, highlighting the importance of data quality in training CLIP models.\nFigure 5. Data Quality: Few-Shot Performances on ImageNet. (a) and (b) one epoch. (c) and (d) the same number of sampled data. Additionally, when the number of sample data points is the same, higher quality datasets have superior 5-shot and 10-shot performance.\nFigure 6. Data Quality: Retrieval Performances on MSCOCO. (a) and (b) one epoch. (c) and (d) the same number of sampled data. When it comes to search performance, the top 80% datasets in particular show the most impressive retrieval performance.\nVariants of Vision Transformers # This study examines how the performance of various CLIP models, differentiated by the size of their vision encoders, is influenced by dataset size and the number of sampled data points. They used different vision encoders (ViT-Ti/16, S/16, B/32, B/16, L/16) while keeping text transformers fixed at vit-base. They sampled ten subsets from the full dataset, ranging from 10M to 3.4B samples, maintaining consistent data distribution and quality. Models were trained for one epoch to assess the effect of data quantity, ensuring fair comparison by training all subsets for the same number of iterations.\nFigure 7. Various ViTs: Zero-Shot performances with various numbers of sample data Zero-shot performance on ImageNet revealed that larger vision encoders (e.g., ViT-L/16) did not consistently outperform smaller ones when the sample size was under 100M. As data size increased, larger encoders showed better performance.\nFigure 8. Various ViTs: Zero-Shot performances with the same number of sampled data: 3.4B As the dataset size grows, the performance difference between larger ViTs and their smaller counterparts becomes more pronounced. Additiallay, accuracy trends across various datasets (ImageNet-R, ImageNet-Sketch, ImageNet-V2, ObjectNet) were nearly linear, except for ImageNet-A, which had a non-linear improvement, highlighting its challenging nature. (appendix)\nFigure 9. Various ViTs: Linear probing performances with various sizes of vision encoders with the same number of sampled data: 3.4B Linear probing results indicated that for smaller datasets, ViT-L/16 underperformed compared to smaller models, but excelled with more data. Larger ViTs demonstrated better robustness on out-of-distribution datasets.\nFigure 10. Various ViTs: Retrieval Performances on MSCOCO Retrieval tasks showed ViT-L/16 performed poorly with less than 100M samples but improved with more data, aligning with zero-shot trends and benefiting more from larger datasets compared to smaller models.\nComparison of Network Architectures # To effectively choose the best network architectures, they performed a comparison among the various architectures. Previous studies have explored various vision encoders for CLIP, such as ResNet, MLP-Mixer, and ViT, but some architectures like Swin-Transformer and ConvNext haven\u0026rsquo;t been investigated. Here, they compared CNN and vision transformer architectures with similar computational costs, including ViT-B/32, ResNet-50, ConvNext-T, Swin-T, and Mixer-B/32. In Zero-shot, when considering limited data samples, ResNet-50 performs better initially, but ViT-B/32 achieves superior performance with more samples due to its stronger ability to capture global information (see Figure 11(a)). In linear probing, MLP-Mixer outperforms others with fewer samples, but ViT-B/32 excels with larger datasets. ViT and MLP-Mixer show better robustness, likely due to their lower inductive bias, leading to improved generalization (Figure 11(b)). For retrieval tasks, ResNet-50 is better with smaller sample sizes, but ViT-B/32 surpasses it as sample sizes increase. Mixer-B/32 performs poorly in retrieval tasks, making ViT the preferred choice for CLIP\u0026rsquo;s vision encoder across various tasks.\nFigure 11. Performances of the various network architectures Training Strategies # In this section, the various training strategies for CLIP are explored, including SLIP, FLIP, and a proposed method from this paper called CLIP+Data Augmentation. SLIP enhances the vision encoder through self-supervised learning but is computationally expensive compared to the original CLIP. FLIP masks patches in training images to reduce computation. However, CLIP+Data Augmentation aimed to enhance CLIP\u0026rsquo;s vision encoder while mitigating the computational demands associated with previous self-supervised learning approaches. By applying data augmentation directly to input images, they offered a cost-effective alternative, validated across four subsets with 30 epochs of training using techniques like crop\u0026amp;flip, RandAugment, and Stacked RandAugment. The results in Figure 12 demonstrated consistent performance improvements of all three methods over raw CLIP, with no additional computational burden incurred, even enabling comparable performance to larger datasets, exemplified by the Stacked RA model trained on a dataset half the size achieving similar results.\nFigure 12. Comparison between various data augmentation for CLIP Their experiments on the ImageNet dataset show that SLIP outperforms CLIP and FLIP when training samples are under one billion, indicating the benefit of self-supervised learning for limited data. However, as sample size increases, CLIP and FLIP surpass SLIP, suggesting that enhancing vision encoders isn\u0026rsquo;t necessary for large datasets. Additionally, SLIP is twice as computationally expensive as CLIP and performs worst in zero-shot tasks when costs are equal. Data augmentation, particularly CLIP + Data Aug, improves performance and generalization on ImageNet and its variants without extra computational costs, especially for larger datasets and multiple epochs of training as presented in Figure 13.\nFigure 13. Zero-shot performance with the various training strategies In the linear probing evaluation, vision encoders trained with CLIP + Data Aug consistently outperformed the other strategies, particularly on OOD datasets. CLIP and CLIP + Data Aug also showed better robustness than SLIP with similar ImageNet accuracy. Combining CLIP with data augmentation offers a more effective feature extractor, balancing performance, and computation cost. The training results on linear probing performance are shown in Figure 14.\nFigure 14. Linear probing performance with the various training strategies In retrieval tasks, SLIP consistently outperformed CLIP, CLIP + Data Aug, and FLIP on both image and text retrieval across all dataset sizes. Unlike its zero-shot performance, SLIP showed the best results for retrieval tasks as presented in Figure 15, suggesting it is a superior strategy for these tasks despite being less effective for classification.\nFigure 15. Retrieval performances with the various training strategies Conclusion # This study examines how data size, network architecture, and training methods affect CLIP\u0026rsquo;s performance. Their experiments highlight the critical roles of data quantity and quality. They also demonstrate that data augmentation can improve CLIP\u0026rsquo;s performance with minimal additional computational cost. Furthermore, they investigate various network architectures and training strategies, finding that some outperform others depending on the computational budget, emphasizing the need for careful selection. From our perspective, the balance between computational efficiency and model accuracy is crucial, and exploring adaptive methods could yield significant benefits. Future research could focus on integrating transfer learning with CLIP to enhance domain-specific performance and investigating AutoML techniques for optimal architecture and strategy selection.\n"},{"id":13,"href":"/docs/spring24/13_/","title":"13","section":"Spring24","content":" Exploring µ-Parameterization in Large-Scale Neural Networks # Paper : A Large-Scale Exploration of µ-Transfer Author : Lucas Dax Lingle produced by Jeonghyun Choi, Minhye Choo Introduction # In the field of artificial intelligence, especially in natural language processing and computer vision, large neural network models have become a cornerstone. However, the initialization and learning rates of these models are often determined through heuristic methods, varying significantly between different studies and model sizes. This inconsistency can lead to suboptimal performance, particularly when scaling up models, resulting in inefficient training processes and less effective models. As models grow larger, the tuning process becomes increasingly costly and time-consuming.\nThe concept of µ-Parameterization (µP) provides a potential solution to this problem. µP offers scaling rules for model initialization and learning rates, enabling zero-shot hyperparameter transfer from small models to larger ones. This technique promises stable training and optimal hyperparameters at scale with minimal cost. Despite its potential, µP has not been widely adopted due to its complexity and the need for further empirical validation.\nFigure 1 : In the default parameterization in PyTorch, the graph on the left, the activation scales diverge in width after one step of training. But in µP, the graph on the right, the activation scales change by a consistent amount regardless of width for any training step. The y-axis shows the change of network activation scales on a fixed input after t=0, 1, 2, 3, and 4 steps of training as the width of the model varies, which is shown along the x-axis. .\nIn this blog post, we delve into the details of µ-Parameterization, its underlying principles, and its practical applications as explored in the paper \u0026ldquo;A Large-Scale Exploration of µ-Transfer\u0026rdquo; by Lucas Dax Lingle.\nWhat is µ-Parameterization? # Concept and Principles # µ-Parameterization (µP) is a set of rules for initializing neural networks and setting learning rates that allows for the seamless transfer of hyperparameters from small proxy models to larger target models. This approach is grounded in a Gaussian Process interpretation of deep neural networks, where the width of the network (number of neurons per layer) is a critical factor.\nThe core idea is to scale the initialization and learning rates based on the width of the network. The general formulation of µP when training with the Adam optimizer and using an i.i.d. Gaussian initialization is as follows:\nInitialization Variance: Parameters are initialized with a variance that scales inversely with the width of the layer. For example, if the width of a layer is M, then the initialization variance for weight matrices WAQ, WAK, WAV is \\(\\frac{1}{M}\\) .\nLearning Rate: The learning rate for each parameter is scaled based on the width of the network. For instance, the learning rate for the same weight matrices WAQ, WAK, WAV is \\(\\frac{αP}{M}\\) , where α is the base learning rate and P is a fixed proxy model width.\nThese scaling rules ensure that the behavior of small and large models remains consistent, facilitating the transfer of optimal hyperparameters across different model sizes.\nPractical Implementation # In practical terms, µP involves specific scaling rules for various components of a transformer model, which is a popular architecture in NLP and computer vision. For example, the initialization variance for the weight matrices in the attention mechanism and MLP blocks is set according to the width of the model. Similarly, the learning rate is adjusted to maintain consistent training dynamics across different scales.\nBelow is an example of µP scaling rules for transformers:\nParameter Initialization Variance Adam Learning Rate WE 1 α WAQ 1/M αP/M WAK 1/M αP/M WAV 1/M αP/M WAO 1/(HD) αP/M WFI 1/M αP/M WFO 1/F αP/M WU 1/M² αP/M Table 1: µP scaling rules for transformers.\nIn this table, M represents the model width, H the number of heads, D the head width, F the hidden width of the MLP, and α the base learning rate.\nµ-Transfer # Figure 2 : Illustration of µ-Transfer\nOne of the significant advantages of µ-Parameterization is the concept of µ-Transfer. This method allows hyperparameters, such as learning rates, found optimal in small models to be transferred directly to larger models without the need for extensive re-tuning. This process is particularly beneficial for scaling models efficiently and maintaining consistent performance across different model sizes.\nSteps in µ-Transfer # Training a Small Proxy Model: Begin by training a small proxy model, which is easier and less expensive to experiment with. Perform hyperparameter tuning on this model to find the optimal learning rate and other hyperparameters. For instance, let\u0026rsquo;s denote the optimal learning rate found for this small model as α.\nScaling the Hyperparameters: Use the scaling rules provided by µP to adapt the hyperparameters for a larger model. The key scaling rule here is that the learning rate should be adjusted based on the ratio of the widths of the large model to the small model. For example, if the small model has a width 128 and the large model has a width 2048, the scaled learning rate for the large model would be \\(\\frac{α2048}{128}\\) .\nApplying the Scaled Hyperparameters: Implement these scaled hyperparameters in the larger model. This involves adjusting the initialization variance and learning rates according to the µP rules to ensure that the training dynamics remain stable and consistent.\nBenefits of µ-Transfer # Efficiency: By using µ-Transfer, researchers can avoid the costly and time-consuming process of hyperparameter tuning on large models. Instead, they can perform this tuning on smaller models and scale the results.\nConsistency: µ-Transfer helps maintain consistent training dynamics across different model sizes. This consistency is crucial for achieving optimal performance as models scale up.\nSimplicity: The process of scaling hyperparameters using µP is straightforward once the initial tuning on the small model is complete. This simplicity can significantly reduce the complexity of managing large-scale model training.\nAblation Experiment # Setup \u0026amp; Objective # The objective of these experiments is to examine how various methods that can enhance performance affect the actual transfer of learning rates from smaller width models to larger models with µP and their impact on performance in reality. While the existing µP aimed at transferring initialization and learning rates, this experiment focused on the learning rate, an important hyperparameter in large transformer models. The experiments were implemented using Jax/Flax on TPU V3, and the optimal learning rate was determined by measuring the validation loss. Models with widths of \\(M\\) = {128, 512, 2048} have parameters ranging from 4.7M to 1.2B, with the depth fixed at L = 24. The reason for focusing solely on width and not depth is that in the case of depthwise µP, only one linear layer is used per residual block, whereas transformers use at least two layers. So, in this experiment, width is the main change to control the # of parameters.\nBaseline \u0026amp; Summary # The baseline represents the experimental results used as a reference for performance improvement or degradation across various experimental settings. In the baseline using µP, it is confirmed that the optimal learning rate for the smallest model is also the optimal learning rate for larger models that are 4x wider (16x larger). The experimental results can be categorized into three main groups:\nTransfer O, Performance Improvement O Cases where both learning rate transfer and performance improvement is observed. Examples: Zero query initialization, Multiplicative Nonlinearities, SP unembedding initialization, Multi-query attention, batch size(4x) Transfer O, Performance Improvement X Cases where learning rate transfer, but there was no improvement in performance. Examples: Projection biases, Embedding normalization, Cosine schedule Transfer X Cases where learning rate does not transfer. Examples: RMS Norm gain, Decoupled weight decay, SP attention scale Cases where learning rate does not transfer are not separately classified with performance improvement, as performance improvement in these cases is not significant.\nProjection Biases # Adding a bias vector to the linear layer does not guarantee an improvement in model performance. In fact, experimental results showed that the performance was similar to that of the baseline and learning rate transfer across the model size and width under µP.\nRMS Norm gain(vector \u0026amp; scalar) \u0026amp; Embedding normalization # RMSNorm is a normalization method that uses the root mean square instead of the mean and standard deviation. \\[\\bar{a_i}=\\frac{a_i}{\\mathrm{RMS(a)}}g_i, \\text{where } \\mathrm{RMS(a)}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^na_i^2}\\] To re-scale the standardized summed inputs, a trainable gain \\(g_i\\) and bias \\(b_i\\) were used. The gain can be implemented in two forms: a vector gain and a scalar multiplier. Similar to projection bias, the use of a trainable gain does not guarantee performance improvement. The results showed that transfer does not occur in any case, vector gain and scalar multiplier, and performance degradation was observed in the models with the largest width. On the other hand, using normalized embedding with RMSNorm without a trainable gain did not improve performance, but it was observed that the learning rate transfer was successful.\nQuery Initialization # For query projection, µP initialization typically uses a Gaussian distribution with variance \\(\\Theta(1/M)\\) , but zero-initialization has been proposed to facilitate transfer. Transfer occurs with zero-initialized query, and there was a slight improvement in performance compared to the baseline, traditional Gaussian initialization.\nCosine schedule # Adjusting the learning rate over iterations is an open problem with no definitive solution, with methods like power and exponential scheduling. This experiment use cosine scheduling, which is the method that periodically decreases and increases the learning rate to prevent convergence to local minima. This approach can help the model escape suboptimal points and potentially find better solutions.The baseline used a linear schedule, but switching to cosine scheduling did not negatively impact transfer. However, a slight performance degradation was observed with cosine scheduling compared to the baseline.\nDecoupled weight decay # When optimizing hyperparameters with Adam, the decoupled weight decay method separates the weight decay from the optimization step, allowing independent exploration of the learning rate and weight decay factor. Experimental results using decoupled weight decay showed that optimal learning rate transfer was not achieved. A large \\(\\lambda\\) value was suggested as a potential cause for this issue. In this experiment, \\(\\lambda = 0.1\\) was used. The smaller difference in optimal learning rates between small and large models, compared to other transfer failures, suggested that reducing \\(\\lambda\\) may help resolve the transfer problem.\nMultiplicative Nonlinearities # To enhance the performance of transformers, multiplicative nonlinearity activation functions such as SwiGLU and Squared ReLU can be utilized. SwiGLU is an activation function created by combining the Swish activation function with the Gated Linear Unit (GLU) and is considered to get better performance compared to traditional activation methods. . The formulas for each are as follows: \\(Swish(x) = x\\sigma (\\beta x)\\) ( \\(\\sigma\\) : sigmoid function, \\(\\beta\\) : trainable parameter ) \\(GLU(x,W,V,b,c) = \\sigma(xW\u0026#43;b)\\otimes (xV\u0026#43;c)\\) ( \\(W,V\\) : trainable tensor, \\(b,c\\) : trainable tensor bias, \\(\\otimes\\) : element-wise multiplication) \\(SwiGLU(x,W,V,b,c, \\beta) = Swish_\\beta(xW\u0026#43;b)\\otimes(xV\u0026#43;c)\\) Similarly, Squared ReLU, which is obtained by squaring the ReLU activation function, is known to help improve performance. Experimental results showed that they allow µ-transfer of the learning rate across model sizes and unlike the RMSNorm gain, there was a performance improvement from the perspective of multiplicative interaction.\nStandard Parameterization # Yang et al. (2021) stateed that µP models perform better than SP models, and our various experiments with SP settings confirmed that some of the µP settings offer advantages in terms of transfer and model performance.\nattention scale Usual attention scaling is \\(\\tau^{-1} = 1/\\sqrt{D}\\) , while µP proposes \\(\\tau^{-1} = \\Theta(1/D)\\) , and baseline experiment was implemented using a simple \\((1/D\\) ) scaling. In this experiment, attention scaling was \\(1/\\sqrt{D}\\) to check the SP setting. For the \\(M = 128\\) model, the optimal learning rate was \\(2^{-8}\\) , but for larger models, the optimal learning rate was changed to \\(2^{-6}\\) . This means that transfer did not occur, and performance slightly deteriorated compared to the original baseline. unembedding initialization The initialization of µP\u0026rsquo;s unembedding matrix follows a Gaussian distribution with a variance of \\(\\Theta(1/M^2)\\) , while standard parametrization (SP) uses \\(1/M\\) . Experiments using the original SP method with \\(1/M\\) showed that transfer was maintained and there was a slight improvement in performance for larger models. To compare the result of the SP and µP, this experiment was implemented using SP and compared the result with baseline's. The differences between the baseline and SP included using trainable biases in linear layers, trainable gains in RMSNorm layers, attention scale \\(1/\\sqrt{D}\\) , and unembedding initialization variance \\(1/M\\) . All other hyperparameters remained the same. The combined results for SP transformers showed that transfer does not occur, and the optimal loss is lower in performance compared to the baseline. Lion Optimizer # The Lion optimizer is known for being more than twice as memory-efficient as Adam while delivering similar performance in transformers. This optimizer restricts updates to \\(\\{-1, 1\\}\\) for each coordinate, yielding a coordinate size of \\(\\Theta(1)\\) per step. Consequently, it seems suitable to use the existing \\(\\Theta(1/M)\\) transfer rule as it is. However, experimental results showed that the learning rate transfer was not successful, indicating the need for further research on the transfer rule.\nMulti-query attention # Transformer LLMs can use multi-query attention and group generalization to increase inference speed by sharing keys/values across multiple heads. Experimental results showed that these methods lead to significant performance improvements compared to other methods, and transfer also occurred effectively.\nBatch Size(4x larger, 4x smaller) # By adjusting the batch size while keeping the number of training tokens constant, it is possible to reduce training time or determine the minimum batch size required for operation. In this case, the learning rate formula is adapted by using twice the specified value for 4x larger batch sizes and half the value for 4x smaller batch sizes. The results showed that learning rate transfer effectively in both cases, though further research is needed to determine the optimal batch size.\nLarge-scale Transfer Experiement # To verify if transfer is possible over a larger scale difference, experiments was implemented by reducing \\(L\\) to 12 and setting the width to \\(\\{128, 512, 2048, 8192\\}\\) , resulting in models with 2M, 40M, 600M, and 10B parameters(5000x). Zero query and Squared ReLU were used, which showed good performance and did not negatively impact transfer. The results confirmed that, despite a 5000x scale difference, the learning rate transfer well. Related Works # To summarize the contributions of the paper and consider the future works, this section summarizes the contents of a relevant research paper and explains its works and limitations.\nTensor Programs V: Tuning large neural networks via zero-shot hyperparameter transfer The performance of µ-Transfer, which reduces computation by tuning a small model and then transferring the obtained hyperparameters to a large model instead of tuning the large parameter model directly, was demonstrated experimentally. Figure : (1) MLP with SP (2) MLP with µP (3) transformer with SP (4) transformer with µP\nThe paper experimentally confirmed that when Standard Parameterization was applied to MLP and Transformer models, the hyperparameter stability was low. However, using µ Parameterization enabled stable transfer by using width-128 network and width-8192 network. The experiement sweeped the width and depth by changing only one of the hyperparameters—learning rate, output weight multiplier, initialization standard deviation, and learning rate schedule to check if they transfer across scale dimensions. Optimal hyperparameters transfered well across scale dimensions when the minimum width, depth, batch size, sequence length, and training steps were met. However, unlike the changes in width where hyperparameter transfer occured to wider models, the transfer to deeper models was less effective. The reason is same as the main paper's fixed depth and the experiments in this paper also adjusted the scale by fixing the depth and only varying the width. In this paper, experiments were conducted to evaluate the efficiency and performance of µ-Transfer using various models: from a 10M to 40M Transformer on IWSLT14 De-En, from a 15M to 211M Transformer on WMT14 En-De, from a 13M to 110M BERT and to 350M BERT-large. The largest model tested was the GPT-3, where the width is shrunk from 4096 to 256, resulting in a scale difference from 40M to 6.7B, representing a 168x scale difference. The total tuning cost was only 7% of total pretraining cost and the hyperparameter was stable across the scale. Contribution of the main paper : The performance of µ-Transfer, which has been proven effective with a scale difference of up to 168x, was experimentally confirmed to be reliable even with a scale difference of up to 5000x. The paper also conducted experiments not only comparing SP (Standard Parameterization) and µP (µ Parameterization) overall but also by changing specific elements such as attention scale.\nConclusion # The paper demonstrates that the transfer properties observed with µ-Parameterization (µP) can be maintained across most scenarios. µP outperforms standard parameterization (SP) and confirms the efficacy of part-by-part transfer for elements like attention scale and unembedding initialization, validating µP\u0026rsquo;s superiority. Additionally, it shows that transfer is feasible for models ranging from 2M to 10B parameters (5000x), suggesting applicability to larger models. However, some issues are identified where optimal learning rate transfer does not occur, or there is a performance decline in large models. For instance, trainable RMSNorm gain and decoupled weight decay do not function properly for learning rate transfer. Although transfer is observed with projection biases and cosine scheduling, there is no performance improvement or even a decline. The impressive performance of µ-Transfer demonstrated in this paper is expected to be highly beneficial in the current AI landscape, where model sizes are continually increasing. However, tuning hyperparameters for large models is not always feasible, indicating a need for further research. The suggested further research areas are as follows:\nDetailed explanation and analysis of each experiment, investigating the causes of transfer failures and potential solutions through the expansion of each experiment (e.g., experiments on specific SP elements, experiments with various batch sizes). Scaling adjustment through depth rather than width and exploring its transferability. Transferability of other hyperparameters. Personal Insights and Future Directions # The significance of this paper lies in summarizing the impact of various methods on performance and transfer under µ through ablation experiments. However, it is limited by focusing on many ablations without additional experiments for detailed results, leading to a lack of comprehensive analysis. For instance, where transfer fails, minor differences in optimal learning rates between small and large models suggest potential for improvement, but no further experiments were conducted. Structural issues in transformers make scaling adjustments through depth challenging, and research on architectures that allow adjustments through both depth and width could increase flexibility. Additionally, expanding the scope of transfer to include other hyperparameters like the output weight multiplier, initialization standard deviation, and learning rate schedule is necessary.\nWe also believe µ-Parameterization offers a promising solution to one of the significant bottlenecks in training large neural networks. Its potential to standardize and simplify the hyperparameter tuning process is a substantial step forward. However, the complexity of its implementation and the need for more empirical validation remain hurdles to its widespread adoption.\nTo fully realize the benefits of µP, the following future research directions are proposed:\nEmpirical Validation: Conduct large-scale studies across various types of neural networks and applications to validate µP’s effectiveness and identify any limitations. Tool Development: Create user-friendly tools and libraries that automate the µP scaling process, making it more accessible to practitioners. Hybrid Approaches: Explore combining µP with other optimization methods like LAMB to further enhance training efficiency and performance. By addressing these future research directions, the full potential of µ-Parameterization can be unlocked, paving the way for more efficient and effective neural network training methodologies.\nReferences # Lingle, L. D. (2024). A Large-Scale Exploration of µ-Transfer. arXiv preprint. Retrieved from arXiv:2404.05728. Yang, G., \u0026amp; Hu, E. J. (2021). Tensor Programs V: Tuning large neural networks via zero-shot hyperparameter transfer. Advances in Neural Information Processing Systems. Biao Zhang and Rico Sennrich. Root mean square layer normalization. CoRR, abs/1910.07467, 2019. URL http://arxiv.org/abs/1910.07467. Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. CoRR, abs/1711.05101, 2017. URL http://arxiv.org/abs/1711.05101. Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/abs/2002.05202. Noam Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019. URL https://arxiv.org/abs/1911.02150. "},{"id":14,"href":"/docs/spring24/14_/","title":"14","section":"Spring24","content":" BinaryDM: Towards Accurate Binarization of Diffusion Model # Authors: Xingyu Zheng, Haotaong Qin, Xudong Ma, Mingyuan Zhang, Haojie Hao, Jiakai Wang, Zixiang Zhao, Jinyang Guo, XianglongLiu Posted by Junhyuk So, Juncheol Shin\nPreliminary # Diffusion # Diffusion models learn how to remove gaussian noise added to original image. Equation below shows how forward process proceeds. In the forward process, Gaussian noise is gradually added to original image for T times. Strength of the noise is controlled by the term \\beta x_t denotes corrupted image at time step t. In the reverse process, diffusion model tries to restore original image by estimating conditional distribution (). Reparameterization trick is used to estimate mean and variance of gaussian the distribution Quantization # Quantization is an optimization technique which restricts data(weights, activations) in low precision. Not only does it reduce the memory footprint, but it also enables accelerated computations given hardware support to low-precision arithmetic. Values are quantized and represented as integers as follows:\nBinarization is extreme case of quantization, which only utilizes 1 bit.\nMotivation # While diffusion models achieved great success in generation tasks, its iterative nature act as a bottleneck to real-world application. Data must processed through heavy diffusion models for multiple steps and requires huge latency and memory footprint.\nQuantization is one reasonable choice for the optimization of diffusion models. Especially when binarization is applied to weight, floating point operations can be substituted with cheap addition and memory footprint of the model can be reduced greatly.\nHowever binary models are hard to binarize in two aspects. One arises from perspective of representation, as binarization is extreme case of quantization which only uses 1bit to represent data. Naive binarization introduces severe degredation in quality of output. Another aspect arises from perspective of optimization. Training becomes unstable with binarized representation and hinders convergence of the model.\nThis work tackles binarization of diffusion models by handling aformentioned two aspects. By introducing Learnable Multi-basis Binarizer(LMB) and Low-rank representation mimicking(LRM), BinaryDM is able to achieve 16.0× and 27.1× reductions on FLOPs and size.\nMethodology # Learnable Multi-basis binarizer # Typical binarization of the weight can be described as follows:\nIn this work, authors propose a learnable multi-basis binarizer(LMB) to maintain quality of representations. Instead of using single base, multiple bases are utilized for the binarization.\nGradient of learnable scalar values can be computed as follows:\nDuring the inference, computation for each bases are indepedent to each other and can be parallely computed. Thus, diffusion model can be fully accelerated with LMB.\nIt is important to note that LMB is applied at only crucial parts of difusion model. Only modules where features cale is greater than or equal to 1/2 input scale. In other words, some of the first consecutive layers and last consecutive layers are binarized with LMB. The binarized modules close to input or output play important role, as they extract patterns from original data or directly influence the final result. Figure below shows result of naive binarization and LMB applied to weights.\nLow-rank representation mimicking # Binarization of weights makes the training hard and hiders convergence. Since full precision model is available, it is natural to align intermediate representations of binarized diffusion model and original model as additional supervision. However, fine-grained alignment of high-dimensional representation leads to blurry optimization direction and binarization makes the model hard to mimic full precision model.\nAuthors propose Low-rank Representation Mimicking(LRM) to handle these problems. LRM utilize principal component analysis(PCA) to project representations to low-rank space. Then representaiton aligning is done in low-rank space by minimizing mean squared error (MSE)\nFirst, covariance matrix of ith module C_i is computed with representation of full-precision diffusion model. Then eigenvector matrix E_i can be obtained and first c/k column eighenvectors are used to projeet representations.\nLRM loss and total loss can be expressed as follows:\nSince computation of transformation matrix E_i is expensive, it is computed with the first batch of input and fixed during entire traning. As shown in the figure below, LRM stabilize training process, accelerating convergence.\nProgressive binarization # Despite the enhanced methodology, training process remains slow and unstable. Authors additionally apply progressive binarization strategy to further stabilize convergence. M/2 th time stepping module is quantized in first iteration and m/2-i-th and m/2+i-th modules are quantized in next i-th iteration. As show in the figure, benefit coming from progressive binarization is significant compared to baseline traning process.\n"},{"id":15,"href":"/docs/spring24/15_/","title":"15","section":"Spring24","content":" # "},{"id":16,"href":"/docs/spring24/16_/","title":"16","section":"Spring24","content":" Accelerating Transformers via Conditional Computation: As Aspect of Mixture of Depths # Posted by: Inkwan Hwang, Minjae Park\nThis image was generated by DALL·E 3. Introduction # “Choice and concentration” is an effective strategies for achieving success in problems. Sometimes, it is not necessary to consume same amount of effort and time into all problems. Expending energy on trivial issues may fail to concentrate on what truly matters. Similarly, in language models, there is a technique that does not focus equally on all tokens but allocates less budget to non-essential tokens. This technique is called conditional computation.\nIn this post, We will explain conditional computation strategies for Transformers, focusing on a technology announced this year called Mixture-of-Texture.\npaper: Mixture-of-Depths: Dynamically allocating compute in transformer-based language models Let\u0026rsquo;s dive in!\nUnderstanding the problem: Uniform computation in Transformers # These days, most language models are based on Transformers, and we stack these blocks to make big models. When given an input sequence, tokens pass through these blocks to predict the next token. The problem is that the models spread computations uniformly across input sequences. Transformers use the same amount of computation for essential tokens as for non-essential ones. For instance, predicting a token within a sentence is cheaper than predicting the first token of the next sentence. Researchers want to address this issue by making Transformers focus on important tokens by allocating less computing resources.\nConditional computation for Transformers # Early exiting\nInstead of passing through all layers, the model can stop early if it is confident enough about its prediction. This saves computation time and resources. Large pre-trained models like BERT can use early exiting ot maintain performance while reducing computational load. CoLT5\nCOLT5’s architecture is almost same as MoD’s with difference is that using Light Attention \u0026 Light MLP in COLT5 than residual path in MoD. Light Attention refers to a local attention layer which just calculates attention value between just few nearby tokens. Heavy Attention refers to a global attention layer which calculates some chosen token(chosen by router) calculates attention values with all input tokens. Same as MoD, router mechanism is top-k routing mechanism which performs well (will be discuss in later section). MoD’s policy is to reduce Router into single one and avoid attention \u0026 MLP calculation. The figure above is the attention map in COLT5. Light colored ones are for light attention(local attention) and bold ones are for heavy attention. In COLT5, they choose 1/16 of query tokens and 1/8 of key value tokens for heavy attention calculation. Mixture of Experts (MoE)\nMoE is an model which consists of parallel expert models which is fitted to certain domains. Like MoD, token-level routing decisions are made across the network depth. Difference between MoD is, MoD chooses path to transformer or to residual connection, MoE chooses path to transformer(Expert) or to transformer(Expert) or both. Overview to Mixture-of-Depths (MoD) # Our goal is to reduce the overall FLOPs by focusing on essential tokens and relatively fewer on non-essential tokens. The router is responsible for determining the path each token should take. A trained router evaluates whether a token is necessary. If the token is deemed essential, it passes through self-attention and the subsequent MLP (requiring FLOPs). Otherwise, it bypasses these stages via a residual connection (saving FLOPs).\nAbove image depicts the path of a MoD (Model of Decoding) with an input sequence length of 64. The purple color shows the computation performed by that layer and the orange color shows the path taken by the residual connection. (삭제예정) MoE is an model which consists of parallel expert models which is fitted to certain domains. Like MoD, token-level routing decisions are made across the network depth. Difference between MoD is, MoD chooses path to transformer or to residual connection, MoE chooses path to transformer(Expert) or to transformer(Expert) or both.\nRouting schemes # Routing implementation is the most crucial part of MoD. The authors compare three routing schemes, demonstrating that MoD is an efficient approach.\nToken-choice routing # Token-choice routing is a method where each tokens select the path it will follow. The router produces probability distributions for each token across the computational paths. Based on this distribution, each token chooses its preferred path at each layer.\nIn token-choice routing, tokens have the flexibility to select their path, allowing for dynamic processing. However, this can lead to path balancing issues as all tokens might preger on the same path. It causes potential overloads on specific paths. To mitigate it, auxility loss is used to ensure that most tokens do not prefer on a single path.\nExpert-choice routing # Expert-choice routing is the reverse of token-choice routing. Similar to token-choice routing, the router produces a probability distribution for each token. In expert-choice routing, instead of tokens selecting their paths, each path selects the top- \\(k\\) tokwns based on the tokens\u0026rsquo; preferences.\nUsing this method ensures that each paths receives k tokens, maintauing balance among the paths. However, some tokens may not be selected beacuse there might be common tokens that multiple paths prefer.\nExpert-choice MoD # This method applies expert-choice routing but uses only a single expert. Since only a single path is utilized, if \\(k\\) is less than the sequence length, not all tokens need to undergo self-attention and MLP computation.\nFor the following reasons, the authors decided to use Expert-choice routing and utilize only single paths:\nEfficiency of computation\nDon\u0026rsquo;t need for an auxiliary balancing loss\nSimplicity of implementation\nSimply can choose the tokens with the highest weight in order\nClear criteria\nCan guarantee that the most important token is calculated since the top- \\(k\\) tokens are independent on magnitude of router weights\nTop- \\(k\\) can divide clearly tokens into two mutually sets\nImplementation # MoD Transformers는 다음과 같은 방식으로 작동한다.\nCalculate routing weights \\[x^{l\u0026#43;1}_i=\\begin{cases}r^{l}_i f_i(\\tilde{X}^l)\u0026#43;x^{l}_i, \u0026amp; \\text{if } r^{l}_i \u0026gt; P_\\beta(R^l)\\\\x^{l}_i, \u0026amp; \\text{if }r^{l}_i \u0026lt; P_\\beta(R^l)\\end{cases}\\] ssss backward path 및 오차보정 More details # Capacity # In this paper, capacity-based routing is employed. Token capacity is the total proportion of tokens composing the input for a given operation. For instance, if the input sequence length is 100 and the capacity is 20%, each layer operates on the top-20 tokens determined by router weights.\nBy lowering the capacity of the computations, a smaller compute budget can be utilized per forward pass compared to the vanila Transformers. In MoD, capacity is utilized as a hyperparameter to determine the proportion of tokens processed per layer operation.\nAutoregressively sampling # We\u0026rsquo;re looking to implement expert-choice routing, but there is one distinct problem: top-k operations rely on future tokens! Our goal is for each token to determine if it belongs to the top-k using routers. To do this, every token needs access to the router weights of future tokens. Unfortunately, we lack the ability to predict the future router weights and cannot employ autoregressive sampling. To solve this problem, the authors propose two methods.\nSimple auxiliary loss\nDesigning an additional binary cross-entropy loss function at the router's output can resolve this issue. By incorporating this, the value of tokens in the top-k is guided to be greater than 0.5, while the value of tokens are not in the top-k is guided to be less than 0.5. As token passes through the router, they are categorized into top-k set if their value exceeds 0.5. Then it passes through the self-attention and subsequent MLP. Conversely, tokens with values below 0.5 passs through the residual connection. Integrating such a function impacts the primary language modeling objective approximately 0.2-0.3%. We believe this likely refers to the extent to which performance and inference time are affected. Small auxiliary MLP predictor\nThe second method does not affect the primary language modeling objective at all. The authors design a new MLP layer that functions as a binary classifier to determine wheather a token is in top-k during the training process. This classifer is trained to make these demterminations, and it is used in real-time during the autoregressive sampling process.\nWith these methods, authors could sample autoregressively by choosing to route tokens to or around a block based on the router\u0026rsquo;s outer which is not depends on the future tokens. They provide empirical result that auxiliary task achieved 99% accuracy.\nOpen source MoD (not official) # The followuing is an implementation of MoD that supports various LM such as Mixtral, LLama3 and BLOOM. It implements MoD using PyTorch and Hugging Face Transformers library.\nLINK: https://github.com/astramind-ai/Mixture-of-depths\nCode Details The code operates in the following steps:\nToken Weight Calculation\n\u0026lsquo;\u0026lsquo;\u0026lsquo;class TokenRouter(nn.Module): def init(self, embed_dim): super().init() self.weight_predictor = nn.Linear(embed_dim, 1)\ndef forward(self, x): weights = self.weight_predictor(x).squeeze(-1) # [batch_size, seq_len] return weights \u0026rsquo;\u0026rsquo;\u0026rsquo;\nThe TokenRouter module caculates weights for each token based on its embedding. This is done using a lnear layer appleid to the embeddingsm resulting in a weight value for each token.\nSelective Processing\nThe processing occurs in the MoD module\u0026rsquo;s forward pass\nFirst token weights are calculated using TokenRouter By a capacity paratmter, the number of tokens are determined. They undergo self-attention and MLP computation. Application to Hugging Face Models\napply_mod_to_hf function applies the MoD mechanism to an existing Hugging Face model.\nResults # The figure above shows the results of training all models with the same number of FLOPs(6e18), regardless of the parameter size. The compared models are the Baseline (isoFLOP optimal baseline, vanilla transformer) and models with MoD applied, set to have either 12.5% capacity or 50% capacity. In the case of random routing, it does not follow the top-k metric but simply randomly chooses whether a token will go to the residual path or the attention + MLP layer path. Additionally, in the case of every 2, it means that the MoD method is not applied to all layers, but only to one out of every two layers. Therefore, from the top-left graph, the 12.5% capacity MoE loss value is less than the baseline model's. The two top-middle graphs show the actual training loss graphs for the points plotted in the left graph, where MoD with 12.5% capacity generally results in lower loss values than baseline. In the case of the right graph, the plotted points #1 \u0026 #3 and #2 \u0026 #4 pairs are models of the same parameter size, with MoD applied, it not only has a loss value lower, but it also has an approximately 66% faster performance than original one. In this figure, the Training FLOPs budget is limited to 6e18, 2e19, and 1e20, compared with the isoFLOP baseline and 12.5% capacity MoD. At a glance, the top-left graph shows that the isoFLOP baseline has a slightly better loss when the number of parameters is small(There’s a crossing point!) However, when the x-axis is converted from Parameters to FLOPs per FFW (Forward Pass) as shown in the top-right graph, it confirms that MoD is better than the baseline in all cases. In this figure, the top-left graph shows the performance degradation in auto-regressive evaluation using predictions using the MLP layer not the top-k routing mechanism, which is non-causal and can’t be used in this case. The author argues that the reason for this performance drop in auto-regressive case is that the prediction performance through the MLP layer is only about 97%, as shown in the top-right graph, but they stress that only minimal degradation occurs. This figure shows the performance of MoDE and its two proposed structures. The top-left graph demonstrates that the performance of MoDE is better than both the Baseline and MoE. The right side explains the structures of Staged MoDE and Integrated MoDE. In Staged MoDE, two routers are deployed to first for determine the depth(MoD) and second for the expert(MoE). In Integrated MoDE, as the name implies, the MoD Router and MoE Router are integrated to one single Router that can simultaneously decide whether to select an expert or the residual path (depth). The paper mentions that the former is computationally efficient as it can skip self-attention operations through the MoD router, and the latter has better performance as the router mechanism is unified and self-attention operations are always performed. Conclusion and discussion # 결론 + 내 생각\nReferences # Arian et.al.,\u0026quot; Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead ,\u0026quot; arXiv, 2021\nJoshua et.al.,\u0026quot; COLT5: Faster Long-Range Transformers with Conditional Computation ,\u0026quot; EMNLP, 2023\nNoam et.al.,\u0026quot; OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER ,\u0026quot; ICLR, 2017\n"},{"id":17,"href":"/docs/spring24/17_/","title":"17","section":"Spring24","content":" QuaRot : Outlier-Free 4-Bit Inference in Rotated LLMs # Author : Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li\nPosted by MyeongJi Yun, JungGyu Min, POSTECH\nThis post assumes that the reader has a structural understanding of Transformer and Llama models. If you need a detailed understanding of these models, please refer to the Transformer, LLaMa.\nLarge Language models ( LLMs ) like GPT-2, LLaMa have become increasingly important due to their countless applications. However, their inference requires a significant amount of computation, memory, and energy. Quantization is among the most important techniques to solve both memory and compute issues in LLM inference.\nOutlier makes quantization difficult # Recent research has shown that LLMs have large outliers and make quantization more difficult, especially in 4-bit case. Also, they mentioned that the activations have more outliers, which makes quantization harder. There are three main streams to solve this problem.\nWeight only quantization LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale, 2022 NeurIPs Weight quantization can ease the memory budget for saving the model. However, since activations are not quantized, the computation still involves integer and float operations, making it difficult to address compute issues. Remain outlier in higher bitwidth QUIK: Towards End-to-End 4-Bit Inference on Generative Large Language Models, 2023 Weight quantization can ease the memory budget for saving the model, and since most operations are integer X integer, compute issues are largely resolved. However, some operations still involve integer X float, and the occurrence of float values is irregular, leaving some compute issues unresolved. Use calibration set and normalize activation SmoothQuant: Accurate and Efficient Post-Training Quantization for LLM, 2023 ICML Accuracy is guaranteed up to 8-bit quantization, but it is not assured with 4-bit quantization. In “ QuaRot : Outlier-Free 4-Bit Inference in Rotated LLMs”, the author introduces a new method for quantizing LLM models end-to-end, by utilizing “computational invariance” to all weights and activation and optimizing the computing process.\nRandom Hadamard transform doesn’t change the result # In the concept of computational invariance theorem, small changes in input parameters do not cause the output difference if the algorithm is stable. When applying this to a transformer-based large language model (LLM), it implies that rotating the coordinate system of activations between weight and computation blocks using an orthogonal matrix does not alter the model\u0026rsquo;s output. According to this theory, instead of using any matrix X that constitutes the transformer, you can use X′=UXV where U and V are orthogonal matrices, and the computational results will remain unchanged.\nIf the number or proportion of outliers in 𝑋′ is less than that in 𝑋, the information loss during quantization can be reduced. QuIP demonstrates that multiplying a matrix by orthogonal matrices on both sides reduces the value of max⁡(𝑋)/mean(𝑋). This means that the presence of extreme values relative to the average is diminished, leading to a more uniform distribution of values within the matrix. However, performing 𝑈𝑋𝑉 also incurs overhead, so selecting orthogonal matrices 𝑈 and 𝑉 that minimize this overhead is essential.\nQuaRot uses Random Hadamard transformation because the result PPL is lower, so random Hadamard transformation is better than random matrix.\nLLama2-7B LLama2-7B LLama2-7B QuaRot ( Random ) 7.45 5.84 4.07 QuaRot (Hadamard) 6.10 5.40 3.79 Random Hadamard transformation matrix H is described below : \\[ H_{2} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{bmatrix}, \\quad H_{2^n} = H_2 \\otimes H_{2^{n-1}} \\] \\[H\u0026#39; = H \\cdot \\mathrm{diag}(s), \\quad s \\sim \\mathrm{Uniform}(\\{-1, \u0026#43;1\\})\\] This transformation pairs elements to perform simultaneous computations, allowing the matrix-vector multiplication between matrix 𝐻 and vector 𝑥 to be executed using only 𝑂(𝑑log⁡𝑑) addition operations without any multiplications, as illustrated below:\n--- QuaRot demonstrates that using this technique reduces the number of outliers. By applying the random Hadamard transformation, the distribution of activations is more uniform, which decreases the number of extreme values or outliers, thereby minimizing information loss during quantization.\nStep by Step modification and quantization # Step 1 involves applying the new schemes proposed by QuaRot to significantly reduce outliers in weights and activations, thereby minimizing accuracy degradation due to quantization held in Step 2. The key technique is to apply the Hadamard transform to each activation and weight in both attention blocks and FFN. This is done by merging operations through the use of two different Hadamard transform matrices across consecutive layers, creating an optimal computational flow.\nStep 1-a. Weight Modification # Note that the multiplication of two orthogonal matrices generates identical matrix, so inserting Q and Q^T between linear layers doesn’t change any output.\n\\[I = Q Q^T, XQQ^TW = XW\\] Considering LayerNorm or RMSNorm at the start of the transformer multiplying some orthogonal matrices does not change output. Also, we can fuse the scaling operation of RMSNorm’s : diag(a) into an adjacent weight matrix.\n\\[RMSNorm(X) = x_i \\leftarrow \\frac{x_i}{||x_i||} = ( \\frac{x_i * Q}{||x_i||} ) Q^T = RMSNorm (XQ^T)Q\\] So for all weights after the RMSNorm layer, the weight becomes :\n\\[W \\leftarrow Q^T diag(a) W, Q = Hdiag(s)\\] Step 1-b. Rotate FFN # Inserting online Hadamard operation can ease the activation value’s quantization difficulty within each block. This operation is implicitly reserved by fusing a Hadamard matrix into the next matrix of the network.\nStep 1-c. Attention Value Projection # This step applies Hadamard transformations to the value and output projection matrices in the attention block throughout both offline weight modification and online activation transformation. Since value and output projection weight are multiplied in each head, two matrices can be transformed using the Hadamard matrix without changing the result of attention.\n\\[W_v^{(h)} \\leftarrow W_v^{(h)}H_{d_h} \\\\ W_{out}^{(h)} \\leftarrow H_{d_h} W_{out}^{(h)} \\] This transformation can be represented with Kronecker multiplication in the point of full attention computation view.\n\\[W_v \\leftarrow W_v(I\\otimes H_{d_h})\\\\W_{out}\\leftarrow (I\\otimes H_{d_h}) W_{out} \\] The following simple lemma defines the remaining Hadamard operation after modification.\n\\[H_{a\\times b}= (I\\otimes H_{b}) (H_{a}\\otimes I )\\] This defines the remaining Hadamard operation as the later term of the upper lemma, which results in a modification of the online forward path.\n\\[Z \\leftarrow Z(H_{n_h} \\otimes I)\\] Step 1-d. Key Rotation # This step applies Hadamard transformation to the key vectors in the attention module. Utilizing the RoPE method (Su et al., 2021), the positional encoding is directly attended to query and key vectors. This reshapes the attention score computation equation into a modification-convenient form.\n\\[\\text{Score}=\\text{Softmax}(\\alpha \\text{Pos}(Q_h) \\text{Pos}(K_h^T)\\odot M)\\] The Hadamard transformation is applied to both position encoded query and key vectors similar to step 1-c.\n\\[\\text{Pos}(Q) = \\text{Pos}(XW_q) \\leftarrow \\text{Pos}(XW_q)(I\\otimes H_{d_h})\\\\\\text{Pos}(K) = \\text{Pos}(XW_k) \\leftarrow \\text{Pos}(XW_k)(I\\otimes H_{d_h}) \\] Note that this transformation can be applied without changing final attention scores since both queries and keys are rotated, therefore no remaining Hadamard transformation exists.\nStep 2 involves applying various state-of-the-art techniques to quantize weights and activations.\nStep 2-a. Weight Quantization # You can quantize the adjusted weights using GPTQ, or you can use a very simple round-to-nearest (RTN) technique. The paper have shown simpler method(RTN) have shown a slight sacrifice in accuracy.\nStep 2-b. Online Quantization # To quantize the activations, find the scale factor for each row (max(row) / 7), then divide all values by the scale factor and convert them to the nearest 4-bit integer. For dequantization, multiply the 32-bit integer output of GEMM by the scale factors of both the activation and the weight, and convert the result to FP16.\nStep 2-c. Quantized Attention # The significance of storing in 4-bit is greater than performing calculations in 4-bit because attention operations are memory-bound. Thus, to compute attention, keep the query, key, and value in FP16 and use Flash Attention for the softmax computation.\nQuaRot saves runtime \u0026amp; memory # As highlighted in the contributions of the paper, this model demonstrates that it maintains accuracy even with 4-bit quantization, achieving the same level of accuracy as other models with significant computation overhead. Additionally, this paper presents results across various model sizes(7B to 70B) and different tasks(PIQA(PQ), ARC-e(A-e), ARc-c(A-c), HellaSwag(HS), Winogrande(WG), LAMBADA(LA) ), demonstrating that as the model size increases, the quantization error compared to FP16 decreases for all tasks. Regarding the Llama-1-7B model\u0026rsquo;s 4-bit quantization situation, which exhibits the largest difference from the FP16 model, we compared it with other recent papers not mentioned in the original study. It is evident that QuaRot, which has lower computational cost, outperforms the generally best-performing QAT and OmniQuant, which involves some additional training on top of SmoothQuant, in 4-bit quantization. Despite this low cost, QuaRot has the smallest inference accuracy difference from the FP16 model, making it a highly effective quantization technique. Moreover, while the original SmoothQuant may have lower computational cost at the same bandwidth due to its simplicity, as shown in the table below, its inference accuracy in 4-bit quantization is so poor that it necessitates the use of 8-bit, making comparisons with QuaRot unnecessary. The key point of QuaRot is that the process of performing the Hadamard transform for quantization to INT4 should not introduce a large overhead compared to the computational benefits gained from converting to INT4. From the perspective of the runtime of the FFN block, it has been confirmed that the overhead remains minimal regardless of layer size, model size, or batch size. Additionally, the memory saving factor ranges from x3.48 to x3.71, which is very close to the ideal value (4 = FP16 / INT4), demonstrating significant efficiency. This paper is particularly noteworthy for addressing the issue of memory overhead in long sequence scenarios by quantizing the KV cache as well. Discussion and future work direction # Why we limited to symmetric INT4 qunatization?\nNumerous papers discuss the limitations of using symmetric quantization in INT4 format for quantization. For example, ANT demonstrate that, even with the same bitwidth, numeric formats like flint and PoT(power of Two), which divide the representation into exponent and mantissa, can achieve better accuracy due to their ability to represent a wider range of values. In the figure below, the INT-4bit example uses only integers, while the others utilize new data formats. It is evident that the Mean Squared Error (MSE) significantly decreases with these new formats.\nQuaRot considers INT4 format for both weight quantization and activation quantization, likely because modern GPUs support efficient operations with INT4 and INT8 formats. If we could use other formats, it might be possible to maintain accuracy even with formats as small as 3-bit, leading to greater memory savings. However, maintaining computational simplicity is challenging because GPUs are not optimized for operations with custom data types, unlike INT4. Therefore, achieving optimal computation with custom data types would require the development of custom hardware.\nToward Quantization + Pruning\nThere are two paper about low-cost LLMs GPTQ and OBS. GPTQ focuses on reconstructing matrices after quantization, while OBS deals with reconstructing models after pruning. Both papers share a common foundation in using the Hessian matrix and employ various optimization techniques such as Wood-Fisher. Combining these two approaches, the OBC study explores methods to preserve the accuracy of networks that undergo both pruning and quantization. SliceGPT similarly achieves effective pruning by employing the concept of computational invariance when multiplying orthogonal matrices. By analyzing the properties of orthogonal matrices in both QuaRot and SliceGPT, I believe it is possible to achieve quantization and pruning simultaneously. Nonlinear layer Quantization\nThis paper discusses performing quantization in an end-to-end manner. However, it lacks detailed explanations regarding operations in layers known to require higher bitwidth, such as the input to the softmax function, gelu, and residual operations in layer normalization. Therefore, future research could potentially extend this approach to include all these operations using only low-bitwidth integer calculations. How to reduce the overhead of online Hadamard transformation\nThe forward path in QuaRot mostly follows the activation-quantized LLM tasks like GPTQ, yet requires the additional task of online Hadamard transformation on attention activation. The online Hadamard transformation can be performed by utilizing existing computational resources by converting the task into a matrix-multiplication form, or tossing the task to a dedicated hardware accelerator. Either way have an optimization point of acceleration, where data scheduling of the Hadamard transformation matrix into GEMM task accelerator, or utilizing various previous works about hardware accelerator Hadamard transformation with dedicated dataflow. "},{"id":18,"href":"/docs/spring24/18_/","title":"18","section":"Spring24","content":" ViTAR: Vision Transformer with Any Resolution # Authors: Qihang Fan, Quanzeng You, Xiaotian Han, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang\nPosted by Jungwon Lee, Minsang Seok\nWhat is Vision Transformer? # Vision Transformer (ViT) is an innovative approach to computer vision that leverages the principles of the Transformer architecture, which was originally designed for natural language processing tasks. ViT has recently emerged as a competitive alternative to Convolutional Neural Networks (CNNs) that are currently state-of-the-art in different image recognition computer vision tasks.\nVision Transformer architecture consists of a series of Transformer blocks, each containing a multi-head self-attention layer and a feed-forward layer. This structure allows ViT to capture complex relationships within an image more effectively than traditional convolutional layers.\nKey Components of ViT # The key coomponents of ViT are described below:\nA. Patch Embedding # Instead of processing the entire image as a whole, ViT divides the input image into fixed-size patches (e.g., 16x16 pixels). Each patch is then flattened into a single vector, essentially treating each patch as a \u0026ldquo;token\u0026rdquo; similar to how words are treated in text processing. These flattened patch vectors are linearly projected to a desired embedding dimension. This projection helps in transforming the patches into a suitable format for the Transformer model. B. Positional Encoding # Since Transformers are permutation-invariant and do not inherently understand the spatial relationships between patches, positional encodings are added to the patch embeddings. These encodings provide information about the position of each patch in the original image. C. Self Attention # The self-attention layer calculates attention weights for each pixel in the image based on its relationship with all other pixels. $$ Q = XW_Q, K=XW_K, V=XW_V $$\n$$ Attention Score = Q K^T $$\n$$ Attention Output = softmax(\\frac{Q \\dot K^T}{\\sqrt{d_k}}V) $$\nC. Multi-Head Self Attention (MHSA) # The multi-head attention extends self-attention mechanism by allowing the model to attend to different parts of the input sequence simultaneously. Each \u0026ldquo;head\u0026rdquo; in the multi-head attention mechanism can capture different features, leading to a richer and more nuanced representation of the image. D. Feedforward Neural Networks: # Each self-attention layer is followed by a feedforward neural network that further processes the information. These networks consist of fully connected layers and typically include activation functions and normalization. Challenge: Multi-Resolution ViT Modeling # Shortcoming of ViT is revealed when receiving multi-resolution images as input. There are limits to its application in actual use environments because ViT cannot process images of various resolutions well.\nThe most common method used to address this problem is to apply interpolation to positional encoding before feeding it into the ViT. This approach allows for some compensation of positional information even when the input resolution changes. However, this method has shown significant performance degradation in image classification tasks.\nRecently, ResFormer proposed adding depth-wise convolution to the existing positional encoding method when performing global-local positional embedding, enabling it to work well even with unseen resolutions. (Chu et al., 2023; Tian et al., 2023).\nHowever, ResFormer has three drawbacks.\nShows high performance only in a relatively small range of resolutions (Degradation significantly when resolution is greater than 892) It cannot be used with self-supervised learning methods like masked auto-encoding (MAE). Computation cost increases as input resolution increases, which has a negative impact on the training and inference process. ViTAR: Vision Transformer with Any Resolution # In this section, we introduces two key innovations to address this issue. Firstly, we propose a novel module for dynamic resolution adjustment, designed with a single Transformer block, specifically to achieve highly efficient incremental token integration. Secondly, we introduce fuzzy positional encoding in the Vision Transformer to provide consistent positional awareness across multiple resolutions, thereby preventing overfitting to any single training resolution. 1. Adaptive Token Merger (ATM Module) # Adaptive Token Merger (ATM) module is designed to efficiently process and merge tokens of different resolutions in a neural network using a simple structure that includes GridAttention and FeedForward network (FFN). ATM Module takes tokens $(H\\times W)$ processed through patch embedding as input. ATM Module specially processes the inputs of different resolutions M times to reduce them to the same preset size $G_{h} \\times G_{w}$ before fed into the MHSA.\nThe detailed process for ATM is as follows: First, ATM divides the tokens of shape $H \\times W $ into a grid of size $G_{th} \\times G_{tw}$.\nFor simplicity, we\u0026rsquo;ll use above Figure as an example. In the figure, we can see $H=4$, $W=4$, $G_{th}=2$, and $G_{tw}=2$.(We assume that H is divisible by $G_{th}$ and W is divisible by $G_{tw})$. The number of tokens in each grid would then be $H/G_{th} × W/G_{tw}$, which is 2x2.\nWithin each grid, the module performs a special operation called Grid Attention.\nGridAttention # For a specific grid, we suppose its tokens are denoted as ${x_{ij}}$, where $0 ≤ i \u0026lt; H/G_{th}$ and $0 ≤ j \u0026lt; W/G_{tw}$.\nAverage Pooling: First, it averages the tokens within a grid to create a mean token. Cross-Attention: Using this mean token as the Query, and all the grid tokens as Key and Value, it applies cross-attention to merge all tokens in the grid into a single token.\n\\[x_{avg} = AvgPool(\\{x_{ij}\\}) \\\\ GridAttn(\\{x_{ij}\\}) = x_{avg} \u0026#43; Attn(x_{avg}, \\{x_{ij}\\}, \\{x_{ij}\\})\\] After passing through GridAttention, the fused token is fed into a standard Feed-Forward Network to complete channel fusion, thereby completing one iteration of merging token. GridAttention and FFN undergo multiple iterations and all iterations share the same weights.\nDuring these iterations, we gradually decrease the value of $(G_{th} , G_{tw})$, until $G_{th} = G_{h}$ and $G_{tw} = G_{w}$. (typically set $Gh = Gw = 14$, in standard ViT)\nThis iteration process effectively reduces the number of tokens even when the resolution of the image is large, and with enough iterations, this size can be reduced effectively. This has the advantage of being computationally efficient because when performing subsequent MHSA calculations, we always use the same size tokens as input, regardless of resolution.\nFor Ablation study, ViTAR-S Model is used to compare with AvgPool which is another token fusion method. The results of the comparison demonstrate that ATM significantly improves the model\u0026rsquo;s performance and resolution adaptability. Specifically, at a resolution of 4032, our proposed ATM achieves a 7.6% increase in accuracy compared with the baseline.\n2. Fuzzy Positional Encoding (FPE) # Existing ViT Models generally use learnable positional encoding or sin-cos positional encoding. However, they do not have the ability to handle various input resolutions because these methods are sensitive to input resolution. In response to this, ResFormer attempted to solve this problem through convolution-based positional embedding.\nHowever, convolution-based positional embedding is not suitable for use in self-supervised learning such as masked auto-encoding (MAE). This is because the method can extract and utilize the complete spatial feature only if it has all adjacent patches, but in the case of MAE, some of the image patches are masked. This makes it difficult for the model to conduct large-scale learning.\nFuzzy Positional Encoding(FPE) differs from the previously mentioned methods. It enhances the model\u0026rsquo;s resolution robustness without introducing specific spatial structures like convolutions. Therefore, it can be applied to self-supervised learning frameworks. This property enables ViTAR to be applied to large-scale, unlabeled training sets for training, aiming to obtain a more powerful vision foundation model.\nInitially, the learnable positional embedding is randomly initialized and used as the model\u0026rsquo;s positional embedding. At this time, FPE provides only fuzzy positional information and experiences changes within a certain range. Specifically, assuming that the exact coordinates of the target token are (i, j), the fuzzy positional information is (i + s1, j + s2). s1 and s2 satisfy -0.5 ≤ s1, s2 ≤ 0.5 and follows uniform distribution.\nDuring training, randomly generated coordinate offsets are added to the reference coordinates during the training process, and grid samples for learnable location embeddings are performed based on the newly generated coordinates to generate fuzzy location encoding.\nIn case of inference, precise positional encoding is used instead of FPE. When there is a change in input resolution, interpolation is performed on learnable positional embedding. This has strong positional resilience because it was somehow seen and used in the FPE used in the training phase.\nTo compare the impact of different positional encodings on the model’s resolution generalization ability, several positional encoding methods were used. This includes commonly used sin-cos absolute position encoding (APE), conditional position encoding (CPE), global-local positional encoding (GLPE) in ResFormer, Relative Positional Bias (RPB) in Swin, and FPE. Note that only APE and FPE are compatible with the MAE framework.ViTAR-S is used for experiments without MAE, and ViTAR-M is used for experiments with MAE. As a result, FPE exhibits a significantly pronounced advantage in resolution generalization capability. Additionally, under the MAE self-supervised learning framework, FPE also demonstrates superior performance relative to APE.\nViTAR shows superior performance with any resolution # Image Classification # ViTAR is trained on ImageNet-1K form scratch and it demonstrates excellent classification accuracy across a considerable range of resolutions. Especially, when the resolution of the input image exceeds 2240, ViTAR is capable of inference at lower computational cost. In contrast, traditional ViT architectures (DeiT and ResFormer) cannot perform high resolution inference due to computational resource limitations.\nObject Detection # For object detection, COCO dataset is used ATM iterates only once because it does not utilize the multi-resolution training strategy in this experiment. If $\\frac{H}{G_{th}}$ and $\\frac{W}{G_{tw}}$ in ATM are fixed to 1, the results indicate that ViTAR achieves performance in both object detection and instance segmentation. And if setting $\\frac{H}{G_{th}}$ and $\\frac{W}{G_{tw}}$ to 2 in ATM, ATM module reduces approximately 50% of the computational cost while maintaining high precision in dense predictions, demonstrating its effectiveness.\nDiscussion # Applicability to Diffusion Models # It is currently challenging to generate images of various resolutions with generative models like Diffusion Models. Additionally, many diffusion models with ViT structures have been proposed recently (e.g. DiT, PixArt-α, Sora). Can the proposed method be applied to Diffusion Models as well? However, one consideration for applying it to diffusion models is how to effectively upscale the reduced size obtained through Grid Attention to ensure that the input and output sizes are the same.\nApplicability to Large Language Models (LLMs) # In LLMs, when receiving long context as input, positional embeddings are sometimes added using interpolation like this case. Would applying Fuzzy Positional Embedding (FPE) help handle long context inputs better? Or, just like training a network on low-resolution images to perform well on high-resolution images, can a network trained on short context in LLM maintain good performance on long context input?\nCan Grid Attention Replace Convolution? # The operation of Grid Attention is quite similar to the process performed by kernels in Convolution when calculating each grid. However, ATM maintains parameter efficiency by sharing weights. Would applying Grid Attention to existing CNN structures (e.g., VGG, ResNet) be more efficient?\n"},{"id":19,"href":"/docs/spring24/19_/","title":"19","section":"Spring24","content":" # "},{"id":20,"href":"/docs/spring24/20_/","title":"20","section":"Spring24","content":"##Background\n"},{"id":21,"href":"/docs/spring24/21_/","title":"21","section":"Spring24","content":" A Unified Framework for Model Editing # Authors: Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli\nPosted by Donggeun An, Jonghyun Chae\nIntroduction # In the rapidly evolving field of artificial intelligence and machine learning, keeping large language models (LLMs) up-to-date with the latest information is crucial. This paper presents a comprehensive framework, Equality-constrained Mass Model Editing for Transformers (EMMET), that integrates two major model editing techniques: Rank-One Model Editing (ROME) and Mass Editing Memory in Transformer (MEMIT). The proposed framework focuses on the retention-memory objective, which aims to inject new knowledge into the model while maintaining the fidelity of existing information.\nFigure 1. A diagrammatic representation of the preservation-memorization objective. Step 1: Find Keys to Preserve Identify key vectors \\(k_0\\) representing existing knowledge, ensuring they remain intact by processing with the weight matrix \\(W_0\\) to produce output vectors \\(v_0\\) . Step 2: Find a Fact to Memorize\nLocate new information to be added, represented by key vector \\(k_e\\) and output vector \\(v_e\\) , ensuring the model generates the correct new fact. Step 3: Update Weight Matrix\nModify \\(W_0\\) to \\(\\hat{W}\\) , preserving existing key vectors \\(k_0\\) while ensuring \\(k_e\\) produces \\(v_e\\) , thus integrating the new information accurately. Model Editing Evaluation Metrics # The success of model editing is measured using standard metrics.\nEfficacy Score (ES): Indicates if an edit has been successfully made to a model. It is measured as the percentage of edits where the probability of the new fact is greater than the probability of the old fact for a given query prompt. Paraphrase Score (PS): Represents the generalization ability of the model under an edit. It measures the percentage of edits where the probability of the new fact is greater than the probability of the old fact for paraphrases of the query prompt. Neighborhood Score (NS): Represents the locality of model editing. It measures whether editing a fact affects other facts stored inside a model. NS indicates the percentage of facts in the neighborhood of the edited fact that remain unaltered post-edit. Generation Entropy (GE): Represents the fluency of a model post-edit. It is calculated by measuring the weighted average of bi-gram and tri-gram entropies of text generated by an edited model. This value drops if the generated text is repetitive, a common failure case of model editing. Score (S): A composite metric defined to represent a combination of edit success, generalization, and locality. It is the harmonic mean of ES, PS, and NS. ROME and MEMIT: Overview # Figure 2: Figure shows a diagrammatic representation of a transformer layer. The layer being edited by ROME, MEMIT and EMMET is the projection weight matrix inside the MLP layer ( \\(W_{proj}\\) ). To further understand how model editing techniques like ROME, MEMIT, and EMMET work, it's essential to look at how they interact with the layers of a transformer model. Input Representation ( \\(h_{l-1}\\) ): The input to the transformer layer, \\(h_{l-1}\\) , is either the output from the previous layer or the initial input embedding. Attention Mechanism (Attn): The input \\(h_{l-1}\\) passes through the attention mechanism, which calculates attention scores and generates a context vector by attending to different parts of the input sequence. Feed-Forward Layer: The transformed input then goes through the feed-forward layer, consisting of a fully connected layer ( \\(W_{fc}\\) ) producing an intermediate representation, followed by a non-linear activation ( \\(\\sigma\\) ) like ReLU or GELU. Key Vector Generation ( \\(k\\) ): After the non-linearity, the intermediate representation is used to generate key vectors \\(k\\) , crucial for storing and retrieving the model\u0026rsquo;s knowledge. Projection Weight Matrix ( \\(W_{proj}\\) ): The projection weight matrix \\(W_{proj}\\) projects the key vectors into the final output space and is the focus of edits in ROME, MEMIT, and EMMET. Output Vector Generation ( \\(v\\) ): The projection weight matrix \\(W_{proj}\\) transforms the key vectors \\(k\\) into output vectors \\(v\\) , integrating the edits made to the model. Layer Output ( \\(h_{l}\\) ): The final output of the transformer layer, \\(h_{l}\\) , serves as the input for the next layer or as the model\u0026rsquo;s final output if it is the last layer. ROME (Rank-One Model Editing) # ROME is a method that facilitates direct modification of model parameters to incorporate new factual knowledge or modify existing information. ROME works by enforcing equality constraints that ensure precise alignment between the output of the updated model and the intended new knowledge. This method uses first-order updates to model parameters. This is expressed mathematically as adding a single outer product of the two vectors to the existing weight matrix. This approach is highly targeted, modifying the weights in a way that exactly matches the new facts with minimal changes elsewhere, making it ideal for precision-critical applications. ROME is effective for single edits or small batches, but because the method strictly adheres to equality constraints, it does not scale well for large edits, potentially leading to inefficiencies or long computation times in batch scenarios.\nMEMIT (Mass Editing Memory in Transformer) # MEMIT is designed for batch updates and is known for its flexibility and scalability in model editing tasks. Unlike ROME, MEMIT uses a least-squares constraint that provides more flexibility in how edits are implemented. This method optimizes a relaxed objective where the goal is to minimize the overall error across a batch of edits rather than achieving exact matches for each individual update. MEMIT’s strength lies in its ability to handle large batches of edits simultaneously, making it particularly useful for applications that require frequent and extensive updates to the stored knowledge. The algorithm adjusts the model\u0026rsquo;s parameters by calculating a closed-form solution that distributes the edits across the parameters in a way that balances the introduction of new facts with the preservation of existing knowledge.\nTable 1. Comparison between ROME and MEMIT when editing only a single layer for CounterFact dataset. The comparison between ROME and MEMIT reveals that both techniques are highly effective at model editing, with each having its strengths. ROME generally excels in generalization and efficacy, while MEMIT performs slightly better in maintaining locality and fluency, especially for larger models like Llama-2. EMMET (Unifying ROME and MEMIT) # Introducing EMMET # EMMET unifies ROME and MEMIT under the preservation-memorization objective. EMMET uses an equality constraint for batched edits, providing a balanced approach that leverages the strengths of both ROME and MEMIT.\nEMMET\u0026rsquo;s Closed-Form Solution # EMMET uses a closed-form solution to implement the equality constraints across batch edits. This solution involves modifying the weight matrix of a transformer model in such a way that the edits are distributed across the parameters efficiently, ensuring that each targeted update is reflected accurately in the model\u0026rsquo;s output. The key formula for EMMET’s update is given by: \\[\\Delta = (V_E-W_0K_E)(K_E^TC_0^{-1}K_E)^{-1}K_E^TC_0^{-1}\\] Here \\(V_E\\) represents the vector of desired outputs, \\(W_0\\) is the original weight matrix, \\(K_E\\) is the key vector representing the input associated with each fact, and \\(C_0\\) is the covariance matrix derived from the existing model parameters. EMMET operates under the preservation-memorization objective, which aims to preserve the integrity of the model’s existing knowledge while accurately incorporating new information. The algorithm is carefully designed to balance these objectives, ensuring that the updates enhance the model\u0026rsquo;s utility without introducing errors or biases. EMMET is designed to incorporate batch edits under equality constraints. This approach is similar to ROME\u0026rsquo;s method of applying precise updates but is scaled to handle multiple edits simultaneously. EMMET ensures that each edit precisely matches the desired update without adversely affecting the existing knowledge encoded in the model. One of the features of EMMET is its ability to perform large-scale batch edits, which can include up to 10,000 edits in a single batch. This is a significant enhancement over traditional methods that typically handle edits one at a time or in smaller batches. EMMET’s batch processing capability makes it particularly valuable for applications requiring frequent and extensive updates to model data.\nExperiments and Results # Figure 3. Single layer editing performance of EMMET as a function of batch size when compared to MEMIT on the CounterFact dataset. Figure 4. Performance comparison of EMMET and MEMIT when distributing the edit over multiple layers using the MEMIT edit-distribution algorithm on the CounterFact dataset. The effectiveness of EMMET has been validated through extensive testing on standard model compilation datasets, including evaluations on various models such as GPT2-XL, GPT-J, and Llama-2-7b. These experiments demonstrated that EMMET matches and sometimes exceeds the performance of MEMIT in terms of editing success rate, maintaining data integrity, and generalization ability across multiple datasets and model architectures. Conclusion # This unified approach allows for a comprehensive comparison of the two methods, showing they can optimize similar objectives through different constraints. The introduction of EMMET, a new algorithm for batch editing under equality constraints, demonstrates the ability to handle large updates efficiently, maintaining performance on par with existing methods. The paper confirms the robustness of these techniques through extensive empirical testing and establishes a solid theoretical foundation for understanding model editing dynamics.\n"},{"id":22,"href":"/docs/spring24/22_/","title":"22","section":"Spring24","content":" Larimar: Large Language Models with Episodic Memory Control # Posted by: Sunggyu Jang, Hyeonwoo Park\nAuthors: Payel Das (IBM AI Research), Subhajit Chaudhury (IBM AI Research) et.al\n1. Background # Large Language Model (LLM) is one of the most popular topics in these days, due to their outstanding performance on various Natural Language Processing (NLP) tasks. However, LLM has faced a lot of challenges at the same time. In this report, we especially focus on the \u0026ldquo;knowledge edit\u0026rdquo; problem.\nKnowledge edit in LLM research # Knowledge edit problem can be summarized as \u0026ldquo;constantly updating the knowledge of pre-trained LLMs to keep models fact-relevant, safe, and ethical after deployment.\u0026rdquo; [1] The point is that, we have to update the knowledge on the pre-trained model accurately and quickly. Figures below illustrate why do we need knowledge update.\nTo update new knowledge To mitigate context length generalization problem To erase sensitive data Fig1. Knowledge update: New knowledge should be injected constantly [2] Fig2. Context length generalization: The ability to quickly update the LLM can help with \"input context length generalization problem\" [3] Fig3. Selective fact forgetting: LLMs should forget personal \u0026 sensitive data [4] Memory network # However, knowledge edit is not so simple as it sounds. Pre-training LLMs requires substantial computational cost due to thier unprecedented amounts of parameters. Considering the fact that we have to introduce new knowledge into the pre-trained model frequently, re-training the whole model is not a feasible solution [2].\nTo tackle the problem, \u0026ldquo;memory network\u0026rdquo; was proposed. The main point of memory network is \u0026ldquo;to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to.\u0026rdquo; [5]\nFor example, let\u0026rsquo;s assume that you\u0026rsquo;re providing new information to a pre-trained LLM. What you expect to the model is to answer the following questions based on the facts you mentioned. In this case, the model can do the job by writing the knowledge from you into a memory and reading the relevant one from the memory to answer the question. This problem is called as \u0026ldquo;Question Answering (QA).\u0026rdquo;\nFig4. Example of QA [5] Variational auto encoder (VAE) # To implement the idea of memory network, concepts from variational auto encoder are usually used. VAE is a kind of generative model to generate an output similar to real data. To be specific, it aims to approximate the true distribution of input data with three components - encoder, decoder, and latent space.\nIn this post, we assume that readers have knowledge about VAE. For details, please refer to [6] and [7].\nFig5. VAE Structure [7] Neocortex-Hippocampus interactions # This paper imitates the role of brain. Humans can rapidly update their knowledge after encountering the first relevant instance. In the brain, this process is facilitated through interactions between the neocortex and the hippocampus. The hippocampus is the site for storing long-term memories, while the neocortex integrates long-term and short-term memories to relay the results to the body.\nFig6. Neocortex and the Hippocampus The Complementary Learning Systems (CLS) theory proposes a model that combines these complementary learning systems of the hippocampus and neocortex. The interaction between the neocortex and hippocampus in the brain is known to promote adaptive behavior through memorization and generalization. Furthermore, it is suggested that memory consolidation from the hippocampus to the neocortex is facilitated by the activation synchronized with multiple exact or false replays of the encoded experience in the hippocampus. This implies that the hippocampus functions as a generative associative network. 2. Contributions # Larimar introduces a class of memory-conditioned language models inspired by complementary learning mechanisms in the brain. This architecture facilitates real-time test-time adaptation without requiring time-intensive gradient-based learning or internal fact tracing, offering a faster method for updating LLMs. Utility Demonstration in Knowledge Editing and Context Generalization:\nThe proposed method is demonstrated on two significant and challenging use cases: knowledge editing and generalizing to longer input contexts. Larimar exhibits fast and accurate training-free adaptation to new inputs in both scenarios, outperforming baseline editing methods and existing language models. Selective Fact Forgetting and Information Leakage Prevention:\nLarimar effectively supports selective fact forgetting and prevents information leakage using its one-shot memory updating mechanism. Recursive Search-Based Solution for Long Context Generalization: A simple recursive search-based approach is provided to enable Larimar\u0026rsquo;s memory to generalize to longer input contexts.\n3. Model architecture [1] # Inspired by human brain (neocortex-hippocampus interactions), authors suggest \u0026ldquo;a class of LLMs augmented with an external episodic memory controller.\u0026rdquo; They utilize an episodic memory to mimic hippocampal fast-learning system, and use LLM as a neocortical slow learning system.\nFig7 below shows the overall architecture of Larimar. Basic idea is to implement VAE with external memory. It consists of three main components: encoder, decoder, and adaptive memory. Comparing the architecture with Fig5 would be helpful. In Larimar, memory corresponds to a latent vector.\nEncoder: Transforms the input into a latent vector Decoder: Generates an answer to the question conditioned on the memory Memory: Stores episodes in encoded form Fig7. Larimar architecture Let\u0026rsquo;s see how it works in two stages.\n3-1. Training # (1) Writing # The memory M in Fig7 has to be trained so as to approximate the distribution of X (X is an exchangeable-order invariant episode: X = { $x_{1}$, \u0026hellip;, $x_{N}$ }, a subset of the input data consisting of N samples). To do so, the model is trained to maximize the conditional log-likelihood of lnp (X|M). In this way, the model learns to compress X in a memory M, which then becomes a distributed associative memory. This process is similar to that of encoder in VAE. (Look at the green arrows in Fig7)\n(2) Reading # The reading weight matrix, W, is a random variable for generative ability of the model. In this paper, authors set a standard Gaussian prior p(W) ~ N(0, $I_{N \\times K}$ ) and posterior q(W) ~ N($\\bar{W}$, $\\sigma^2_{W} \\cdot I_{N \\times K}$), where the mean $\\bar{W}$ is estimated from each episode and $\\sigma_{W}$ is learnable. Memory readouts are obtained as Z$_{readout}$ = WM.\n(3) Summary # Three main components - encoder(e), associative memory(M), and decoder(d) - are jointly trained and optimized for an episode X, using the following loss:\n3-2. Inference # Once M$_{0}$ is trained via backpropagation, the posterior memory M is updated in one-shot by solving a minimization problem below. This problem can be efficiently done with the preudo-inverse of matrix. For more details, please refer to [8].\n4. Memory Operations [1] # In this paper, authors followed the ideas from [8] to combine pre-trained LLMs and memory component for knowledge edit. Fig8 illustrates the single training step of the memory.\nFig8. Basic memory operations [8] On top of that, sequential writing and forgetting is conducted as follows.\nFirst, given an initial set of encodings $Z_{0}$ and writing weights $W_{0}$, memory matrix and key covariance matrix are initialized as below.\nNext, memory $M_{i-1}$ is sequentially updated by adding a new set of encodings $Z_{i}$ or forgetting a previously written set of encodings $Z_{i}$. This process is conducted as below.\nFor instance, $\\alpha_{i}$ is 1 for writing, -1 for forgetting.\n5. Results # Wall Clock time # Fig9. Comparison between different editing methods and the wall clock time for a single edit The experiment was conducted on a single A100 GPU. Comparing the wall clock time for each editing method across 10 single edits, as shown in Fig8, Larimar was found to be approximately 4-10 times faster than the existing ROME and GRACE methods. Additionally, Larimar demonstrates the ability to address sequential edits, batch edits, and forgetting/deletion, which were not previously addressed in existing work. Single Fact Editing # This paper utilizes the CounterFact dataset for comparing Single Fact editing. The CounterFact dataset is designed to test the language model\u0026rsquo;s ability to handle counterfactual edits. It evaluates whether the model accurately learns new facts. It contains a total of 21,919 data points, and the evaluation is conducted using the first 2000 samples. In contrast to training the LLM on edits or causally tracing the original fact within the LLM and updating the relevant parameters, Larimar leverages one-shot memory update for editing. In this approach, the memory posterior is updated as the edit(s) of interest is written, and then the updated memory is queried. The read-out from the memory conditions the decoder to output the edit.\nFig10.Single fact edit performanceon CounterFact dataset comparing with baseline. Top two best systems are highlighted. The results are shown in Fig 9. Edit Success measures the percentage of cases where the edited result has a higher probability than the original result, while Paraphrase evaluates whether the model achieves the same performance using paraphrase prompts. Neighborhood assesses the model's ability to retain knowledge about the original object. Larimar demonstrates comparable performance in editing new facts and handling prompts. Sequential Fact Editing # To check sequential fact editing, Test retention rate(TRR) and edit retention rate(ERR) are used. TRR check how well an edited model retains its performence on tis original testing data. Larminar decoder\u0026rsquo;s perplexity was tested on 1000 random test samples from wikitext using a separate language model. In comparison, baseline models compute TRR from mean F1 scores from 1000 random samples of NQ data. ERR check how well an edited model retains previous edits. This paper, ERR was evaluated by F1 score after 1000 sequential edits when querying the memory with the encoded query Zquery for each written fact.\nFig11. Sequential editing on ZsRE dataset According to the figure 9, Larimar’s comparable ERR performance to GRACE, while preserving its original test set performance.Larimar-1.3B achieves editing speeds approximately 10 or more times faster than GRACE on GPT-2 XL. Selective Forgetting # This results shows that specific fact can be selectively erased from N facts that are have been written in Larimar\u0026rsquo;s memory.\nFig12. Batch editing accuracy on counterfact dataset. Green: MEMIT, Orange: ROME, Magenta: MEND, Black: Larimar. Fig 10 shows many edits can be written at once to memory and accurately retrieve from it. Rewrite accuracy is near 100% for up to 512 edits (the memory size K) and then drops to 82% for 1024 edits. This result shows Larimar's ability to compress more than K facts into its size-K memory. This performance level is higher when compared to baselines like MEND and ROME, but subpar compared to MEMIT, which can accurately handle a very large batch of edits at a cost of reduced editing speed and is also not meant to handle sequential editing. To test the ability of Larimar for selectively forgetting specified facts during inference, write N facts to memory and then forget one fact, and also write to memoty in its place the same fact with the answer replaced with the string \"unknown.\" Then, compare recall for the forgotten fact before and after the forgetting operation. Paper also report the recall on the remaining N −1 facts in memory to demonstrate that forgetting does not compromise other memories. The samples used are from the ZsRE validation set and from the Counterfact test set. Fig13. Fraction of facts with accurate recall, for the Counterfact and ZsRE datasets, after writing N factrs to memory and removing one. As a result, Larimar achived higher performance in forgotten and retained information is all testbench than Basemodel. This shows that Larimar works better on selective forgetting. Recall Performance # Larimar performs fact recall with long context using data that is not present in the base decoders pretraining corpus. Facts are curated from CNN Fast Facts. Recursive search in the latent memory space and using readouts to construct new higher-level memory is performed to process the long context with Larimar’s memory trained on a relative small episode length. It should be noted that memory hierarchy is also found in hippocampus and thought to be implicated in learning.\nThe recall rate, in the context of information retrieval, is a measure of how well a system retrieves all relevant items of a specific class. It represents the proportion of relevant items that the system correctly identifies out of all the relevant items available. For example, in a search engine scenario, the recall rate indicates how many of the relevant documents related to a user's query are successfully retrieved by the system. A high recall rate implies that the system effectively captures most, if not all, of the relevant information. Fig14. Novel fact addition recall rate on FastFacts. Fig 12 shows Larimar’s recall performance does not degrade much with increasing input context length, even compared to some of most competitive baseline LLMs trained with longer training context. We also compare with Supersizing Transformer, which is a memory-augmented model, however it did not show competitive recall performance because it was not trained to perform memory-conditioned generation. Due to memory processing in the latent space, Larimar is also efficient is terms of number of KV cache token computation compared to baseline methods. 6. Conclusion and further improvements # This paper propose enhancing Large Language Models (LLMs) with a dynamically updatable and distributed episodic memory. By leveraging a one-shot memory update mechanism and combining it with memory-conditioned decoding, this framework demonstrates precise, robust, and significantly faster editing performance compared to baselines in both single-fact and challenging sequential editing experiments. Using the same memory update mechanism enable fast and selective fact deletion operations, as well as effective information deletion mechanisms. Additionally, provide a simple approach for handling long input contexts, demonstrating better fact recall from longer input contexts in Larimar\u0026rsquo;s memory space compared to state-of-the-art LLMs trained with much larger training context windows. When compared to cutting-edge LLMs trained with much larger training context windows, Larimar showcases its advantages.\nJust as the interaction between the Neocortex and Hippocampus inspired the design of the memory module in this paper, drawing inspiration from the Corpus Callosum to conceptualize hardware could also be a viable approach. The Corpus Callosum, as a part of the brain, serves as a major connecting structure of the cerebral nervous system responsible for communication between the hemispheres. It spans across the entire brain, situated between the left and right hemispheres, facilitating the exchange and coordination of information between them to harmonize various brain functions. Adjusting all parameters of the model during the process of learning new knowledge in LLMs incurs significant costs. I propose a method to divide the model\u0026rsquo;s parameters into parts and update only the relevant parameters corresponding to the data being trained, thereby reducing costs. Introducing a module performing the role of the Corpus Callosum separately allows for the exchange and adjustment of data between parts, enabling more efficient learning with reduced costs and facilitating the processing of various types of data individually and complex information collectively within the model.\nAnother improvement could be to add a memory module specifically for image processing. The memory module used in this paper accepts natural language as input and produces natural language as output. I propose introducing a separate memory module for processing images, so that when both natural language and images are input simultaneously, the information can be processed and reflected in the output. This would enable more effective processing by LLMs when both images and natural language are provided as input. For example, it could be used to provide a photo of a crime scene and information as input, and obtain clues about the suspect as output.\n7. References # [1] https://arxiv.org/abs/2403.11901 -\u0026gt; Larimar: Large Language Models with Episodic Memory Control\n[2] https://arxiv.org/abs/2310.16218 -\u0026gt; Knowledge Editing for Large Language Models: A Survey\n[3] https://arxiv.org/abs/2207.04901 -\u0026gt; Exploring Length Generalization in Large Language Models\n[4] https://arxiv.org/abs/2402.05813 -\u0026gt; Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models\n[5] https://arxiv.org/abs/1410.3916 -\u0026gt; Memory Networks\n[6] https://arxiv.org/abs/1312.6114 -\u0026gt; Auto-Encoding Variational Bayes\n[7] https://process-mining.tistory.com/161 -\u0026gt; VAE, blog post\n[8] https://openreview.net/forum?id=Harn4_EZBw -\u0026gt; Generative Pseudo-Inverse Memory\nbrain figure\n"},{"id":23,"href":"/docs/spring24/23_/","title":"23","section":"Spring24","content":" Beyond Language Models: Byte Models are Digital World Simulators # Authors: Shangda Wu, Xu Tan, Zili Wang, Rui Wang, Xiaobing Li, Maosong Sun Links: Paper, GitHub, Hugging Face, Official Project Page\nPosted by Dohun Kim and Yeongwoo Kim\nbGPT: from Language Models to Byte Models # Byte models expand traditional language models to the byte level, starting from the premise that all digital data and operations are fundamentally byte-based. These models process data from various modalities such as text, audio, and images uniformly as bytes, increasing their applicability in a wide digital environment.\nFigure 1: The bGPT framework simulates digital systems using native binary data. It integrates diverse data types into a single model by treating everything as a byte sequence.\nIn this paper, bGPT is introduced. bGPT is designed to model digital data at the byte level and is optimized to effectively process byte sequences. It has demonstrated performance comparable to specialized models across various modalities, including text, audio, and images, and offers new possibilities for predicting, simulating, and diagnosing algorithms or hardware operations. Designed to predict and understand bytes, bGPT provides a deeper understanding of and interaction with digital systems.\nThe main contributions of this paper are as follows:\nbGPT, a model with next-byte prediction is presented to simulate digital systems Hierarchical Transformer architecture is adapted to handle byte sequences efficiently. In-depth analysis of bGPT\u0026rsquo;s performance on text, audio, and image data is provided. Novel benchmarks are introduced to show bGPT\u0026rsquo;s capabilities for digital world modeling. Proposed bGPT Framework # Architecture # Figure 2: The hierachical Transformer architecture of bGPT. It segments sequence of bytes into a sequence of patches, to balance the need for long sequences and computational efficiency.\nLearning patterns in digital systems at the byte level provides a unified approach to integrating various data types, but the high resolution of bytes results in long sequences that significantly increase computational costs. This issue is especially pronounced in transformer-based models, limiting the efficiency and scalability of processing binary data. bGPT is equipped with a hierarchical structure designed to efficiently handle entire byte sequences. This structure segments a sequence of byte \\(B = \\{b_1, b_2, \\ldots, b_T\\}\\) of length \\(T\\) into a sequence of patches \\(\\mathcal{P}\\) , where each patch contains exactly \\(S\\) bytes: \\(\\mathcal{P} = [P_1, P_2, \\ldots, P_N]\\) where \\(N = \\left\\lceil \\frac{T}{S} \\right\\rceil\\) is the number of patches,\n\\(P_i = [b_{(i-1)S\u0026#43;1}, \\ldots, b_{(i)S}]\\) for \\(( 1 \\leq i \\leq N)\\) , if \\(T \\mod S \\neq 0\\) , the last patch defined as \\(P_N = [b_{(N-1)S\u0026#43;1}, \\ldots, b_T, \\underbrace{e, \\ldots, e}_{S - (T \\mod S)}]\\) where \\(e\\) represents the \u0026lt;eop\u0026gt; (end-of-patch).\nComponents\nLinear Projection Layer: Each byte patch is mapped to a high-dimensional feature space through a linear projection layer. During this process, each byte is encoded into a 257-dimensional vector, which includes the 256 possible byte values and a special \u0026lt;eop\u0026gt; (end-of-patch) token. Patch-Level Decoder: The embedded patches are processed by a patch-level decoder. This decoder plays a role in predicting the features of the next patch from the embedding of each patch, thereby learning the structural patterns of the entire dataset. Byte-Level Decoder: Based on the predicted patch features, the byte sequence within each patch is reconstructed. The byte-level decoder uses the features of each patch to predict the next byte within that patch, processing the detailed information of the entire byte sequence. Training Objectives # 1. Generative Modeling # This approach requires the model to predict the next byte in a given byte sequence. The model takes the byte sequence \\(B = \\{b_1, b_2, \\ldots, b_T\\}\\) as input and utilizes all previous byte information to predict the next byte \\(b_{i\u0026#43;1}\\) at each position.\nAs a loss function, the negative log likelihood of the next byte at each step is minimized. This encourages the model to maximize the likelihood of the actual occurrence of the next byte. \\(\\mathcal{L}_{\\text{GEN}}(\\theta) = - \\sum_{i=1}^{T-1} \\log p(b_{i\u0026#43;1} \\mid b_1, b_2, \\ldots, b_i; \\theta)\\) 2. Classification # Based on the knowledge acquired through generative modeling, bGPT can also be applied to classification tasks for labeled datasets. In this process, the model takes a byte sequence as input and predicts the category to which that sequence belongs. For classification tasks, the loss function used is the cross-entropy loss, which ensures that the model accurately outputs the prediction probabilities for each category.\n\\(\\mathcal{L}_{\\text{CLF}}(\\theta) = - \\sum_{k=1}^{K} y_k \\log p(y_k \\mid B; \\theta)\\) These training objectives enable bGPT to understand various byte-based data and accurately mimic digital patterns of the real world. The combination of generative approaches and classification capabilities grants the model the flexibility to tackle a diverse range of problems. Through this, the model can go beyond simple pattern recognition to play a crucial role in predicting and analyzing the operations of complex digital systems.\nApplications # 1. Digital Media Processing\nbGPT is used for processing various types of digital media data such as text, audio, and images. This model performs learning targeted at media files through generative modeling, transforming the data into features and subsequently performing classification tasks based on these features.\nFor example, audio files are converted to and processed in WAV format, while images are processed at a low resolution in BMP format. By utilizing these standardized datasets, bGPT can develop a generalized understanding of various media types.\n2. Algorithm and Hardware Simulation\nbGPT is particularly useful for tasks such as data conversion and CPU state modeling. This means bGPT can learn the digital conversion process and simulate the operation of CPUs to predict the state of the CPU after various commands are executed.\nFor example, in the task of converting the music data format from ABC notation to MIDI format, bGPT learns to transform text-based music scores in ABC notation into binary performance signals in MIDI. Additionally, this model is also capable of performing the reverse conversion from MIDI back to ABC notation.\nExperiment # 1. Processing Various Digital Media\nExperiment Overview To assess the flexibility and versatility of the bGPT model, experiments with various types of digital media data were conducted. This involved handling a wide range of file types including text, audio, and image data, with the aim to measure the model\u0026rsquo;s ability to process these types and to see how well bGPT generalizes compared to specialized models. The experiment included both generative modeling and classification tasks.\nExperimental Data The datasets used in the experiment included:\nText: Wikipedia data and AG news dataset. Audio: LibriSpeech and Speech Commands v2 dataset. Images: ImageNet and CIFAR-10 dataset. These datasets are ideal resources for evaluating the diverse media processing capabilities of bGPT.\nExperimental Setup The bGPT model was trained under various settings:\nbGPTimage: Trained exclusively with image data (ImageNet). bGPTlibri: Trained exclusively with text data (Wikipedia). bGPTmix: Trained with a mix of all the above datasets. Each model was fine-tuned for specific types of classification and generative tasks post-pretraining, providing a direct comparison of each model\u0026rsquo;s performance.\nResult and Analysis\nText Processing: bGPTwiki showed high classification accuracy on the AG News dataset, indicating bGPT\u0026rsquo;s strong performance in text-based tasks. Audio Processing: bGPTlibri demonstrated excellent performance on the Speech Commands v2 dataset, showcasing its high potential in audio processing. Image Processing: bGPTimage recorded high accuracy on the CIFAR-10 dataset but showed somewhat lower performance on ImageNet. This suggests that while bGPT works well with relatively simple images, it may have limitations with more complex images. Model | AG News (4 classes) | CIFAR-10 (10 classes) | Speech Commands v2 (36 classes) | BPB Acc (%) BPB Acc (%) BPB \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; bGPT_random 1.3496 84.74 3.4928 76.73 1.5414 bGPT_wiki 1.0639 92.49 3.6663 77.02 1.5719 bGPT_image 1.4179 83.16 3.1234 88.69 1.5326 bGPT_libri 1.3993 83.59 3.3345 83.51 1.4818 bGPT_signal 1.4058 83.80 3.1554 87.65 1.4898 bGPT_mix 1.0935 91.75 3.2279 84.32 1.5086 Baselines 0.9237 94.50 — 98.13 — 2. Algorithm and Hardware Simulation\nExperiment Overview One of the unique capabilities of the bGPT model is its ability to simulate the operations of algorithms and hardware. This experimental section assesses how bGPT handles complex data conversion processes and CPU state modeling tasks. These capabilities are particularly significant in the fields of cybersecurity, system diagnostics, and hardware optimization.\nExperiment Methods bGPT\u0026rsquo;s performance was evaluated in the following two key areas:\nData Conversion: This experiment evaluates whether bGPT can learn the process of converting ABC music notation into MIDI format. The task tests how bGPT models complex algorithms and their ability to convert actual music files. CPU State Modeling: CPU state modeling assesses how bGPT predicts and updates the state of a CPU based on a given set of machine instructions. This is particularly useful for understanding and predicting hardware operations. Results and Analysis\nData Conversion Performance: bGPT performed the conversion between MIDI and ABC notation with high accuracy. Notably, it also showed high accuracy in converting MIDI back to ABC notation, indicating that bGPT successfully learned the inherent structures and patterns of the data. CPU State Modeling Performance: bGPT accurately predicted the resulting state of CPUs from an initial state across a variety of CPU instructions. It achieved over 99% accuracy even with complex instruction sequences, demonstrating bGPT\u0026rsquo;s detailed understanding of the internal workings of hardware. Conclusion and Future Work # bGPT has proven to be a powerful model capable of effectively processing various types of digital media data. Particularly, this model can be flexibly applied to different types of data and has demonstrated performance that can compete with models pretrained on specific datasets. These results show that bGPT can be extremely useful in solving a wide range of real-world problems.\nMoreover, bGPT has proven its ability to go beyond simply processing data, successfully modeling and simulating the operations of complex algorithms and hardware. This capability will be particularly valuable in fields related to technical problem solving and new hardware design.\nbGPT extends deep learning to binary data processing through byte prediction. Experiments have demonstrated bGPT\u0026rsquo;s strong scalability in native binary data modeling.\nFuture research directions for byte models include:\nReducing training costs to make byte model training more feasible. Expanding the model and dataset sizes to accommodate a wider range of native binary data and handle larger digital media files such as high-resolution images and videos. Improving performance in underexplored tasks involving native binary data across various application domains. References # Beyond Language Models: Byte Models are Digital World Simulators (arXiv) "},{"id":24,"href":"/docs/spring24/24_/","title":"24","section":"Spring24","content":" # "},{"id":25,"href":"/docs/spring24/25_/","title":"25","section":"Spring24","content":" Merging Text Transformer Models from Different Initializations # Posted by: Kyungtae Kim, Minwoo Kim\nAuthors: Neha Verma (Johns Hopkins University), Maha Elbayad (Meta)\n[paper link].\nAlthough recent works on model merging have exhibited low- or zero-barrier mode connectivity between models with different initialization, model merging on transformer architecture has not yet been studied extensively. The application of previous merging techniques on the transformer structure is limited due to its unique structural characteristics, such as residual connection, multi-head attention (MHA), and sequential input. The paper merges separate transformer minima, proposing a new model merging technique to investigate the relationship between the pre-trained models\u0026rsquo; minima in the loss landscape. Using permutation-based model merging, authors found lower loss barriers between minima compared to other model merging techniques such as model averaging. The results showed that the model has less sharp and isolated minima than previously expected.\nThe contributions of the researchers are listed as follows:\nThey introduced a new transformer merging algorithm based on model permutation. They showed that the technique leads to decreased loss barriers between masked language models trained from different initializations compared to other merging methods. They extended their approach to fine-tuned models and showed consistently smaller loss barriers between models compared to vanilla merging. Background # Transformer # Figure 1. Basic structure of a transformer. Transformer is a type of sequence-to-sequence (seq2seq) models that takes a sequence of tokens as an input, and computes according to the input token. Unlike previous seq2seq models where a certain input token had a hard time affecting every output tokens, transformer uses self-attention, which allows all tokens to affect every output tokens. This allows for better performance in data where the distance between tokens has low relationship to the importance of the tokens\u0026rsquo; importance. For more details on transformers and attention, see the paper \u0026lsquo;Attention is All You Need\u0026rsquo;.\nLoss Landscape # Loss landscape is a representation of the loss values around the weight space of the network. Loss landscape helps researchers see how well a neural network has been trained and gives researchers new insights on their models.\nFigure 2. An example of a loss landscape of a deep learning model. DNNs are trained by optimizing a loss function with an stochastic gradient descent (SGD) variant. The loss landscapes of these networks have been shown to contain infinitely many global minimizers that are reachable with the help of SGD. One reason for the abundance of minima is overparameterization, which leads different functions to behave similarly on the training data. Permutation and scaling invariances also lead to functionally identical minima that differ in the weight space. Prior works stated that the optima of loss functions are connected by simple curves over which training and test accuracy are nearly constant (no loss barrier). This is called mode connectivity. Other researchers conjectured that if the permutation invariances of neural networks are taken into account, these optima are linearly mode connected, i.e. the linear path connecting these two models has no loss barrier. In the paper, the authors pay attention on how permutation between models could lead to similar or identical loss landscapes.\nModel Interpolation # Model interpolation is a technique that blends two or more models to create an intermediate model. This process is mostly done by averaging the model weights. Researchers found out that if fine-tuned models lie in a single low error basin, then the weight averaging performs similarly to ensembling, which combines the output of multiple fine-tuned model to hopefully obtain a better result. It is however not guaranteed that fine-tuned models (starting from the same initialization) will reside in the same loss basin. Prior work on linear interpolation-based model merging has focused on improving the algorithms used to bring the hidden units of two networks into alignment, in order to reduce the barrier to interpolation between them.\nPermutation-based Merging # Feed-Forward Layers # In this section, we explain how the authors of the paper used permutation to find the similarities between two distinct models and merge them. In short, permutation order that maximizes the cross-correlation of the models is calculated and the permutation is used to change the weight ordering.\nIn more details, given two models \\(\\theta_A\\) and \\(\\theta_B\\) trained from distinct initializations, the authors compute post-activation features for each layer or sublayer parameter \\(\\text{W}_l\\subset \\theta\\) in order to compute the similar parts across models. The researchers compute \\(d\\) -dimensional activations across \\(n\\) tokens from both models \\(\\text{X}_A, \\text{X}_B\\in \\mathbb{R}^{n\\times d}\\) . Then, the feature relatedness via cross-correlation is computed as\n\\[C=\\text{corr}(\\text{X}_A, \\text{X}_B)=\\frac{\\mathbb{E}[(\\text{X}_A-\\boldsymbol{\\mu}_{\\text{X}_A})^\\text{T}(\\text{X}_B-\\boldsymbol{\\mu}_{\\text{X}_B})]}{\\boldsymbol{\\sigma}_{\\text{X}_A}\\boldsymbol{\\sigma}_{\\text{X}_B}},\\] where \\(\\boldsymbol{\\sigma}\\) is a standard deviation vector, and \\(\\boldsymbol{\\mu}\\) is a mean vector. The features are standardized since the magnitude of features values can vary greatly depending on the initialization. Next, the permutation that gives the highest correlation score is computed, and is declared as the optimal computation. More specifically, given \\(C\\in\\mathbb{R}^{d\\times d}\\) and a permutation mapping \\(\\pi\\) , the optimal permutation is computed as follows:\n\\[\\text{arg}\\max_\\pi \\sum_{i=1}^{d} C(i, \\pi(i)).\\] cf. The above problem is solved using the Jonker-Volgenant algorithm.\nNext, the permutation mapping \\(\\pi\\) is converted to a permutation matrix \\(\\text{P}\\) . The matrix is then multiplied to the original weight matrix of \\(B\\) denoted as \\(\\text{W}_l^B \\subset \\theta_B\\) . Then the permuted weight matrix \\(\\text{P}\\text{W}_l^B\\) closely resembles the weight \\(A\\) , denoted as \\(\\text{W}_l^A \\subset \\theta_A\\) . Denoting the modified model parameters as \\(\\theta_B\u0026#39;\\) , the final merged model is computed as \\(\\lambda\\theta_A\u0026#43;(1-\\lambda)\\theta_B\\) for some \\(\\lambda\\in[0,1]\\) .\ncf. If permutation matrix \\(\\text{P}\\) is multiplied in layer \\(l\\) , then \\(\\text{P}^{\\text{T}}=\\text{P}^{-1}\\) is applied in the next layer to unpermute the ordering, i.e.,\n\\[\\text{W}_{l\u0026#43;1}^{B\u0026#39;} \\leftarrow \\text{W}_{l\u0026#43;1}^{B}\\text{P}^{\\text{T}}.\\] Multi-head Attentions # Figure 3. Different head alignments occuring from different initialization. Though the 5 heads of the multi-head attention all connect to the same 5 features, the order of the heads may differ. As aformentioned, multi-head attention mechanism can be challenging to deal with due to its unique properties. The authors propose using permutation on each attention head separately and not permuting features between attention heads.\nMore specifically, multi-head attention parameters include parameters from key, query, value, and linear layer each denoted as \\(\\text{W}_K\\) , \\(\\text{W}_Q\\) , \\(\\text{W}_V\\) , and \\(\\text{W}_O\\) . For each key, query, and value weights, the whole parameter \\(\\text{W} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}\\) is partitioned into \\(H\\) attention heads each of output dimension \\(d_k = d_{\\text{model}}/H\\) . Permutation should be operated on each attention head separately, in order to apply a permutation to full weight matrices and maintain the functional equivalence of the overall model. This is because the final hidden vector from MHA reflects a concatenation of the result from each head, which are computed separately with weights \\(\\text{W}_{K_i} ,\\text{W}_{Q_i} , \\text{W}_{V_i}\\) for head \\(i\\) . As can be seen from figure 123, since the models are trained from different initializations, the correspondence of their attention heads may differ in addition to the correspondence of features within each head. The features are extracted just after the attention computation and before the linear layer. The features are used to compute \\(C\\) , and then the correlation matrix is partitioned by heads into \\(d_k \\times d_k\\) correlation matrices, for each potential attention head pair. Next, optimal permutation for each unique head pair \\((j, k)\\) is computed. Each head\u0026rsquo;s internal permutation is computed and stored, and the cost is computed as\n\\[\\text{cost}(j,k)=\\max_\\pi \\sum_{i=1}^{d_k} C_{jk}(i,\\pi(i)),\\] where \\(C_{jk}\\) refers to the specific partition of the overall correlation matrix. The outer head correspondence permutation is computed as\n\\[\\pi_{\\text{outer}}=\\text{arg}\\max_\\pi \\sum_{h=1}^{H} \\text{cost}(h,\\pi(h)).\\] The algorithm returns a permuting matrix \\(\\text{P}_{\\text{MHA}}\\) , which is applied to each of \\(\\text{W}_V\\) , \\(\\text{W}_K\\) and \\(\\text{W}_Q\\) .\nResidual Connections # Each transformer layer comes with two residual connections, as can be seen from Figure 1. The residual connections can be formulated as follows:\n\\[\\begin{aligned} x_a^r\u0026amp;=\\text{LN}(\\text{W}_O \\text{MHA}(x) \u0026#43; x),\\\\ x_f^r\u0026amp;=\\text{LN}(\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; x_a^r). \\end{aligned}\\] The input and output of both sublayers are added to create a new output. This implies that if a permutation operation is applied to the output state, the permutation should be the same for both addends. Also, since the inputs passes through the LayerNorm module, the permutation to the output should also permute the features of the LayerNorm module also. Ignoring the parameters of the LayerNorm,\n\\[\\begin{aligned} \\text{P}x_f^r\u0026amp;=\\text{P}(\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; x_a^r)\\\\ \u0026amp;=\\text{P}\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; \\text{P}x_a^r\\\\ \u0026amp;=\\text{P}\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; \\text{P}(\\text{W}_O \\text{MHA}(x) \u0026#43; x) \\end{aligned}\\] Since the input to each layer must be permuted ( \\(\\text{P}x\\) ), and the output of each layer is also permuted ( \\(\\text{P}x_f^r\\) ), the entire transformer architecture uses the same \\(\\{\\text{P}, \\text{P}^{\\text{T}}\\}\\) matrices for all weights involved in residual connections.\nModels, Tasks, Datasets and Evaluation settings # In this work, the authors investigated 5 different BERT models from the MultiBERTs reproductions seeds 1 through 5 (See \u0026lsquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u0026rsquo; \u0026amp; \u0026lsquo;The MultiBERTs: BERT Reproductions for Robustness Analysis\u0026rsquo;). Each model has the following properties:\nEach model comes from a bert-base-uncased checkpoint. Different random initialization and random ordering. Same original BERT vocabulary and tokenizer. To test the baseline method, they used the masked language modeling task while employing the validation set of the Wikitext-103 benchmark as the evaluation data. Next, they extracted over one million sentences from the Books corpus. In classification tasks, they employed fine-tuned models with randomly initialized classification head with pooling layer and classification layer weights. The authors kept the head initializations the same across the models. They used the General Language Understanding Evaluation (GLUE) benchmark excluding WNLI. As a baseline for comparison, vanilla averaging is defined as:\n\\[\\theta_{avg} = \\frac{1}{2}(\\theta_A\u0026#43;\\theta_B)\\] In this work, they defined new evaluation definitions. They defined loss-barriers as (\u0026lsquo;M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations\u0026rsquo;): \\[\\max_{\\lambda} \\mathcal{L}(\\lambda\\theta_A \u0026#43; (1 - \\lambda)\\theta_B) - \\frac{1}{2}(\\mathcal{L}(\\theta_A) \u0026#43; \\mathcal{L}(\\theta_B))\\] A masking probability of \\(p = 0.15\\) across block sizes of 128 tokens were used to compute MLM loss/pseudo-perplexity. For \\(N\\) masked samples in the text \\(\\textbf{W}\\) , pseudo-perplexity is defined as:\n\\[\\mathrm{Pseudo-PPL}(\\textbf{W};\\theta) = 2^{-\\frac{1}{N} \\sum_{i=1}^{N}\\log_{2}\\,p_\\theta(\\omega_i|\\textbf{W}_{\\backslash{i}})}\\] Results # By component # First, they found that the merging all feed-forward sublayers and/or merging all multi-headed attention sublayers reduces the pseudo-perplexity compared to the baseline. Remakably, combination of them leads to the reducuction of the perplexity by about 7 times at \\(\\lambda = 0.5\\) (See Figure 4). The reduced barrier suggests that a lower loss path has formed among these models, indicating a connection between the minima with a barrier similar to what they report.\nFigure 4. Results of pseudo-perplexity scores of 10 MultiBERTs with vanilla averaging, merging all feed-forward sublayers, and merging all multi-headed attention sublayers and all multi-headed attention sublayers. Next, they investigated how well these Transformers learn similar representations of the model. The average feature correlations of both the Feed-Forward layer and the attention pre-merged/merged with our method are calculated. The aligned models show higher average feature correlations than the orignal models. However, these values are no more than 0.3 because some pre-trained transformers can be sparsely activated and be pruned heavily leading to lower average feature correlations (Li et al.,2023; Dalvi et al., 2020).\nFigure 5. Results of average feature correlations between 10 masked language model pairs. Multi-headed attention # Next, they investigated loss barrier of Head-Permutation multi-headed attention approach. It is worth noting that this approach maintains head structure while allowing different head correspondences (Head-Perm). The proposed method exhibits lower loss barrier than method using simple attention averaging (Vanilla Attention Avg.), method that ignores the multiheaded structure of the weight parameters (Ignore-Heads), and method that does not allow for different head correspondences across different models (Monotonic) while exhibiting clear attention head boundaries of the correlation matrix (See Figure 5 and Table 1).\nMethod Loss Barrier\u0026darr; Std. Err. Vanilla Attention Avg. 4.31 0.21 Monotonic Head Alignment 4.13 0.20 Ignore-Heads 3.97 0.25 Head-Perm 3.71 0.23 Table 1. Loss Barriers of 10 MultiBERTs merged with feed-forward and attention components merged. Figure 6. Results of a correlation matrix between the first multi-headed attention layer from two different MultiBERTs models. Residual Stream # Next, the effect of the permutation alignment involving residual connection parameters is discussed. Repeated Add/Norm components sharing the permutation operations reduce the permutation symmetries and available residual stream parameters. The identity permutation which uses the identity matrix \\({I_d}\\) exhibits the lowest loss barrier because only one pair of \\({\\{P, P^T\\}}\\) is in the residual stream. We note that the seperate permutation approach, despite it having the largest loss barrier and no valid symmetry, has largest degrees of freedom.\nMethod Loss Barrier\u0026darr; Std. Err. Identity 4.95 0.38 First 7.58 0.19 Last 7.41 0.18 All 7.34 0.22 Seperate 9.38 0.49 Table 2. Loss Barriers of merged MultiBERTs with only residual components merged. Amount of Data # Moreover, they investigate the effect of the amount of sentences on the loss barrier. Despite the combination of feed-forward and attention layers, there is no strong directional relationship between the amount of data and the loss barrier. It seems that some variations are attributed by the quality of the data (See Figure 7).\nFigure 7. Results of loss barrier respect to the amount of sentences. GLUE results # Finally, they compared the loss barriers of their method to those of vanilla averaging approach for eight different GLUE tasks including residual permutations (See Figure 8 and Table 3). Vanilla averaging (STS-B) exhibits the highest loss, but some tasks show that the vanilla averaging outperforms their approach. They observe inconsistent loss reduction, with lower loss barriers than those of the masked language modeling setting. They also observe that lower loss pattern than either parent model at about \\(\\lambda = 0.15\\) and \\(\\lambda = 0.85\\) . Interestingly, M-shaped curve can be found in some vanilla merges between these fine-tuned models. In this perspective, their method could be extended to explore lower loss paths between finely-tuned minima. However, the selection of optimal data for the lowest loss, understanding of fine-tuned models and pre-connectivity in the loss landscape are remained for future work.\nFigure 8. Loss barrier curves for 8 GLUE tasks for vanilla interpolation and our strategy. Vanilla averaging Proposed Barrier Error Barrier Error MNLI-mm 0.61 0.03 0.72 0.08 QQP 1.37 0.09 1.20 0.11 QNLI 0.64 0.04 0.77 0.06 SST-2 0.42 0.04 0.36 0.07 CoLA 1.31 0.14 1.11 0.13 STS-B 5.15 0.44 4.24 0.35 MRPC 2.74 0.08 1.93 0.11 RTE 0.53 0.04 0.41 0.05 Table 3. Comparison of loss bariers between fine-tuned BERT model across 8 GLUE tasks for vanilla interpolation and our strategy. Conclusion # In this work, the authors develop a new strategy for model mergring based on permutation mapping and demonstrates reduced loss barriers between masked languaged models with different initialziation compared to vanilla merging. Next, they extend their approach to fine-tuned models. The authors suggest that understanding the connectedness between models lead to achieving sharpness of minima and smoothness Transformer loss space. Moreover, it can open up new possibilities for improving design optimization methods, ensembles of models, and additional merging techniques. Specifically, this paper shows that permutation invariances of Transformer model is considered to characterize the geometric features of minima. Finally, they shad the light on the relationships between fine-tuned models, Transformer width and loss barriers, and the data for characterize the relationship between Transformer minima.\nFurther research is possible, examining the proposed techniques on a more complicated or larger dataset. Also, more sophisticated inspection on the aformentioned M-shaped loss curve is needed.\nReferences # Paper: https://arxiv.org/abs/2403.00986\nAttention mechanism: https://arxiv.org/abs/1706.03762\nLoss landscape explanation: https://arxiv.org/abs/1712.09913\nBERT: https://aclanthology.org/N19-1423/\nMultiBERTs: https://arxiv.org/abs/2106.16163\nWikitext-103 benchmark: https://arxiv.org/abs/1609.07843\nBooks corpus: https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html\nGLUE: https://arxiv.org/abs/1804.07461\nWNLI: https://arxiv.org/abs/1810.04805\nLoss barrier: https://arxiv.org/abs/1803.0363\n"}]